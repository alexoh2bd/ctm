{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2491748,"sourceType":"datasetVersion","datasetId":1500837}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport os\nfrom PIL import Image\nimport json\nimport tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom torch.amp import autocast, GradScaler\nimport time\nimport copy\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# for dirname, _, filenames in os.walk('/kaggle/input/imagenet100'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:59:37.650608Z","iopub.execute_input":"2025-06-04T15:59:37.650918Z","iopub.status.idle":"2025-06-04T15:59:37.656300Z","shell.execute_reply.started":"2025-06-04T15:59:37.650892Z","shell.execute_reply":"2025-06-04T15:59:37.655581Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Preprocessing Steps","metadata":{}},{"cell_type":"code","source":"def collect_all_data(data_dir, json_mapping_path):\n    \"\"\"\n    Collect all image paths and labels in a deterministic way\n    \"\"\"\n    data_dir = (data_dir)\n    \n    # Load class mapping\n    with open(json_mapping_path, 'r') as f:\n        folder_to_class = json.load(f)\n    \n    # Create class to index mapping\n    classes = sorted(list(set(folder_to_class.values())))\n    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n    \n    # Collect all image paths and labels\n    all_image_paths = []\n    all_labels = []\n    directories = os.listdir(data_dir)[1:]\n\n    # Sort folder names for deterministic ordering\n    folder_names = []\n    folder_paths = []\n\n    for d in directories:\n        folder_names.extend([f for f in os.listdir(f'{data_dir}/{d}')\n                              if f in folder_to_class])\n        folder_paths.extend([os.path.join(f'{data_dir}/{d}', f) for f in os.listdir(f'{data_dir}/{d}')\n                              if f in folder_to_class])\n    # print(folder_names[0])\n    # print(folder_paths[0])\n    for i, folder_name in enumerate(folder_names):\n        if os.path.isdir(folder_paths[i]):\n            class_name = folder_to_class[folder_name]\n            class_idx = class_to_idx[class_name]\n            \n            # Get all image files and sort them for deterministic ordering\n            image_files = sorted([\n                os.path.join(folder_paths[i], img_file) for img_file in os.listdir(folder_paths[i])\n                if img_file.endswith('.JPEG')\n            ])\n            # print(image_files)\n            # for img_file in image_files:\n            all_image_paths.extend(image_files)\n            all_labels.extend([class_idx]*len(image_files))\n    \n    return all_image_paths, all_labels, classes, class_to_idx, folder_to_class\n# collect_all_data(root, f'{root}/Labels.json')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:45:52.226248Z","iopub.execute_input":"2025-06-04T15:45:52.226618Z","iopub.status.idle":"2025-06-04T15:45:52.233577Z","shell.execute_reply.started":"2025-06-04T15:45:52.226594Z","shell.execute_reply":"2025-06-04T15:45:52.232940Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# split data paths and labels\ndef split(all_image_paths, all_labels, test_size=0.2, val_size=0.1, random_state=42):\n    \"\"\"\n    Split data once to ensure mutual exclusivity\n    \"\"\"\n    # print(f\"Splitting: {all_image_paths[0]}\")\n    # First split: separate test set\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        all_image_paths, all_labels,\n        test_size=test_size,\n        stratify=all_labels,\n        random_state=random_state\n    )\n    \n    # Second split: separate train and validation from remaining data\n    val_size_adjusted = val_size / (1 - test_size)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp,\n        test_size=val_size_adjusted,\n        stratify=y_temp,\n        random_state=random_state\n    )\n    # print(X_train[0])\n    # Verify mutual exclusivity\n    train_set = set(X_train)\n    val_set = set(X_val)\n    test_set = set(X_test)\n    # print(train_set[0])\n    \n    assert len(train_set.intersection(val_set)) == 0, \"Train and validation sets overlap!\"\n    assert len(train_set.intersection(test_set)) == 0, \"Train and test sets overlap!\"\n    assert len(val_set.intersection(test_set)) == 0, \"Validation and test sets overlap!\"\n    \n    print(\"✓ Data splits verified as mutually exclusive\")\n    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n    \n    return {\n        'train': {'paths': X_train, 'labels': y_train},\n        'val': {'paths': X_val, 'labels': y_val},\n        'test': {'paths': X_test, 'labels': y_test}\n    }\n# img_path_splits = split(img_paths, labels)\n# img_path_splits['train']['paths'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:45:52.234304Z","iopub.execute_input":"2025-06-04T15:45:52.234981Z","iopub.status.idle":"2025-06-04T15:45:52.256863Z","shell.execute_reply.started":"2025-06-04T15:45:52.234960Z","shell.execute_reply":"2025-06-04T15:45:52.256167Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class imagenet100(Dataset):\n    def __init__(self, image_paths, labels, classes, classes_to_idx, transform = None, split=None):\n        self.transform = transform\n        self.image_paths = image_paths\n        self.labels = labels\n        self.classes = classes\n        self.classes_to_idx = classes_to_idx\n        self.num_classes = len(classes)\n        \n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:45:52.257499Z","iopub.execute_input":"2025-06-04T15:45:52.257752Z","iopub.status.idle":"2025-06-04T15:45:52.278114Z","shell.execute_reply.started":"2025-06-04T15:45:52.257727Z","shell.execute_reply":"2025-06-04T15:45:52.277527Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torchvision import transforms\n\ndef get_transforms(input_size):\n    return transforms.Compose([\n        transforms.Resize((input_size, input_size)),     # Match ResNet18 input\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:45:52.279798Z","iopub.execute_input":"2025-06-04T15:45:52.279990Z","iopub.status.idle":"2025-06-04T15:45:55.397478Z","shell.execute_reply.started":"2025-06-04T15:45:52.279975Z","shell.execute_reply":"2025-06-04T15:45:55.396710Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# train_size = int(0.8*len(dataset))\n# val_size = len(dataset) - train_size\n# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ndef create_data_loaders(data_dir, json_mapping, batch_size=64, num_workers=4, input_size=224):\n    img_paths, labels, classes, class_to_idx, folder_to_class = collect_all_data(data_dir, json_mapping)\n    # print(img_paths[0]) \n    splits = split(img_paths, labels)\n    transform = get_transforms(input_size)\n    print(splits['train']['paths'][0])\n    # Create datasets\n    train_dataset = imagenet100(\n        splits['train']['paths'], \n        splits['train']['labels'], \n        classes,\n        class_to_idx,\n        transform=transform, \n    )\n    \n    val_dataset = imagenet100(\n        splits['val']['paths'], \n        splits['val']['labels'], \n        classes,\n        class_to_idx,\n        transform=transform, \n    )\n    \n    test_dataset = imagenet100(\n        splits['test']['paths'], \n        splits['test']['labels'], \n        classes,\n        class_to_idx,\n        transform=transform, \n    )\n    # print(train_dataset.image_paths)\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=num_workers,\n        pin_memory=True  # Faster GPU transfer\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=num_workers,\n        pin_memory=True\n    )\n    print(f\"Dataset sizes:\")\n    print(f\"Train: {len(train_dataset)} images\")\n    print(f\"Validation: {len(val_dataset)} images\")\n    print(f\"Test: {len(test_dataset)} images\")\n    print(f\"Number of classes: {len(classes)}\")\n    \n    return train_loader, val_loader, test_loader, len(classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:45:55.398096Z","iopub.execute_input":"2025-06-04T15:45:55.398505Z","iopub.status.idle":"2025-06-04T15:45:55.405323Z","shell.execute_reply.started":"2025-06-04T15:45:55.398480Z","shell.execute_reply":"2025-06-04T15:45:55.404639Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"root = f\"/kaggle/input/imagenet100\"\ntrain_loader, val_loader, test_loader, num_classes = create_data_loaders(root, os.path.join(root, 'Labels.json'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:45:55.406043Z","iopub.execute_input":"2025-06-04T15:45:55.406307Z","iopub.status.idle":"2025-06-04T15:46:00.199609Z","shell.execute_reply.started":"2025-06-04T15:45:55.406281Z","shell.execute_reply":"2025-06-04T15:46:00.198967Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"✓ Data splits verified as mutually exclusive\nTrain: 94500, Val: 13500, Test: 27000\n/kaggle/input/imagenet100/train.X4/n01860187/n01860187_1819.JPEG\nDataset sizes:\nTrain: 94500 images\nValidation: 13500 images\nTest: 27000 images\nNumber of classes: 100\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"\\nTesting DataLoaders...\")\nfor batch_idx, (images, labels) in enumerate(train_loader):\n    print(f\"Batch {batch_idx}: Images shape: {images.shape}, Labels shape: {labels.shape}\")\n    print(f\"Label range: {labels.min().item()} to {labels.max().item()}\")\n    if batch_idx == 2:  # Just show first few batches\n        break\n\n\n# Show some class information\nprint(f\"\\nFirst 10 classes: {train_loader.dataset.classes[:10]}\")\nprint(f\"Class to index mapping (first 5): {dict(list(train_loader.dataset.classes_to_idx.items())[:5])}\")\nprint(f\"\\n first 10 image paths: {train_loader.dataset.image_paths[:10]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:46:00.200254Z","iopub.execute_input":"2025-06-04T15:46:00.200438Z","iopub.status.idle":"2025-06-04T15:46:02.799148Z","shell.execute_reply.started":"2025-06-04T15:46:00.200422Z","shell.execute_reply":"2025-06-04T15:46:02.798253Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"\nTesting DataLoaders...\nBatch 0: Images shape: torch.Size([64, 3, 224, 224]), Labels shape: torch.Size([64])\nLabel range: 0 to 99\nBatch 1: Images shape: torch.Size([64, 3, 224, 224]), Labels shape: torch.Size([64])\nLabel range: 0 to 97\nBatch 2: Images shape: torch.Size([64, 3, 224, 224]), Labels shape: torch.Size([64])\nLabel range: 4 to 91\n\nFirst 10 classes: ['American alligator, Alligator mississipiensis', 'American coot, marsh hen, mud hen, water hen, Fulica americana', 'Dungeness crab, Cancer magister', 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis', 'agama', 'albatross, mollymawk', 'axolotl, mud puppy, Ambystoma mexicanum', 'bald eagle, American eagle, Haliaeetus leucocephalus', 'banded gecko', 'barn spider, Araneus cavaticus']\nClass to index mapping (first 5): {'American alligator, Alligator mississipiensis': 0, 'American coot, marsh hen, mud hen, water hen, Fulica americana': 1, 'Dungeness crab, Cancer magister': 2, 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis': 3, 'agama': 4}\n\n first 10 image paths: ['/kaggle/input/imagenet100/train.X4/n01860187/n01860187_1819.JPEG', '/kaggle/input/imagenet100/train.X4/n01806143/n01806143_55893.JPEG', '/kaggle/input/imagenet100/train.X1/n01820546/n01820546_5742.JPEG', '/kaggle/input/imagenet100/train.X2/n01843383/n01843383_5766.JPEG', '/kaggle/input/imagenet100/train.X3/n01664065/n01664065_15929.JPEG', '/kaggle/input/imagenet100/train.X3/n01828970/n01828970_10882.JPEG', '/kaggle/input/imagenet100/val.X/n01824575/ILSVRC2012_val_00015739.JPEG', '/kaggle/input/imagenet100/train.X1/n01818515/n01818515_4408.JPEG', '/kaggle/input/imagenet100/train.X2/n02077923/n02077923_10374.JPEG', '/kaggle/input/imagenet100/train.X2/n01740131/n01740131_5731.JPEG']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# RESNET18","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"def conv3x3(in_channels, out_channels, stride, dilation=1):\n    return nn.Conv2d(\n        in_channels=in_channels, \n        out_channels=out_channels, \n        kernel_size=3, \n        stride=stride, \n        dilation=dilation, \n        padding=dilation, \n        bias=False,\n    )\n    \nclass block(nn.Module):\n    '''\n    Basic Block: 3x3 Conv -> Batch Norm 1 -> ReLU -> 3x3 Conv -> Batch Norm 2 -> += initial -> ReLU\n    '''\n    \n    def __init__(self, in_channels, out_channels, stride=1):\n        super(block, self).__init__()\n        self.conv1 = conv3x3(in_channels, out_channels, stride=stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.ReLU = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(out_channels, out_channels, stride=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # add another layer if channel\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    def forward(self, x):\n        \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.ReLU(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        # skip connection / identity matching\n        out += self.shortcut(x)\n        out = self.ReLU(out)\n        \n        return out\n\nclass resnet18(nn.Module):\n    def __init__(self, num_classes=100,stride=1):\n        super(resnet18, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(\n            in_channels=3, \n            out_channels=self.in_channels, \n            kernel_size=7, \n            stride=2, \n            padding=1, \n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.ReLU = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self.make_layer(64,  2, stride=1)\n        self.layer2 = self.make_layer(128, 2, stride=2)\n        self.layer3 = self.make_layer(256, 2, stride=2)\n        self.layer4 = self.make_layer(512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc = nn.Linear(512, num_classes)\n    \n        \n    def make_layer(self, out_channels, num_blocks, stride=1):\n        strides = [stride] + [1] * (num_blocks-1)\n        layers = []\n        for s in strides:\n            layers.append(block(self.in_channels, out_channels, stride=s))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.ReLU(x)\n        x = self.maxpool(x)\n\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        \n        return out\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:46:02.800473Z","iopub.execute_input":"2025-06-04T15:46:02.800816Z","iopub.status.idle":"2025-06-04T15:46:02.814399Z","shell.execute_reply.started":"2025-06-04T15:46:02.800778Z","shell.execute_reply":"2025-06-04T15:46:02.813650Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# CTM","metadata":{}},{"cell_type":"code","source":"def get_model_size(model):\n    # Count total parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    # Calculate memory usage (assumes float32 = 4 bytes)\n    total_size = sum(p.numel() * p.element_size() for p in model.parameters())\n    \n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    print(f\"Model size: {total_size / 1024**2:.2f} MB\")\n    print(f\"Model size: {total_size / 1024**3:.3f} GB\")\n    \n    return total_params, total_size\n\n# Usage\ntotal_params, model_size = get_model_size(resnet18())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:46:02.815146Z","iopub.execute_input":"2025-06-04T15:46:02.816064Z","iopub.status.idle":"2025-06-04T15:46:02.956546Z","shell.execute_reply.started":"2025-06-04T15:46:02.816041Z","shell.execute_reply":"2025-06-04T15:46:02.956004Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Total parameters: 11,227,812\nTrainable parameters: 11,227,812\nModel size: 42.83 MB\nModel size: 0.042 GB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import gc \n# Check current usage\"\ndef print_memory():\n    print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.0f} MB\")\n    print(f\"Reserved: {torch.cuda.memory_reserved()/1024**2:.0f} MB\")\n    # Get detailed memory info\n    print(torch.cuda.memory_summary())\n\ndef clean_memory():\n    # Check current usage\n    del model, optimizer, criterion  # Replace with your variable names\n    gc.collect()\n    torch.cuda.empty_cache()\nprint_memory()\nclean_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:46:02.957212Z","iopub.execute_input":"2025-06-04T15:46:02.957657Z","iopub.status.idle":"2025-06-04T15:46:02.998770Z","shell.execute_reply.started":"2025-06-04T15:46:02.957639Z","shell.execute_reply":"2025-06-04T15:46:02.997712Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Allocated: 0 MB\nReserved: 0 MB\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Allocations           |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize allocations  |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\n|===========================================================================|\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2436752901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclean_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/2436752901.py\u001b[0m in \u001b[0;36mclean_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Check current usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m  \u001b[0;31m# Replace with your variable names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'model' where it is not associated with a value"],"ename":"UnboundLocalError","evalue":"cannot access local variable 'model' where it is not associated with a value","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":"# TRain Model","metadata":{}},{"cell_type":"code","source":"def validate_model(model, val_loader, criterion, device):\n    \"\"\"\n    Comprehensive validation function\n    \"\"\"\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n            \n            # Forward pass\n            output = model(data)\n            loss = criterion(output, target)\n            \n            # Statistics\n            val_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            \n            # Store for detailed metrics\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n            \n            # Clean up memory\n            del data, target, output, loss\n    \n    # Calculate metrics\n    avg_loss = val_loss / len(val_loader)\n    accuracy = 100.0 * correct / total\n    \n    return avg_loss, accuracy, all_preds, all_targets\n\ndef train_epoch(model, train_loader, optimizer, criterion, device, \n                grad_clip_norm=1.0, print_interval=100):\n    \"\"\"\n    Train for one epoch with progress tracking\n    \"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images, labels = images.to('cuda', non_blocking=True), labels.to('cuda', non_blocking=True)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n        optimizer.step()\n        \n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(output.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        # Print progress\n        if batch_idx % print_interval == 0:\n            print(f'Batch {batch_idx}/{len(train_loader)}, '\n                  f'Loss: {loss.item():.4f}, '\n                  f'Acc: {100.*correct/total:.2f}%')\n        \n        # Memory cleanup\n        if batch_idx % 50 == 0:\n            torch.cuda.empty_cache()\n        \n        # del data, labels, output, loss\n    \n    avg_loss = running_loss / len(train_loader)\n    accuracy = 100.0 * correct / total\n    \n    return avg_loss, accuracy\n\nclass EarlyStopping:\n    \"\"\"\n    Early stopping to prevent overfitting\n    \"\"\"\n    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n        \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n            \n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n    \n    def save_checkpoint(self, model):\n        self.best_weights = copy.deepcopy(model.state_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:46:58.196291Z","iopub.execute_input":"2025-06-04T15:46:58.196948Z","iopub.status.idle":"2025-06-04T15:46:58.208081Z","shell.execute_reply.started":"2025-06-04T15:46:58.196926Z","shell.execute_reply":"2025-06-04T15:46:58.207373Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train(model, train_loader, val_loader, num_epochs=10, \n                          learning_rate=0.001, device='cuda', patience=10):\n    \"\"\"\n    Complete training loop with validation and early stopping\n    \"\"\"\n    # Setup training components\n    model.to(device)\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n    )\n    early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n    \n    # Training history\n    history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': [],\n        'learning_rates': []\n    }\n    \n    print(f\"Starting training for {num_epochs} epochs...\")\n    print(f\"Device: {device}\")\n    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n    print(\"-\" * 60)\n    \n    for epoch in range(num_epochs):\n        start_time = time.time()\n        \n        # Training phase\n        train_loss, train_acc = train_epoch(\n            model, train_loader, optimizer, criterion, device\n        )\n        \n        # Validation phase\n        val_loss, val_acc, val_preds, val_targets = validate_model(\n            model, val_loader, criterion, device\n        )\n        \n        # Learning rate scheduling\n        scheduler.step(val_loss)\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Store history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['learning_rates'].append(current_lr)\n        \n        # Print epoch results\n        epoch_time = time.time() - start_time\n        print(f\"\\nEpoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s)\")\n        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n        print(f\"LR: {current_lr:.6f}\")\n        \n        # Early stopping check\n        if early_stopping(val_loss, model):\n            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n            print(f\"Best validation loss: {early_stopping.best_loss:.4f}\")\n            break\n        \n        # Memory cleanup\n        torch.cuda.empty_cache()\n        print(\"-\" * 60)\n    \n    return history, model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T16:55:20.594075Z","iopub.execute_input":"2025-06-04T16:55:20.594452Z","iopub.status.idle":"2025-06-04T16:55:20.604199Z","shell.execute_reply.started":"2025-06-04T16:55:20.594424Z","shell.execute_reply":"2025-06-04T16:55:20.603366Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model = resnet18()\nhistory, model = train(model, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:59:44.527832Z","iopub.execute_input":"2025-06-04T15:59:44.528096Z","iopub.status.idle":"2025-06-04T16:55:20.591893Z","shell.execute_reply.started":"2025-06-04T15:59:44.528076Z","shell.execute_reply":"2025-06-04T16:55:20.590733Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Starting training for 10 epochs...\nDevice: cuda\nTrain batches: 1477, Val batches: 211\n------------------------------------------------------------\nBatch 0/1477, Loss: 4.6514, Acc: 0.00%\nBatch 100/1477, Loss: 4.5330, Acc: 3.00%\nBatch 200/1477, Loss: 4.5050, Acc: 3.97%\nBatch 300/1477, Loss: 4.2283, Acc: 4.68%\nBatch 400/1477, Loss: 3.9372, Acc: 5.15%\nBatch 500/1477, Loss: 4.1974, Acc: 5.74%\nBatch 600/1477, Loss: 4.0947, Acc: 6.25%\nBatch 700/1477, Loss: 3.7566, Acc: 7.08%\nBatch 800/1477, Loss: 3.7826, Acc: 7.87%\nBatch 900/1477, Loss: 3.8561, Acc: 8.70%\nBatch 1000/1477, Loss: 3.6027, Acc: 9.60%\nBatch 1100/1477, Loss: 3.5132, Acc: 10.48%\nBatch 1200/1477, Loss: 3.3873, Acc: 11.38%\nBatch 1300/1477, Loss: 3.4763, Acc: 12.26%\nBatch 1400/1477, Loss: 3.3101, Acc: 13.05%\n\nEpoch 1/10 (317.7s)\nTrain - Loss: 3.8596, Acc: 13.63%\nVal   - Loss: 3.4835, Acc: 21.24%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 3.1798, Acc: 32.81%\nBatch 100/1477, Loss: 3.1046, Acc: 26.50%\nBatch 200/1477, Loss: 3.2663, Acc: 27.20%\nBatch 300/1477, Loss: 3.5071, Acc: 27.75%\nBatch 400/1477, Loss: 3.2050, Acc: 28.32%\nBatch 500/1477, Loss: 3.2985, Acc: 28.81%\nBatch 600/1477, Loss: 2.9269, Acc: 29.21%\nBatch 700/1477, Loss: 3.0483, Acc: 29.69%\nBatch 800/1477, Loss: 2.8368, Acc: 30.25%\nBatch 900/1477, Loss: 2.9615, Acc: 30.75%\nBatch 1000/1477, Loss: 3.0455, Acc: 31.26%\nBatch 1100/1477, Loss: 2.9014, Acc: 31.70%\nBatch 1200/1477, Loss: 2.9499, Acc: 32.12%\nBatch 1300/1477, Loss: 2.5906, Acc: 32.46%\nBatch 1400/1477, Loss: 2.8730, Acc: 32.86%\n\nEpoch 2/10 (310.2s)\nTrain - Loss: 3.0226, Acc: 33.16%\nVal   - Loss: 2.9477, Acc: 35.58%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 2.7170, Acc: 45.31%\nBatch 100/1477, Loss: 3.1551, Acc: 41.04%\nBatch 200/1477, Loss: 2.5665, Acc: 41.37%\nBatch 300/1477, Loss: 3.1256, Acc: 41.81%\nBatch 400/1477, Loss: 2.5072, Acc: 41.95%\nBatch 500/1477, Loss: 2.3489, Acc: 42.12%\nBatch 600/1477, Loss: 2.5878, Acc: 42.39%\nBatch 700/1477, Loss: 2.5340, Acc: 42.62%\nBatch 800/1477, Loss: 2.4722, Acc: 42.83%\nBatch 900/1477, Loss: 2.8928, Acc: 43.05%\nBatch 1000/1477, Loss: 2.5256, Acc: 43.23%\nBatch 1100/1477, Loss: 2.6298, Acc: 43.43%\nBatch 1200/1477, Loss: 2.5410, Acc: 43.66%\nBatch 1300/1477, Loss: 2.5553, Acc: 43.89%\nBatch 1400/1477, Loss: 2.7475, Acc: 44.11%\n\nEpoch 3/10 (309.5s)\nTrain - Loss: 2.6276, Acc: 44.28%\nVal   - Loss: 2.5524, Acc: 46.59%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 2.3048, Acc: 56.25%\nBatch 100/1477, Loss: 2.4744, Acc: 51.81%\nBatch 200/1477, Loss: 2.3579, Acc: 51.52%\nBatch 300/1477, Loss: 2.5664, Acc: 51.84%\nBatch 400/1477, Loss: 2.2524, Acc: 51.74%\nBatch 500/1477, Loss: 2.3852, Acc: 51.58%\nBatch 600/1477, Loss: 2.6263, Acc: 51.65%\nBatch 700/1477, Loss: 2.7226, Acc: 51.73%\nBatch 800/1477, Loss: 2.2085, Acc: 51.97%\nBatch 900/1477, Loss: 2.3749, Acc: 52.15%\nBatch 1000/1477, Loss: 2.3721, Acc: 52.22%\nBatch 1100/1477, Loss: 2.1977, Acc: 52.41%\nBatch 1200/1477, Loss: 2.7620, Acc: 52.62%\nBatch 1300/1477, Loss: 2.3889, Acc: 52.60%\nBatch 1400/1477, Loss: 2.1577, Acc: 52.67%\n\nEpoch 4/10 (313.2s)\nTrain - Loss: 2.3409, Acc: 52.75%\nVal   - Loss: 2.4455, Acc: 49.80%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 2.0725, Acc: 67.19%\nBatch 100/1477, Loss: 2.3271, Acc: 59.61%\nBatch 200/1477, Loss: 2.2850, Acc: 59.55%\nBatch 300/1477, Loss: 2.2432, Acc: 59.70%\nBatch 400/1477, Loss: 1.9753, Acc: 59.86%\nBatch 500/1477, Loss: 2.2245, Acc: 59.86%\nBatch 600/1477, Loss: 2.0110, Acc: 59.76%\nBatch 700/1477, Loss: 2.2975, Acc: 59.60%\nBatch 800/1477, Loss: 2.0910, Acc: 59.74%\nBatch 900/1477, Loss: 2.0208, Acc: 59.76%\nBatch 1000/1477, Loss: 2.1291, Acc: 59.69%\nBatch 1100/1477, Loss: 2.2073, Acc: 59.67%\nBatch 1200/1477, Loss: 2.0344, Acc: 59.79%\nBatch 1300/1477, Loss: 1.8740, Acc: 59.80%\nBatch 1400/1477, Loss: 2.1811, Acc: 59.83%\n\nEpoch 5/10 (334.1s)\nTrain - Loss: 2.1055, Acc: 59.81%\nVal   - Loss: 2.2607, Acc: 55.46%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 1.7575, Acc: 71.88%\nBatch 100/1477, Loss: 1.9806, Acc: 67.02%\nBatch 200/1477, Loss: 1.7321, Acc: 67.04%\nBatch 300/1477, Loss: 1.8782, Acc: 66.31%\nBatch 400/1477, Loss: 2.2690, Acc: 66.44%\nBatch 500/1477, Loss: 1.8635, Acc: 66.45%\nBatch 600/1477, Loss: 1.9198, Acc: 66.43%\nBatch 700/1477, Loss: 1.9990, Acc: 66.53%\nBatch 800/1477, Loss: 2.0245, Acc: 66.50%\nBatch 900/1477, Loss: 1.9488, Acc: 66.53%\nBatch 1000/1477, Loss: 1.7208, Acc: 66.62%\nBatch 1100/1477, Loss: 1.8000, Acc: 66.60%\nBatch 1200/1477, Loss: 1.8672, Acc: 66.64%\nBatch 1300/1477, Loss: 1.8953, Acc: 66.55%\nBatch 1400/1477, Loss: 1.9376, Acc: 66.53%\n\nEpoch 6/10 (320.3s)\nTrain - Loss: 1.8969, Acc: 66.52%\nVal   - Loss: 2.3180, Acc: 53.79%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 1.5708, Acc: 82.81%\nBatch 100/1477, Loss: 1.7062, Acc: 75.22%\nBatch 200/1477, Loss: 1.7492, Acc: 74.71%\nBatch 300/1477, Loss: 1.9767, Acc: 74.48%\nBatch 400/1477, Loss: 1.7230, Acc: 74.31%\nBatch 500/1477, Loss: 1.9716, Acc: 74.01%\nBatch 600/1477, Loss: 1.7240, Acc: 73.95%\nBatch 700/1477, Loss: 1.6840, Acc: 73.83%\nBatch 800/1477, Loss: 1.7896, Acc: 73.77%\nBatch 900/1477, Loss: 1.7895, Acc: 73.64%\nBatch 1000/1477, Loss: 1.8309, Acc: 73.41%\nBatch 1100/1477, Loss: 1.5288, Acc: 73.28%\nBatch 1200/1477, Loss: 1.7584, Acc: 73.23%\nBatch 1300/1477, Loss: 1.8052, Acc: 73.22%\nBatch 1400/1477, Loss: 1.7773, Acc: 73.18%\n\nEpoch 7/10 (321.9s)\nTrain - Loss: 1.6959, Acc: 73.10%\nVal   - Loss: 2.1774, Acc: 58.09%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 1.5662, Acc: 81.25%\nBatch 100/1477, Loss: 1.5248, Acc: 81.65%\nBatch 200/1477, Loss: 1.4674, Acc: 82.00%\nBatch 300/1477, Loss: 1.4246, Acc: 81.72%\nBatch 400/1477, Loss: 1.5430, Acc: 81.50%\nBatch 500/1477, Loss: 1.3961, Acc: 81.24%\nBatch 600/1477, Loss: 1.4139, Acc: 80.92%\nBatch 700/1477, Loss: 1.4269, Acc: 80.92%\nBatch 800/1477, Loss: 1.6073, Acc: 80.76%\nBatch 900/1477, Loss: 1.5097, Acc: 80.69%\nBatch 1000/1477, Loss: 1.4093, Acc: 80.49%\nBatch 1100/1477, Loss: 1.3940, Acc: 80.36%\nBatch 1200/1477, Loss: 1.5256, Acc: 80.22%\nBatch 1300/1477, Loss: 1.3095, Acc: 80.13%\nBatch 1400/1477, Loss: 1.3929, Acc: 80.06%\n\nEpoch 8/10 (360.1s)\nTrain - Loss: 1.5016, Acc: 79.94%\nVal   - Loss: 2.1653, Acc: 59.29%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 1.3287, Acc: 84.38%\nBatch 100/1477, Loss: 1.2614, Acc: 88.91%\nBatch 200/1477, Loss: 1.1981, Acc: 88.88%\nBatch 300/1477, Loss: 1.2998, Acc: 88.73%\nBatch 400/1477, Loss: 1.2731, Acc: 88.42%\nBatch 500/1477, Loss: 1.4324, Acc: 88.22%\nBatch 600/1477, Loss: 1.3106, Acc: 88.03%\nBatch 700/1477, Loss: 1.4978, Acc: 87.78%\nBatch 800/1477, Loss: 1.2517, Acc: 87.51%\nBatch 900/1477, Loss: 1.4038, Acc: 87.37%\nBatch 1000/1477, Loss: 1.3353, Acc: 87.30%\nBatch 1100/1477, Loss: 1.2442, Acc: 87.14%\nBatch 1200/1477, Loss: 1.3803, Acc: 86.99%\nBatch 1300/1477, Loss: 1.3713, Acc: 86.86%\nBatch 1400/1477, Loss: 1.3394, Acc: 86.78%\n\nEpoch 9/10 (374.8s)\nTrain - Loss: 1.3195, Acc: 86.69%\nVal   - Loss: 2.2438, Acc: 57.41%\nLR: 0.001000\n------------------------------------------------------------\nBatch 0/1477, Loss: 1.2593, Acc: 87.50%\nBatch 100/1477, Loss: 1.1336, Acc: 93.80%\nBatch 200/1477, Loss: 1.0531, Acc: 93.92%\nBatch 300/1477, Loss: 1.2547, Acc: 93.45%\nBatch 400/1477, Loss: 1.1067, Acc: 93.43%\nBatch 500/1477, Loss: 1.1462, Acc: 93.25%\nBatch 600/1477, Loss: 1.1897, Acc: 93.19%\nBatch 700/1477, Loss: 1.2261, Acc: 92.97%\nBatch 800/1477, Loss: 1.1311, Acc: 92.84%\nBatch 900/1477, Loss: 1.2313, Acc: 92.62%\nBatch 1000/1477, Loss: 1.2096, Acc: 92.53%\nBatch 1100/1477, Loss: 1.1151, Acc: 92.38%\nBatch 1200/1477, Loss: 1.1387, Acc: 92.25%\nBatch 1300/1477, Loss: 1.2945, Acc: 92.09%\nBatch 1400/1477, Loss: 1.2732, Acc: 91.97%\n\nEpoch 10/10 (373.6s)\nTrain - Loss: 1.1838, Acc: 91.88%\nVal   - Loss: 2.2770, Acc: 57.36%\nLR: 0.001000\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"model.state_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:23:31.346850Z","iopub.execute_input":"2025-06-04T17:23:31.347152Z","iopub.status.idle":"2025-06-04T17:23:31.684027Z","shell.execute_reply.started":"2025-06-04T17:23:31.347132Z","shell.execute_reply":"2025-06-04T17:23:31.682942Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"OrderedDict([('conv1.weight',\n              tensor([[[[-1.3664e-01, -1.1040e-01,  4.4606e-02,  ..., -2.2538e-03,\n                         -1.1823e-01, -6.9003e-02],\n                        [-6.4455e-02, -8.4982e-02,  3.1349e-02,  ...,  4.8349e-02,\n                         -6.6766e-02, -2.9746e-02],\n                        [ 3.5980e-02, -8.1370e-02, -1.0576e-01,  ...,  7.6561e-02,\n                          1.5189e-03,  2.1370e-02],\n                        ...,\n                        [-1.1154e-01,  7.2375e-04, -4.4782e-02,  ...,  1.1009e-01,\n                          6.2710e-02, -8.9515e-02],\n                        [-1.2802e-01, -7.9691e-02, -5.4215e-02,  ...,  1.5370e-01,\n                          1.2523e-01, -3.6549e-02],\n                        [-3.5150e-03, -3.2835e-02, -3.2212e-02,  ..., -1.2887e-02,\n                         -1.0561e-02, -1.2962e-02]],\n              \n                       [[-2.0618e-01, -1.1127e-01,  4.7792e-02,  ..., -1.1628e-01,\n                         -1.4385e-01, -1.4994e-01],\n                        [-3.1076e-02,  9.0520e-03,  3.0306e-02,  ..., -1.4578e-03,\n                         -4.1564e-02, -8.5869e-02],\n                        [ 4.4545e-02, -8.4916e-03, -6.2345e-02,  ...,  2.3826e-01,\n                         -2.3899e-02,  4.0100e-02],\n                        ...,\n                        [ 6.8890e-04, -7.0857e-03,  1.6905e-01,  ...,  3.3457e-01,\n                          2.4029e-01,  4.1699e-02],\n                        [-1.0631e-02, -1.3499e-01,  8.6120e-02,  ...,  3.3299e-01,\n                          2.4702e-01, -2.0478e-02],\n                        [-1.7821e-01, -1.0699e-01, -1.2892e-01,  ...,  1.3225e-01,\n                          7.7064e-02, -1.3618e-02]],\n              \n                       [[ 6.6892e-02,  1.1334e-01,  7.1387e-02,  ...,  1.0753e-01,\n                          1.2616e-01,  1.8565e-01],\n                        [ 1.2570e-01,  1.1580e-01,  1.4135e-02,  ..., -7.5041e-03,\n                          3.7523e-02,  1.4420e-01],\n                        [ 1.6640e-01,  4.5916e-02, -3.2565e-02,  ..., -2.3259e-02,\n                         -4.4184e-02,  1.8438e-01],\n                        ...,\n                        [ 6.5040e-02, -4.7501e-02, -6.5819e-02,  ..., -1.4414e-01,\n                         -1.3546e-01, -1.9660e-02],\n                        [ 1.3073e-01,  1.6276e-02, -1.4603e-01,  ..., -1.2180e-01,\n                         -1.3114e-01, -1.1754e-02],\n                        [ 7.8160e-02,  1.0122e-01, -7.8502e-02,  ..., -8.1116e-02,\n                         -1.3298e-01, -6.5054e-02]]],\n              \n              \n                      [[[ 1.8487e-01,  7.6203e-02,  1.8625e-02,  ...,  1.6135e-02,\n                          2.0441e-02,  1.3546e-01],\n                        [ 1.0847e-01, -1.5784e-03, -3.7655e-02,  ...,  1.2298e-02,\n                         -1.0757e-01,  5.9187e-03],\n                        [-4.4716e-02, -9.6210e-02, -9.0997e-02,  ..., -1.5724e-01,\n                         -1.4252e-01, -3.9917e-02],\n                        ...,\n                        [-1.5688e-01, -6.8969e-02, -1.8372e-01,  ..., -3.0263e-01,\n                         -2.2716e-01, -1.6670e-01],\n                        [-1.3645e-01, -5.3995e-02, -1.0833e-01,  ..., -1.1851e-01,\n                         -8.3138e-02, -1.6361e-01],\n                        [-1.3921e-02, -4.6350e-02, -5.0611e-02,  ..., -1.8212e-01,\n                         -9.0754e-03, -3.2684e-02]],\n              \n                       [[ 9.4362e-02,  1.6227e-01,  7.4138e-02,  ...,  7.2286e-02,\n                          8.7154e-03,  9.0444e-02],\n                        [ 8.0241e-02,  7.5042e-02,  9.1352e-02,  ...,  7.3027e-02,\n                         -2.4786e-02,  4.7523e-02],\n                        [-2.6145e-02, -7.5815e-03,  3.3051e-02,  ...,  9.1381e-02,\n                         -4.1984e-02, -1.5564e-03],\n                        ...,\n                        [ 5.8244e-02,  3.7481e-02, -1.1088e-02,  ...,  7.4728e-02,\n                          1.1725e-01,  3.9313e-02],\n                        [ 1.0005e-02,  1.0622e-01,  6.2954e-02,  ...,  1.7285e-03,\n                          3.4848e-02, -2.6288e-03],\n                        [ 3.8741e-02,  3.5990e-02, -3.7430e-02,  ...,  7.7337e-02,\n                          1.5586e-02,  8.3858e-02]],\n              \n                       [[-4.3706e-02,  1.4862e-02, -1.0343e-01,  ..., -2.6251e-02,\n                          5.1934e-04,  7.9268e-02],\n                        [-3.2688e-02, -1.6510e-01, -9.0458e-02,  ..., -1.7524e-01,\n                         -1.6566e-01,  2.3073e-02],\n                        [-2.2262e-01, -1.4089e-01, -2.4011e-01,  ..., -1.3640e-01,\n                         -1.5036e-01, -6.6807e-02],\n                        ...,\n                        [-9.4180e-02, -9.7918e-02, -1.7524e-01,  ..., -1.9398e-01,\n                         -5.3936e-02, -9.1042e-02],\n                        [-7.5063e-02, -1.2055e-01, -3.1290e-02,  ..., -1.3829e-01,\n                         -7.2951e-02, -6.7296e-02],\n                        [ 3.2358e-02, -7.3061e-02, -1.1892e-01,  ..., -1.0745e-01,\n                         -6.8206e-02,  1.0024e-01]]],\n              \n              \n                      [[[-1.3967e-01, -8.9363e-02, -9.0827e-02,  ..., -3.5513e-03,\n                         -3.1362e-02, -1.3870e-02],\n                        [-3.7747e-02, -3.3700e-02, -7.1902e-02,  ..., -1.8614e-02,\n                          2.1912e-02, -4.9523e-02],\n                        [ 1.6753e-02,  5.2374e-02,  3.1895e-02,  ...,  5.9753e-03,\n                         -5.7037e-02, -5.7485e-02],\n                        ...,\n                        [-3.1474e-03,  1.0145e-01,  4.1444e-02,  ...,  2.9076e-02,\n                          5.6876e-02,  7.2771e-02],\n                        [ 4.3199e-02, -4.2466e-03, -2.0141e-02,  ...,  3.6610e-02,\n                          7.9482e-02,  7.1116e-02],\n                        [-1.5072e-02, -1.4312e-02, -1.0665e-02,  ...,  4.3526e-02,\n                          9.4977e-02,  2.5255e-02]],\n              \n                       [[ 1.5693e-01,  1.1691e-01,  1.9403e-01,  ...,  9.7831e-02,\n                          2.0444e-02, -9.8279e-02],\n                        [ 1.3809e-01,  2.2453e-01,  3.4651e-01,  ...,  1.1747e-01,\n                          5.9740e-02, -4.8276e-02],\n                        [ 6.7396e-02,  1.9417e-01,  2.7375e-01,  ..., -9.1673e-03,\n                          2.3197e-04, -7.7650e-02],\n                        ...,\n                        [ 2.3744e-02, -6.3066e-02, -7.9786e-02,  ..., -1.7080e-01,\n                         -1.4231e-01, -9.7476e-02],\n                        [-1.1709e-01, -1.7430e-01, -2.0819e-01,  ..., -1.7659e-01,\n                         -1.1676e-01, -4.4046e-02],\n                        [-1.8773e-01, -1.8516e-01, -2.5207e-01,  ..., -1.4986e-01,\n                         -7.5162e-02, -2.3805e-02]],\n              \n                       [[ 9.2887e-02,  1.8559e-01,  1.8805e-01,  ...,  2.7945e-02,\n                          4.7107e-03, -6.7905e-02],\n                        [ 1.1984e-01,  2.8060e-01,  2.8881e-01,  ...,  3.8581e-02,\n                         -8.4692e-02, -6.5393e-02],\n                        [ 1.2447e-01,  2.8678e-01,  3.5054e-01,  ...,  5.2347e-02,\n                         -9.9273e-02, -1.7845e-01],\n                        ...,\n                        [-3.8247e-02, -5.6094e-02, -5.6207e-03,  ..., -1.3648e-01,\n                         -8.3025e-02, -1.0539e-02],\n                        [-1.5569e-01, -1.5035e-01, -1.7424e-01,  ..., -1.4574e-01,\n                         -3.3151e-02, -6.0729e-02],\n                        [-2.0479e-01, -2.1506e-01, -2.3153e-01,  ..., -8.8875e-02,\n                         -5.5244e-02, -4.6566e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.7607e-02, -3.8864e-02, -4.4215e-02,  ..., -1.3662e-01,\n                         -1.0609e-01, -1.8645e-01],\n                        [ 4.9648e-02,  8.8793e-03, -5.5724e-02,  ..., -1.0622e-01,\n                         -4.4503e-02, -4.1881e-02],\n                        [-2.1298e-02, -1.1496e-01, -1.0719e-01,  ..., -1.3558e-01,\n                         -3.5432e-02, -5.9153e-02],\n                        ...,\n                        [-1.0584e-03, -9.4294e-02, -1.0294e-01,  ..., -6.1416e-02,\n                         -1.2010e-01, -7.2212e-02],\n                        [-1.6823e-01, -1.1356e-01, -3.2729e-02,  ..., -8.2511e-02,\n                         -5.4592e-02, -1.2415e-02],\n                        [-5.9596e-02, -7.4581e-02, -9.7265e-02,  ...,  1.8891e-02,\n                         -2.4164e-02, -5.1339e-02]],\n              \n                       [[ 1.1688e-01,  3.3548e-02,  3.0430e-02,  ..., -1.6149e-02,\n                         -7.3078e-02, -1.2112e-01],\n                        [ 4.3245e-02, -6.3399e-03,  1.5424e-02,  ...,  3.2620e-03,\n                          1.5020e-02, -3.7126e-02],\n                        [ 1.1627e-02,  5.7100e-02,  5.6614e-03,  ..., -6.9935e-02,\n                         -7.1688e-02, -7.6932e-02],\n                        ...,\n                        [-3.4020e-02, -5.6280e-02,  3.4418e-02,  ..., -2.6842e-02,\n                          2.9778e-02, -4.5539e-02],\n                        [-6.3433e-02, -2.8645e-02, -1.0431e-01,  ...,  1.2148e-03,\n                         -4.1607e-02, -1.7027e-02],\n                        [-1.1158e-01, -6.5143e-02, -6.3365e-02,  ..., -1.0298e-02,\n                          4.9589e-02, -1.2170e-03]],\n              \n                       [[ 2.6416e-01,  1.7385e-01,  1.7682e-01,  ...,  7.3701e-02,\n                          1.0396e-02, -1.2331e-02],\n                        [ 2.8499e-01,  1.2175e-01,  1.3644e-01,  ...,  1.3521e-01,\n                          7.7428e-02,  1.0140e-01],\n                        [ 2.5775e-01,  1.6507e-01,  1.3566e-01,  ...,  2.0026e-01,\n                          2.0269e-01,  1.5358e-01],\n                        ...,\n                        [ 1.0147e-01,  1.3305e-01,  1.8490e-01,  ...,  2.5533e-01,\n                          2.6645e-01,  2.3114e-01],\n                        [ 8.9196e-02,  7.4421e-02,  1.7713e-01,  ...,  2.7263e-01,\n                          2.3327e-01,  2.1322e-01],\n                        [ 1.0625e-01,  9.5025e-02,  1.8765e-01,  ...,  3.4112e-01,\n                          1.8925e-01,  2.4436e-01]]],\n              \n              \n                      [[[ 1.2219e-01,  1.7963e-01,  2.4085e-01,  ...,  1.3230e-01,\n                          1.2327e-01, -9.3376e-02],\n                        [ 1.3397e-01,  2.6156e-01,  2.8729e-01,  ...,  2.8127e-01,\n                          1.9378e-01,  2.9403e-02],\n                        [ 1.2847e-01,  1.9130e-01,  1.9091e-01,  ...,  1.8548e-01,\n                          2.3541e-01, -2.8211e-02],\n                        ...,\n                        [ 1.2830e-01,  2.4704e-01,  2.9925e-01,  ...,  2.5596e-01,\n                          2.6278e-01,  7.9166e-02],\n                        [ 1.4612e-01,  7.8504e-02,  1.7059e-01,  ...,  2.2312e-01,\n                          1.6743e-01,  6.3702e-02],\n                        [ 1.4804e-01,  1.3913e-01,  1.1734e-02,  ...,  1.7340e-01,\n                          1.8674e-01,  2.6110e-02]],\n              \n                       [[ 8.9033e-02,  1.5583e-01,  1.4475e-01,  ...,  1.2183e-01,\n                          1.0176e-01, -2.8757e-02],\n                        [ 9.9428e-02,  1.2894e-01,  1.4705e-01,  ...,  2.3966e-01,\n                          1.3439e-01,  2.7574e-02],\n                        [-7.1591e-02,  5.0468e-02,  2.6198e-02,  ...,  2.9394e-02,\n                          9.1429e-02,  3.0122e-02],\n                        ...,\n                        [ 3.5844e-02, -4.7384e-02, -1.5535e-02,  ..., -1.4322e-02,\n                         -2.0495e-02,  2.3397e-02],\n                        [-5.6201e-02,  1.6741e-02, -7.2494e-02,  ...,  3.0896e-03,\n                          8.1526e-03, -3.2814e-02],\n                        [ 3.7955e-03, -1.4907e-02, -6.6348e-02,  ...,  2.8056e-02,\n                          5.9662e-02, -4.0449e-02]],\n              \n                       [[ 6.2058e-02,  1.1501e-01,  1.4421e-01,  ...,  6.4546e-02,\n                          3.4566e-02,  3.1895e-02],\n                        [ 1.1537e-01,  9.1797e-02,  4.3087e-02,  ...,  1.4339e-01,\n                          1.3602e-02, -1.3670e-02],\n                        [ 7.3063e-03,  5.6302e-02,  1.1268e-01,  ...,  7.8165e-02,\n                          5.3522e-02,  5.6766e-02],\n                        ...,\n                        [ 2.6749e-02, -9.9484e-03,  7.8969e-02,  ...,  2.7349e-02,\n                          5.6701e-02, -1.3182e-02],\n                        [ 3.6910e-02, -4.8712e-02, -4.7603e-02,  ...,  7.3389e-02,\n                         -4.9478e-02, -6.2727e-03],\n                        [ 3.7555e-02,  3.5119e-02, -5.5768e-02,  ..., -4.2685e-02,\n                          2.0560e-02, -7.6217e-02]]],\n              \n              \n                      [[[ 5.0851e-02,  8.2833e-02,  7.4112e-02,  ...,  4.9510e-02,\n                          6.5306e-02,  3.0253e-02],\n                        [ 4.7982e-02, -8.7482e-03, -3.6065e-03,  ..., -2.4423e-02,\n                          3.7677e-02,  2.9993e-02],\n                        [ 2.1802e-02,  9.3011e-02,  9.0748e-02,  ...,  6.7427e-02,\n                         -4.0336e-02, -9.2665e-03],\n                        ...,\n                        [-5.0116e-02,  2.9582e-02, -6.2400e-02,  ..., -7.4182e-02,\n                         -1.3710e-02, -8.7693e-02],\n                        [-4.5493e-02, -7.4732e-02, -5.5965e-03,  ...,  1.2251e-03,\n                          6.2707e-03,  2.2994e-02],\n                        [-6.3846e-02, -4.4563e-02, -3.4332e-02,  ..., -6.5041e-02,\n                         -3.1288e-02, -1.9055e-02]],\n              \n                       [[-5.5996e-02, -1.7267e-01, -1.3507e-01,  ..., -1.8846e-01,\n                         -1.1268e-01,  4.4332e-03],\n                        [-6.1899e-02, -7.5034e-02, -1.1297e-01,  ..., -4.0185e-02,\n                         -6.8876e-02,  1.1291e-02],\n                        [ 2.8496e-02,  2.3324e-02,  9.5563e-02,  ...,  9.0012e-02,\n                          1.1939e-01,  9.1832e-02],\n                        ...,\n                        [ 1.7000e-02, -2.7271e-03,  8.1600e-02,  ..., -1.0926e-02,\n                         -4.6519e-02,  8.4143e-03],\n                        [-1.8751e-02,  1.5409e-02,  1.2744e-02,  ..., -4.4505e-02,\n                         -5.9671e-02,  2.3751e-02],\n                        [ 3.4063e-03, -2.0719e-02, -1.0904e-01,  ..., -8.5138e-02,\n                         -1.4743e-01, -3.2738e-02]],\n              \n                       [[-2.1865e-01, -3.0810e-01, -4.3999e-01,  ..., -2.9933e-01,\n                         -2.5002e-01, -1.7783e-01],\n                        [-5.9742e-02, -7.2526e-02, -1.3164e-01,  ..., -2.1319e-01,\n                         -1.6307e-01, -1.9769e-01],\n                        [-5.5374e-02,  1.0131e-01,  9.3648e-02,  ...,  5.7575e-02,\n                          2.7426e-02, -3.8794e-02],\n                        ...,\n                        [ 6.5113e-02,  1.0626e-01,  1.8497e-01,  ...,  3.3213e-01,\n                          2.8130e-01,  1.8244e-01],\n                        [ 4.4765e-02,  5.1066e-02,  8.7647e-02,  ...,  1.8928e-01,\n                          1.9541e-01,  1.0225e-01],\n                        [-8.3371e-03, -2.4692e-02, -9.8791e-02,  ..., -5.2449e-02,\n                          3.6251e-02, -8.2814e-02]]]])),\n             ('bn1.weight',\n              tensor([0.8048, 0.8766, 0.9255, 1.1131, 0.4383, 1.2965, 1.3181, 0.6532, 0.9673,\n                      0.7890, 1.0022, 1.0690, 0.9214, 0.8572, 0.5059, 0.9758, 0.8470, 0.6844,\n                      0.5801, 0.7680, 0.7174, 0.7959, 0.5659, 0.7768, 0.7923, 0.6792, 0.9196,\n                      0.5458, 0.7764, 1.0006, 0.7504, 1.3361, 0.6035, 0.8304, 0.8046, 1.0432,\n                      1.1768, 0.7731, 0.7618, 1.0483, 0.4836, 0.8555, 0.8156, 0.8106, 0.9586,\n                      0.7872, 1.2215, 1.1297, 0.7937, 1.2776, 0.6990, 1.0984, 1.3842, 0.8621,\n                      0.6761, 1.3016, 0.9460, 1.1418, 1.0090, 0.6417, 0.6886, 0.6575, 0.6970,\n                      1.0600])),\n             ('bn1.bias',\n              tensor([ 0.1884,  0.0868,  0.1494,  0.3914, -0.1776,  0.6268,  0.3111, -0.1935,\n                       0.4296,  0.2726,  0.1952,  0.4029,  0.0912,  0.0590, -0.3200,  0.0033,\n                       0.5027,  0.1709, -0.0930, -0.1831,  0.3031, -0.0785,  0.0662,  0.1734,\n                      -0.1130,  0.0561,  0.1385, -0.3156,  0.0573,  0.1948, -0.1362,  0.5553,\n                      -0.1164,  0.1275,  0.3832,  0.1916,  0.4144,  0.0367, -0.2531,  0.6201,\n                      -0.2752,  0.1706, -0.3796,  0.0667,  0.0668,  0.0846,  0.6648,  0.3478,\n                      -0.3181,  0.2882, -0.1701,  0.6736,  0.4543,  0.3789, -0.0515,  0.4045,\n                       0.2153,  0.1310,  0.8904,  0.0016,  0.1860, -0.0046, -0.0488,  0.4528])),\n             ('bn1.running_mean',\n              tensor([ 0.0365,  0.8282, -0.0253, -0.0496,  0.1717, -0.0193,  0.0904, -0.5373,\n                       0.0202, -0.4232,  0.0023, -0.1674,  0.2528, -0.6169, -0.1526, -0.0360,\n                      -0.5692,  0.1565, -0.7239, -0.1799,  0.5450,  0.4235,  0.2928, -0.1877,\n                      -0.0616,  0.1873, -0.0272,  0.9262, -0.9653, -0.0843,  0.1102,  0.0043,\n                      -0.1490, -0.1611, -0.3115,  0.2481, -0.2499,  0.3958,  1.6381, -0.0084,\n                      -0.4266,  0.2052,  0.8283, -0.0873,  0.1084, -0.0782,  0.0125,  0.0408,\n                       0.2416,  0.0225, -0.0717,  0.0150, -0.0059,  0.0430, -0.0421, -0.1207,\n                      -1.0001,  0.0067,  0.2272,  0.4091,  0.9246, -0.1430, -1.1372, -0.0221])),\n             ('bn1.running_var',\n              tensor([  5.5709,  57.4694,   4.1514,   0.2951,   9.0850,   1.2115,   6.7606,\n                       65.6222,   3.0338,   4.1824,   8.1447,  26.4818,  15.1172,   9.7319,\n                      136.3971,   8.8940,  13.1565,  24.1766, 170.1071,  37.8702,   4.8704,\n                       22.5088,  18.7373,   5.0906,   3.3896,  16.7893,   1.8320,  65.7883,\n                       45.7767,  19.4512,   3.3583,   0.3929,   7.0416,   4.5065,  11.8802,\n                       24.3767,   2.2502,  14.1179, 214.8059,   0.3072, 111.4899,   6.1346,\n                       97.1771,  23.5829,   7.7794,   6.1025,   6.9175,   6.3067,  28.4497,\n                        0.6024,  69.9098,   0.3423,   0.6549,   1.4109,  45.7336,   0.5034,\n                       45.3901,   3.6447,   1.2899,  24.8495, 191.8462,  48.9999, 147.7495,\n                        2.0034])),\n             ('bn1.num_batches_tracked', tensor(14770)),\n             ('layer1.0.conv1.weight',\n              tensor([[[[ 1.5932e-02,  1.4703e-02,  7.0459e-03],\n                        [-4.6670e-02, -4.2426e-02, -1.1847e-02],\n                        [-9.1687e-03, -3.0136e-02, -3.4535e-02]],\n              \n                       [[-4.0163e-02,  6.0191e-02, -1.0142e-02],\n                        [ 4.3383e-02,  1.3762e-01,  6.8683e-02],\n                        [ 3.6489e-02,  1.6727e-01,  1.4460e-01]],\n              \n                       [[ 1.1799e-01,  1.6432e-01,  1.3088e-01],\n                        [ 1.5040e-01,  1.0236e-01,  1.1394e-01],\n                        [ 8.6033e-02,  5.2319e-02,  2.7007e-03]],\n              \n                       ...,\n              \n                       [[-4.9651e-02, -8.1087e-02, -8.2045e-02],\n                        [-5.3107e-02, -1.3335e-01, -1.1690e-01],\n                        [-8.1636e-02, -1.5074e-01, -1.5248e-01]],\n              \n                       [[-3.6387e-02,  2.6610e-04, -3.4611e-02],\n                        [-4.2168e-02, -6.8555e-02, -2.8067e-02],\n                        [-4.4878e-02, -5.9555e-02, -1.0168e-01]],\n              \n                       [[-1.3525e-01, -1.3148e-01, -1.3150e-01],\n                        [-1.4306e-02,  2.6950e-02,  6.5298e-02],\n                        [ 3.3834e-02,  1.8035e-01,  2.0259e-01]]],\n              \n              \n                      [[[-9.7786e-02, -5.9651e-03, -9.2850e-02],\n                        [-6.5817e-03,  2.5849e-02, -4.2166e-02],\n                        [-1.9904e-02,  1.5689e-02, -3.9495e-02]],\n              \n                       [[ 1.0798e-01,  1.6382e-01,  1.0907e-01],\n                        [ 1.8024e-01,  2.2317e-01,  6.7135e-02],\n                        [ 1.2955e-01,  1.8146e-01,  8.6363e-02]],\n              \n                       [[ 2.4515e-02,  8.9160e-02,  3.5953e-02],\n                        [ 5.9984e-02,  1.1627e-01,  1.0008e-01],\n                        [ 1.2799e-01,  1.9233e-01,  1.9923e-01]],\n              \n                       ...,\n              \n                       [[-1.9002e-02,  1.8815e-02,  2.6952e-03],\n                        [-5.3140e-02, -5.0255e-02, -3.0200e-02],\n                        [-4.1310e-02, -6.9769e-02, -7.1527e-02]],\n              \n                       [[-1.2255e-01, -7.1733e-02, -1.3666e-01],\n                        [-8.5314e-02, -4.8288e-03, -7.8252e-02],\n                        [-5.0343e-02,  2.7065e-03, -7.9832e-03]],\n              \n                       [[ 2.3305e-01,  2.1044e-01,  1.3424e-01],\n                        [ 1.8632e-01,  2.0135e-01,  1.4261e-01],\n                        [ 1.4116e-01,  1.0556e-01,  5.9602e-02]]],\n              \n              \n                      [[[ 6.6109e-02,  5.7923e-02,  6.8523e-02],\n                        [ 4.0383e-02,  1.4832e-02,  3.2816e-02],\n                        [ 1.2155e-01,  1.1883e-01,  1.3661e-01]],\n              \n                       [[-5.3494e-02, -1.8677e-01, -5.5333e-02],\n                        [-3.9897e-02, -2.3358e-01, -1.0452e-01],\n                        [-5.4487e-03, -8.3409e-02, -5.4804e-02]],\n              \n                       [[ 6.9089e-02,  6.3364e-03, -5.5705e-02],\n                        [ 1.0395e-01, -2.2470e-02, -7.9453e-02],\n                        [ 7.1005e-02,  3.2476e-02, -3.3356e-02]],\n              \n                       ...,\n              \n                       [[ 5.0088e-02,  5.0340e-02,  2.1945e-02],\n                        [ 5.9674e-02,  2.6523e-02, -5.7562e-03],\n                        [ 4.7896e-02, -8.7205e-03, -4.1830e-02]],\n              \n                       [[ 1.0188e-01,  1.2518e-01,  6.4025e-02],\n                        [ 8.5759e-02,  1.3179e-01,  1.2492e-01],\n                        [ 7.9414e-02,  8.2971e-02,  6.2314e-02]],\n              \n                       [[-2.8196e-02, -3.1557e-03,  4.8000e-02],\n                        [-9.0529e-02, -8.5056e-02, -2.8498e-02],\n                        [-9.4937e-02, -1.3637e-01, -2.0134e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 8.2486e-02,  8.0670e-02,  8.8296e-02],\n                        [ 3.7441e-02,  9.2148e-02,  9.5539e-02],\n                        [ 2.7787e-02, -1.4592e-02,  5.0616e-03]],\n              \n                       [[-2.1889e-02,  5.0026e-02,  8.5417e-02],\n                        [-7.6617e-02,  1.3051e-02,  4.0615e-02],\n                        [-3.9780e-02,  4.5456e-02,  5.3535e-02]],\n              \n                       [[-3.9878e-02,  1.6181e-02,  8.2267e-02],\n                        [ 8.3070e-02,  1.2844e-01,  1.9529e-01],\n                        [ 1.6166e-01,  1.6447e-01,  1.4391e-01]],\n              \n                       ...,\n              \n                       [[-6.1056e-02,  8.2579e-03,  2.5846e-02],\n                        [ 4.0573e-02,  9.3138e-02,  7.3513e-02],\n                        [ 2.1861e-02,  8.1425e-02,  3.8572e-02]],\n              \n                       [[ 5.3428e-02,  4.6188e-02, -1.4057e-02],\n                        [ 9.2092e-02,  1.8424e-02,  2.4798e-02],\n                        [ 3.7479e-02,  2.5804e-03,  1.8428e-02]],\n              \n                       [[ 1.1394e-01,  4.8234e-02,  9.9058e-02],\n                        [ 1.5142e-01,  1.7734e-02,  7.3041e-02],\n                        [ 1.7436e-02, -4.3167e-02, -9.3978e-02]]],\n              \n              \n                      [[[-7.4119e-02, -6.5979e-02, -1.2732e-01],\n                        [-1.8767e-01, -1.9356e-01, -2.4496e-01],\n                        [-2.0093e-01, -2.1975e-01, -1.9998e-01]],\n              \n                       [[ 2.4822e-02,  6.8967e-02,  7.2563e-02],\n                        [ 8.1739e-02,  5.6755e-02,  1.2120e-01],\n                        [ 5.9715e-02,  7.9819e-02,  1.1766e-01]],\n              \n                       [[ 8.8527e-03,  1.2293e-02, -3.0652e-02],\n                        [-1.5203e-01, -1.7440e-01, -2.5953e-01],\n                        [-2.3552e-01, -2.4117e-01, -3.0205e-01]],\n              \n                       ...,\n              \n                       [[ 5.8925e-02, -1.4950e-05, -9.2554e-02],\n                        [ 4.8473e-03, -5.5710e-02, -6.0796e-02],\n                        [ 8.6897e-02,  3.8933e-02,  4.4336e-02]],\n              \n                       [[ 4.9009e-02,  3.1161e-02, -1.3013e-02],\n                        [-8.8780e-02, -2.2430e-02, -8.8234e-02],\n                        [-3.6950e-02, -8.6751e-02, -1.2337e-01]],\n              \n                       [[ 3.6375e-02, -1.3004e-03, -6.0142e-02],\n                        [ 2.1683e-02, -1.8909e-02,  1.8429e-02],\n                        [ 1.4975e-01,  4.5231e-02,  7.9049e-02]]],\n              \n              \n                      [[[-3.3516e-02,  1.8347e-02, -7.1824e-02],\n                        [ 6.7724e-02,  7.7231e-02,  1.2257e-03],\n                        [ 8.7514e-02,  1.4132e-01,  7.9585e-02]],\n              \n                       [[-1.4228e-01, -8.5969e-02, -2.1213e-01],\n                        [-1.5533e-01, -1.6642e-01, -1.5696e-01],\n                        [-1.6975e-01, -1.2367e-01, -2.2594e-01]],\n              \n                       [[-2.0585e-01, -3.0482e-01, -2.5112e-01],\n                        [-1.1165e-01, -4.8904e-02, -8.0955e-02],\n                        [ 3.1458e-02,  1.1695e-01,  5.2211e-02]],\n              \n                       ...,\n              \n                       [[ 3.1946e-02,  8.8893e-03,  4.9486e-02],\n                        [ 5.2864e-02,  6.0068e-02,  7.2826e-02],\n                        [ 1.3303e-01,  1.2680e-01,  1.0523e-01]],\n              \n                       [[ 1.9981e-02, -6.6989e-03, -3.5821e-03],\n                        [ 7.7178e-02,  3.5998e-02,  4.0242e-02],\n                        [ 7.6321e-02,  1.3530e-01,  1.2448e-01]],\n              \n                       [[ 1.6323e-01,  1.1544e-01,  4.9612e-02],\n                        [ 1.6128e-02,  4.1910e-02, -1.9873e-02],\n                        [ 5.5550e-02,  2.7252e-02,  4.4774e-02]]]])),\n             ('layer1.0.bn1.weight',\n              tensor([0.8464, 0.6671, 0.7283, 0.9708, 0.8421, 0.9305, 0.8660, 0.9181, 0.8697,\n                      0.8825, 0.7836, 0.8857, 0.9094, 0.9562, 0.8040, 0.6767, 0.7239, 0.7820,\n                      1.0457, 0.9602, 0.7346, 0.5691, 0.6921, 0.6529, 0.8030, 0.7592, 0.7454,\n                      0.9366, 1.0443, 0.7338, 0.9120, 0.7779, 0.9027, 0.7950, 0.8775, 0.8633,\n                      0.8656, 0.5841, 0.9654, 0.9104, 0.7107, 0.7384, 0.8044, 0.9668, 0.9062,\n                      0.8436, 0.8015, 0.8203, 0.8658, 0.6342, 0.8297, 0.7080, 0.7773, 0.7870,\n                      0.8782, 0.8801, 0.7565, 0.8290, 0.8904, 1.1149, 1.2653, 0.7470, 0.7710,\n                      0.6796])),\n             ('layer1.0.bn1.bias',\n              tensor([-0.0053, -0.3717,  0.0871, -0.0255,  0.0645, -0.1540,  0.0806, -0.0245,\n                      -0.1284, -0.1482, -0.5088, -0.2212, -0.1569, -0.2491, -0.2171, -0.1295,\n                      -0.2915, -0.6820,  0.0506, -0.1938, -0.2043, -0.4130, -0.0097, -0.2648,\n                      -0.1412,  0.1056, -0.1490,  0.0246, -0.0057, -0.1360, -0.3600, -0.0988,\n                      -0.1025,  0.1324, -0.1179, -0.1980, -0.1276, -0.2234,  0.0304, -0.0630,\n                      -0.1708, -0.0958,  0.1699, -0.0151,  0.0327, -0.1231, -0.0446,  0.0029,\n                      -0.2277, -0.2643, -0.0155, -0.0600, -0.0955, -0.0816, -0.1486,  0.0033,\n                      -0.1405, -0.1643, -0.0622, -0.1974, -0.1554,  0.0161, -0.0277, -0.0455])),\n             ('layer1.0.bn1.running_mean',\n              tensor([ -2.8367,   7.0295,  -7.0915,  -3.0212,  -4.4892,  -5.5417,  -5.1403,\n                        5.7974,  -2.6292,  -9.8499, -21.1525, -13.0502,  -8.6487, -17.4183,\n                      -12.0065,   1.9592, -11.7050, -26.7963,  -2.3892,  -7.5757,  -4.9518,\n                       -2.1110,  -9.6468,  -7.8224,  -4.4144, -12.0696,  -9.4155,  -4.7570,\n                      -11.9918,  -8.5691, -16.7234, -11.7513,   0.5085,  -8.4806,  -9.5381,\n                       -4.5534,  -4.1534,  -6.2812,  -9.9895, -10.7740,  -0.1917,  -2.1281,\n                       -1.1269,  -0.7151,  -5.4819, -10.0306, -13.7539,  -6.7632,  -8.8160,\n                        2.9768,  -8.8843,  -5.6042, -11.3665,  -7.1598,  -7.9513, -10.4469,\n                       -8.8444,  -6.9600,  -6.6041,  -0.8528,  -1.8479,   3.4727,  -9.5046,\n                      -11.7087])),\n             ('layer1.0.bn1.running_var',\n              tensor([102.8032, 164.8937,  55.1931,  28.4518, 103.2811,  31.4021,  59.3631,\n                       29.4971,  28.2233, 108.5741, 176.3170,  86.6466,  84.0842, 180.0097,\n                      101.7938,  64.8048,  94.6846, 321.5177,  40.4385,  63.6201,  67.9075,\n                       36.4408,  79.8952,  90.7053,  66.5638, 128.0966, 171.9527,  53.8320,\n                      160.4409,  88.2988, 139.0299,  41.0219,  22.8573, 120.3445,  51.2422,\n                       36.4763,  45.9995,  62.4411,  77.7297,  46.5729,  24.7877, 112.7953,\n                       24.1897,  59.4463,  31.2287,  72.6767, 201.0859, 112.5939,  48.4201,\n                       67.2796,  66.0080, 168.4721,  95.6905,  49.5278,  51.9972, 163.7828,\n                       99.7022,  75.6801,  63.6685,  50.8392,  36.8913,  65.9526,  63.0428,\n                      116.1049])),\n             ('layer1.0.bn1.num_batches_tracked', tensor(14770)),\n             ('layer1.0.conv2.weight',\n              tensor([[[[-0.0210,  0.0654,  0.0417],\n                        [ 0.1046,  0.1080,  0.1486],\n                        [ 0.1230,  0.2001,  0.2138]],\n              \n                       [[ 0.1526,  0.0935,  0.0587],\n                        [ 0.0694,  0.1106,  0.0262],\n                        [ 0.1192,  0.0756,  0.0252]],\n              \n                       [[ 0.0317, -0.0860, -0.2283],\n                        [-0.1778, -0.2674, -0.4589],\n                        [-0.1370, -0.2832, -0.3710]],\n              \n                       ...,\n              \n                       [[ 0.0360, -0.0201, -0.0505],\n                        [-0.0703, -0.1384, -0.1048],\n                        [-0.0552, -0.0837, -0.0109]],\n              \n                       [[ 0.1022,  0.0574,  0.0639],\n                        [ 0.1319,  0.0631,  0.0717],\n                        [ 0.2164,  0.2297,  0.1398]],\n              \n                       [[-0.0487, -0.0322,  0.0148],\n                        [-0.0209, -0.0151,  0.0029],\n                        [ 0.0124, -0.0122,  0.0085]]],\n              \n              \n                      [[[ 0.0352,  0.1621,  0.0754],\n                        [ 0.0641,  0.1211,  0.0953],\n                        [ 0.0769,  0.1099,  0.0525]],\n              \n                       [[ 0.1245,  0.0886,  0.1029],\n                        [ 0.1755,  0.2059,  0.1313],\n                        [ 0.1396,  0.1781,  0.1750]],\n              \n                       [[ 0.0446, -0.0300, -0.0858],\n                        [-0.0356, -0.1134, -0.1262],\n                        [-0.0237, -0.1285, -0.1354]],\n              \n                       ...,\n              \n                       [[-0.0475, -0.0835, -0.1398],\n                        [ 0.0272,  0.0758, -0.0865],\n                        [-0.0150,  0.0138, -0.0366]],\n              \n                       [[ 0.0377,  0.0459, -0.0146],\n                        [ 0.0330,  0.0468,  0.0416],\n                        [ 0.0764,  0.0231,  0.0416]],\n              \n                       [[-0.0983, -0.1064, -0.1019],\n                        [-0.0179, -0.0474, -0.1023],\n                        [ 0.0478,  0.0836,  0.0230]]],\n              \n              \n                      [[[-0.0980, -0.1402, -0.0233],\n                        [-0.0420, -0.0765, -0.0854],\n                        [-0.0381, -0.0577, -0.1204]],\n              \n                       [[ 0.0466,  0.0465,  0.0615],\n                        [-0.0059,  0.0040,  0.0325],\n                        [-0.1048, -0.1092, -0.0816]],\n              \n                       [[ 0.0378,  0.0969,  0.1166],\n                        [ 0.0942,  0.1233,  0.1200],\n                        [ 0.1036,  0.0567,  0.0590]],\n              \n                       ...,\n              \n                       [[ 0.0582, -0.0082, -0.0394],\n                        [-0.0250, -0.0658, -0.0784],\n                        [-0.1024, -0.1795, -0.1526]],\n              \n                       [[-0.0578, -0.0606, -0.0741],\n                        [-0.0711, -0.0088,  0.0384],\n                        [ 0.0213,  0.0304, -0.0447]],\n              \n                       [[ 0.2381,  0.2245,  0.1874],\n                        [ 0.2368,  0.1607,  0.0437],\n                        [ 0.1038,  0.0312, -0.0193]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1198, -0.0737, -0.1047],\n                        [-0.1404, -0.0984, -0.1275],\n                        [-0.1098, -0.0992, -0.1282]],\n              \n                       [[ 0.1363,  0.1288,  0.0141],\n                        [ 0.0745,  0.0924,  0.0147],\n                        [ 0.0629,  0.0107,  0.0056]],\n              \n                       [[ 0.0064, -0.0113,  0.0243],\n                        [-0.0588,  0.0434,  0.0464],\n                        [-0.0339,  0.0009,  0.1000]],\n              \n                       ...,\n              \n                       [[ 0.0403,  0.0023, -0.0127],\n                        [ 0.0643,  0.0116,  0.0675],\n                        [ 0.1235,  0.0345, -0.0121]],\n              \n                       [[-0.0375,  0.0476,  0.0403],\n                        [-0.0694, -0.0260,  0.0185],\n                        [-0.1504, -0.1132, -0.1362]],\n              \n                       [[ 0.0753,  0.0775,  0.1623],\n                        [ 0.0202,  0.0737,  0.0820],\n                        [ 0.0111,  0.0399,  0.0012]]],\n              \n              \n                      [[[-0.1096, -0.1199, -0.0400],\n                        [-0.0421, -0.0461, -0.0215],\n                        [-0.0081,  0.1094,  0.1175]],\n              \n                       [[-0.2216, -0.1764, -0.0814],\n                        [-0.1626, -0.1864, -0.0181],\n                        [-0.2251, -0.1392,  0.0021]],\n              \n                       [[ 0.0850,  0.0178,  0.0598],\n                        [ 0.1089,  0.0069,  0.0881],\n                        [ 0.1037,  0.0478,  0.0036]],\n              \n                       ...,\n              \n                       [[-0.1775, -0.1043, -0.0887],\n                        [-0.1723, -0.0634, -0.0361],\n                        [-0.1648, -0.0938, -0.0385]],\n              \n                       [[-0.0470, -0.0884, -0.0065],\n                        [-0.0922, -0.1093, -0.0173],\n                        [-0.0936, -0.0580, -0.0661]],\n              \n                       [[ 0.1291,  0.1354,  0.0975],\n                        [ 0.1518,  0.1700,  0.1116],\n                        [ 0.0463,  0.0489,  0.0630]]],\n              \n              \n                      [[[ 0.0702,  0.0435, -0.0198],\n                        [ 0.0105, -0.0518, -0.0698],\n                        [-0.0591, -0.1356, -0.0906]],\n              \n                       [[ 0.0784,  0.0875,  0.0445],\n                        [ 0.1590,  0.1679,  0.0657],\n                        [ 0.1364,  0.0869,  0.0111]],\n              \n                       [[ 0.0403,  0.1202,  0.1232],\n                        [ 0.0338,  0.1059,  0.1718],\n                        [ 0.0697,  0.1233,  0.1135]],\n              \n                       ...,\n              \n                       [[-0.0580, -0.1301, -0.1257],\n                        [-0.0998, -0.0940, -0.1364],\n                        [-0.0925, -0.1053, -0.0709]],\n              \n                       [[ 0.0681, -0.0155, -0.0146],\n                        [-0.0383, -0.0347, -0.0622],\n                        [-0.0129, -0.0821, -0.0352]],\n              \n                       [[ 0.0093, -0.0602, -0.0027],\n                        [-0.0579, -0.0665, -0.0622],\n                        [-0.0221, -0.0122, -0.0554]]]])),\n             ('layer1.0.bn2.weight',\n              tensor([0.7839, 0.8585, 0.7845, 0.6622, 0.7946, 0.6919, 0.5750, 0.9351, 0.7418,\n                      0.6531, 0.8350, 0.7435, 0.7076, 0.7642, 0.7971, 0.7872, 0.5145, 0.7008,\n                      1.0211, 0.7705, 0.7882, 0.7840, 1.0342, 0.7512, 0.6938, 0.5922, 0.8461,\n                      0.6250, 0.7130, 0.8041, 0.7502, 0.8016, 0.7955, 0.6505, 0.8092, 0.8224,\n                      0.6544, 0.7252, 0.8516, 0.6942, 0.8452, 0.7980, 0.7809, 0.7729, 0.7906,\n                      0.8483, 0.7115, 0.7452, 0.7851, 0.6587, 0.9426, 0.7117, 0.8544, 0.7708,\n                      0.7332, 0.8303, 0.6349, 0.7122, 0.6449, 0.7820, 0.9521, 0.7611, 0.9255,\n                      0.7173])),\n             ('layer1.0.bn2.bias',\n              tensor([-0.0448, -0.0207, -0.1328, -0.0862, -0.0794,  0.1434,  0.2199, -0.2783,\n                       0.1162,  0.0560,  0.0853,  0.2354, -0.1216,  0.0671, -0.4514, -0.0590,\n                       0.2900, -0.0934, -0.1265, -0.1652,  0.0334, -0.0494, -0.0713,  0.0358,\n                      -0.2635, -0.0105,  0.0355, -0.2737, -0.0744,  0.2673, -0.1492,  0.1321,\n                      -0.1164, -0.0800, -0.0313,  0.1862,  0.1606, -0.0284, -0.1995,  0.1169,\n                      -0.1638,  0.3016, -0.2681,  0.1959,  0.2318,  0.3143,  0.3998,  0.1079,\n                      -0.3353, -0.0409, -0.2111,  0.1985, -0.0826,  0.0785, -0.2353,  0.2209,\n                       0.0062,  0.0363,  0.4139, -0.1896, -0.2955, -0.0061,  0.0459,  0.2162])),\n             ('layer1.0.bn2.running_mean',\n              tensor([-2.0317,  2.3049, -2.0025, -1.3470, -6.7171,  2.3754, -2.6938,  0.4095,\n                      -1.0847, -0.9040, -0.4009, -4.5953,  2.2554,  5.2830,  3.5278,  3.4603,\n                       0.2745,  2.7578,  4.5402,  1.4420, -1.5659, -4.5802, -0.8150, -2.8194,\n                       0.8295, -2.5331, -3.0864,  0.6642,  2.2185, -1.8520,  2.2236,  0.8281,\n                      -4.0255,  6.5212,  2.4586,  1.4682,  1.2737,  3.0589,  1.4471, -1.2834,\n                       3.6037, -2.2249,  3.3743, -0.6669, -2.8355,  0.4786, -0.8224, -0.3525,\n                       1.8174, -2.5150,  4.5261,  1.5242,  1.0485, -1.4877,  5.1318, -2.0660,\n                       3.8466, -1.1138, -2.2574,  1.1504,  1.7386,  2.2406,  3.1897, -1.5836])),\n             ('layer1.0.bn2.running_var',\n              tensor([23.6156, 36.1168, 31.5392, 18.3080, 15.7890, 37.0456, 20.8910, 13.2669,\n                      31.7491, 21.9750, 25.8170, 30.6867, 17.6690, 23.6445, 29.6348, 13.1693,\n                      26.2113, 17.1262, 20.2722, 33.5571, 22.3096, 27.3858, 14.9798, 36.1936,\n                      24.0588, 39.2551, 13.3870, 20.0232, 32.0911, 28.9777, 47.3260, 48.7033,\n                      26.4387, 17.4875, 28.6720, 50.2476, 23.8036, 27.0644, 27.9897, 18.3426,\n                      29.7810, 36.6373, 45.5032, 27.7519, 27.7135, 44.6805, 24.9873, 28.6427,\n                       8.3335, 26.7832, 39.0037, 30.4079, 14.1200, 28.5204,  7.3418, 19.1908,\n                      45.0865, 22.2945, 20.3395, 25.1826, 33.9235, 25.2242, 32.7360, 29.2262])),\n             ('layer1.0.bn2.num_batches_tracked', tensor(14770)),\n             ('layer1.1.conv1.weight',\n              tensor([[[[ 0.0156,  0.0435,  0.0093],\n                        [ 0.0496, -0.0048, -0.0264],\n                        [ 0.0495, -0.0302, -0.0152]],\n              \n                       [[-0.0793, -0.0435, -0.0539],\n                        [-0.0795, -0.0341, -0.0644],\n                        [-0.0474, -0.0439,  0.0329]],\n              \n                       [[ 0.0826,  0.0847,  0.0602],\n                        [ 0.0603,  0.0861,  0.1510],\n                        [ 0.1483,  0.1700,  0.1883]],\n              \n                       ...,\n              \n                       [[ 0.1832,  0.1590,  0.1291],\n                        [ 0.1001,  0.1102,  0.0607],\n                        [ 0.0178,  0.0582, -0.0113]],\n              \n                       [[ 0.1208,  0.1398,  0.0334],\n                        [ 0.0859,  0.1543,  0.0874],\n                        [ 0.0662,  0.0420, -0.0239]],\n              \n                       [[ 0.0981,  0.0527,  0.0204],\n                        [ 0.0519, -0.0292, -0.0269],\n                        [ 0.0106, -0.1074, -0.0936]]],\n              \n              \n                      [[[ 0.0535,  0.0905,  0.0404],\n                        [ 0.0816,  0.0860,  0.0233],\n                        [ 0.1332,  0.0354,  0.0646]],\n              \n                       [[-0.0249, -0.0566, -0.1520],\n                        [-0.0222, -0.0836, -0.1128],\n                        [-0.0506, -0.1065, -0.0950]],\n              \n                       [[-0.1085, -0.0965, -0.0966],\n                        [-0.0729, -0.1199, -0.1582],\n                        [-0.0698, -0.1234, -0.1323]],\n              \n                       ...,\n              \n                       [[ 0.0936,  0.0985,  0.1299],\n                        [ 0.0268,  0.0985,  0.1179],\n                        [ 0.0204,  0.1117,  0.0780]],\n              \n                       [[ 0.0915,  0.1157,  0.0447],\n                        [ 0.0499,  0.0769,  0.0780],\n                        [-0.0081,  0.0192, -0.0333]],\n              \n                       [[-0.0942, -0.0216,  0.0040],\n                        [-0.2334, -0.2104, -0.1906],\n                        [-0.1721, -0.1281, -0.0991]]],\n              \n              \n                      [[[-0.0249,  0.0477,  0.1457],\n                        [-0.0173,  0.0928,  0.1318],\n                        [ 0.0435,  0.0719,  0.0472]],\n              \n                       [[-0.2241, -0.2296, -0.1629],\n                        [-0.1828, -0.2456, -0.1395],\n                        [-0.0753, -0.1020, -0.0182]],\n              \n                       [[-0.0870, -0.0813, -0.1042],\n                        [-0.0767, -0.0382,  0.0255],\n                        [-0.0693, -0.0240,  0.0828]],\n              \n                       ...,\n              \n                       [[ 0.1207,  0.0687,  0.0577],\n                        [ 0.1491,  0.0937,  0.0562],\n                        [ 0.0918,  0.0988,  0.0418]],\n              \n                       [[ 0.1302,  0.0697,  0.0898],\n                        [ 0.1237,  0.1230,  0.0908],\n                        [ 0.1206,  0.0579,  0.0204]],\n              \n                       [[ 0.1529,  0.2219,  0.0582],\n                        [-0.0763, -0.0758, -0.1563],\n                        [-0.2001, -0.2944, -0.4153]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1215, -0.1328, -0.0935],\n                        [-0.0371, -0.1000, -0.0829],\n                        [ 0.0519, -0.0650, -0.0880]],\n              \n                       [[-0.0079,  0.0591,  0.0072],\n                        [ 0.0819,  0.1089,  0.0826],\n                        [ 0.1173,  0.1379,  0.1205]],\n              \n                       [[ 0.0459, -0.0061,  0.0391],\n                        [ 0.1037,  0.0798, -0.0037],\n                        [ 0.0394,  0.0337, -0.0582]],\n              \n                       ...,\n              \n                       [[-0.0680, -0.0927, -0.0692],\n                        [-0.0693, -0.0330, -0.0825],\n                        [-0.0675, -0.0777, -0.0874]],\n              \n                       [[-0.0519, -0.0266, -0.0056],\n                        [-0.0386,  0.0050,  0.0476],\n                        [-0.0145, -0.0304,  0.0085]],\n              \n                       [[-0.0118, -0.0640, -0.0633],\n                        [-0.0832, -0.1452, -0.1485],\n                        [-0.0430, -0.1266, -0.0662]]],\n              \n              \n                      [[[ 0.0523,  0.0308, -0.0509],\n                        [ 0.0287, -0.0156, -0.0539],\n                        [-0.0272, -0.0641, -0.0923]],\n              \n                       [[ 0.0297,  0.0537, -0.0075],\n                        [ 0.0418,  0.0999,  0.0857],\n                        [ 0.0749,  0.0675,  0.0837]],\n              \n                       [[-0.0560, -0.0310, -0.0398],\n                        [-0.0379, -0.0560, -0.1230],\n                        [ 0.0536, -0.0484, -0.1309]],\n              \n                       ...,\n              \n                       [[ 0.0751,  0.0319,  0.0756],\n                        [ 0.0571,  0.0775,  0.0784],\n                        [ 0.0721,  0.0254,  0.0646]],\n              \n                       [[-0.0496, -0.0133, -0.0804],\n                        [-0.0356, -0.0434, -0.0523],\n                        [ 0.0108, -0.0108, -0.0166]],\n              \n                       [[-0.1510, -0.2086, -0.1433],\n                        [-0.0953, -0.1841, -0.1164],\n                        [-0.0914, -0.1630, -0.0216]]],\n              \n              \n                      [[[-0.0675, -0.1131,  0.0175],\n                        [-0.0565, -0.0587, -0.0609],\n                        [ 0.0633,  0.0800,  0.0915]],\n              \n                       [[-0.1414, -0.0734, -0.0639],\n                        [-0.1861, -0.0877, -0.0568],\n                        [-0.2348, -0.1797, -0.1919]],\n              \n                       [[ 0.1306,  0.0084,  0.0816],\n                        [ 0.0155, -0.0644,  0.0449],\n                        [-0.0377, -0.0947,  0.0540]],\n              \n                       ...,\n              \n                       [[-0.0694, -0.0205, -0.0008],\n                        [-0.0492, -0.0059, -0.0038],\n                        [-0.0576, -0.0428, -0.0275]],\n              \n                       [[ 0.1001,  0.0339, -0.0082],\n                        [ 0.0216, -0.0021, -0.0202],\n                        [ 0.0288,  0.0170,  0.0127]],\n              \n                       [[ 0.0246, -0.0006,  0.0627],\n                        [-0.0644, -0.0763,  0.0165],\n                        [-0.0953, -0.0216, -0.0502]]]])),\n             ('layer1.1.bn1.weight',\n              tensor([0.5730, 0.7027, 1.0476, 0.7346, 0.7936, 0.7319, 0.8976, 0.8152, 1.1461,\n                      0.7927, 0.9354, 0.9354, 0.8817, 0.8333, 0.7297, 0.8095, 0.8524, 0.9366,\n                      0.8284, 0.6722, 0.6649, 0.8007, 0.8203, 0.7846, 0.7399, 0.7536, 0.7535,\n                      0.7637, 0.7581, 0.6553, 0.8413, 0.5752, 0.8827, 1.2854, 0.6858, 0.8948,\n                      0.8248, 0.6765, 0.8952, 0.8018, 0.6858, 0.8984, 0.7624, 0.8823, 0.7765,\n                      0.7333, 0.8721, 0.8484, 0.8101, 0.7869, 0.8766, 0.7992, 0.7912, 0.5769,\n                      0.7897, 0.7605, 0.7966, 0.8426, 0.8751, 0.7674, 0.8981, 0.7063, 0.7790,\n                      0.9341])),\n             ('layer1.1.bn1.bias',\n              tensor([-0.3882, -0.4245,  0.0737, -0.2399,  0.0654, -0.3603, -0.1915, -0.0588,\n                      -0.1287, -0.2533, -0.0808, -0.1444, -0.1613, -0.0849, -0.3179, -0.3548,\n                      -0.0294,  0.0833,  0.0128, -0.3391, -0.2633, -0.2967, -0.4330, -0.4039,\n                      -0.8923, -0.5345, -0.5293, -0.0726, -0.2958, -0.3058,  0.0937, -0.3521,\n                      -0.1525,  0.2777, -0.3215, -0.3211, -0.3491, -0.4568, -0.0132, -0.5642,\n                      -0.5296, -0.0268, -0.1371,  0.0531,  0.0995, -0.3259, -0.0330, -0.4205,\n                      -0.0093, -0.5841, -0.1241, -0.1858, -0.6427, -0.2006, -0.2490, -0.2772,\n                      -0.3066, -0.4825, -0.1595, -0.5988,  0.1639, -0.2561, -0.4653,  0.0960])),\n             ('layer1.1.bn1.running_mean',\n              tensor([ -8.2435,  -3.7199,  -3.7900,   6.9384,   0.6718,  -8.3434,  -3.6086,\n                        5.5294,  -2.2226,  -5.5686,   2.1484,   9.2937,  -2.5557, -14.1214,\n                       -1.8851, -12.3829, -17.0669,   0.5986, -31.4580,   5.7125,  -5.3870,\n                      -11.2505, -14.3784,  -0.5159,  -1.0480, -15.4350, -20.7752,  -7.0087,\n                      -11.3034,   4.9300,   1.6553,  23.8098, -10.2870,  -4.6014,  -4.8126,\n                      -10.7995,  -6.5135,   1.8468,  -3.7432,  -8.6955, -10.9661,   0.3201,\n                       -4.7306,  -5.3083,  -8.8051, -12.2609,   5.1976, -13.8975,  -7.4192,\n                      -10.8887,  -0.8672,  -3.5344,  -0.8243,  -8.5399, -16.9540,   2.3617,\n                        8.5240,  -9.7245,  -2.8032,   0.5795, -15.5924, -19.4149, -11.0660,\n                       -4.6805])),\n             ('layer1.1.bn1.running_var',\n              tensor([158.5760,  72.6276,  61.3078, 145.5786, 272.7996,  90.2267, 140.7554,\n                      167.4707,  17.6157, 113.0965,  81.7546,  94.1839, 147.1432, 304.8795,\n                       92.4930, 121.3338, 158.5474, 204.9671, 155.3107,  67.6991,  89.0984,\n                      107.6011, 158.7567, 129.4729,  70.9380, 183.6757, 173.8238, 123.5224,\n                       66.1401, 129.4719,  64.4948,  97.4128, 139.4643,  27.8881, 143.7733,\n                      139.3599, 117.0774,  93.7466, 218.6092, 133.8646, 133.5050, 153.5474,\n                      152.6761,  85.4904, 189.4142, 115.7865, 299.0279, 156.7418, 178.4143,\n                      167.0308, 136.8445,  95.7970,  73.4371, 196.0612, 103.1656,  64.1321,\n                       55.1114, 111.0848,  87.4867,  49.5095,  71.1866, 134.2980, 143.2126,\n                      120.7293])),\n             ('layer1.1.bn1.num_batches_tracked', tensor(14770)),\n             ('layer1.1.conv2.weight',\n              tensor([[[[-2.2658e-02, -6.7796e-02, -2.9309e-02],\n                        [-3.3574e-02,  1.1387e-02, -4.1765e-02],\n                        [-8.5582e-02, -3.6183e-02, -7.8506e-02]],\n              \n                       [[-8.6026e-02, -1.2651e-01, -4.2749e-02],\n                        [-6.2657e-02, -7.0483e-02,  9.3277e-03],\n                        [-7.8481e-02, -5.5687e-02,  2.1302e-02]],\n              \n                       [[ 5.0764e-02, -7.7822e-03, -1.7744e-02],\n                        [ 1.5605e-02,  5.7142e-02, -5.2645e-02],\n                        [ 1.9959e-02, -4.3840e-03, -2.9992e-02]],\n              \n                       ...,\n              \n                       [[-1.3646e-01, -8.5786e-02, -4.6840e-02],\n                        [-5.8711e-02, -1.3919e-01, -5.9386e-02],\n                        [-8.6685e-04,  1.2368e-02, -4.3008e-02]],\n              \n                       [[ 1.0810e-01,  1.0545e-02, -6.0706e-02],\n                        [ 1.4512e-01,  7.1484e-02,  4.9293e-02],\n                        [ 2.4008e-01,  1.8948e-01,  7.2142e-02]],\n              \n                       [[-5.2709e-03, -4.3897e-02,  3.4599e-02],\n                        [-8.8052e-02, -9.0655e-02, -4.7311e-02],\n                        [-1.2372e-01, -9.7896e-02, -5.2678e-02]]],\n              \n              \n                      [[[-4.2769e-03,  5.4922e-02,  5.2246e-03],\n                        [-7.9270e-02, -3.5799e-03,  4.9587e-02],\n                        [ 2.9721e-02,  4.4672e-02,  1.0690e-02]],\n              \n                       [[-1.2198e-01, -9.8128e-02, -7.7423e-02],\n                        [-1.6429e-01, -1.9567e-01, -1.6284e-01],\n                        [-1.8454e-01, -2.1289e-01, -1.4262e-01]],\n              \n                       [[ 8.7492e-02,  1.3544e-01,  1.1782e-01],\n                        [ 3.3142e-02, -7.8311e-03, -1.2237e-02],\n                        [ 1.1028e-01,  7.8052e-02,  7.9942e-02]],\n              \n                       ...,\n              \n                       [[ 2.8068e-02,  1.9345e-02,  5.4847e-02],\n                        [-1.0125e-02,  2.4050e-03,  5.3871e-02],\n                        [-9.9269e-02, -2.5040e-02,  1.0040e-02]],\n              \n                       [[-4.2920e-02, -6.4860e-02, -3.2604e-02],\n                        [-8.0043e-02, -4.9348e-02, -4.4608e-02],\n                        [-1.2110e-01, -9.5029e-02, -1.1398e-01]],\n              \n                       [[ 1.3539e-01,  7.3035e-02, -1.5762e-02],\n                        [ 9.3848e-02,  1.4759e-01, -8.4521e-02],\n                        [ 1.1171e-01,  3.9307e-02, -6.3115e-02]]],\n              \n              \n                      [[[ 3.3677e-02, -1.6230e-04, -4.7740e-02],\n                        [ 7.5413e-02,  5.7885e-02, -6.0198e-02],\n                        [-1.3217e-02, -8.5124e-02, -7.3147e-02]],\n              \n                       [[ 1.1460e-03, -6.7377e-02, -1.2167e-01],\n                        [-4.8548e-02, -1.3627e-01, -8.9561e-02],\n                        [-1.8860e-01, -2.0078e-01, -1.8199e-01]],\n              \n                       [[ 3.6672e-02,  1.4295e-01,  7.0562e-02],\n                        [ 3.2462e-02,  3.7291e-02, -2.2643e-02],\n                        [-3.7199e-02, -6.2690e-02, -1.3714e-01]],\n              \n                       ...,\n              \n                       [[ 3.1236e-02, -6.1126e-03,  2.7089e-03],\n                        [ 6.8074e-02,  7.0165e-02,  8.1054e-02],\n                        [ 4.4470e-02,  1.1560e-01,  5.8538e-02]],\n              \n                       [[ 1.0833e-01,  1.3775e-01,  1.9575e-01],\n                        [ 4.6778e-02,  7.2033e-02,  1.5505e-01],\n                        [-1.4549e-03,  7.9387e-02,  6.4966e-02]],\n              \n                       [[ 2.7277e-02, -4.2249e-02, -5.8954e-02],\n                        [-3.9661e-02, -7.4911e-02, -4.3120e-02],\n                        [-1.0684e-02, -5.8168e-02, -3.9725e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-2.4121e-01, -1.9457e-01, -1.2797e-01],\n                        [-1.8911e-01, -1.0030e-01, -9.8839e-02],\n                        [-7.2416e-02, -1.1507e-01, -7.5540e-02]],\n              \n                       [[-5.4984e-02, -1.7274e-01, -1.8127e-01],\n                        [-3.8863e-02, -1.1791e-01, -1.5771e-01],\n                        [ 6.8050e-02,  3.9102e-03, -5.1808e-02]],\n              \n                       [[ 2.3077e-02,  4.9542e-02,  4.0321e-02],\n                        [ 1.1071e-03,  3.5725e-02, -1.4131e-03],\n                        [-5.9390e-02,  3.8627e-02,  5.1140e-02]],\n              \n                       ...,\n              \n                       [[ 9.2774e-04, -7.9161e-02, -5.0504e-02],\n                        [-6.5587e-02, -5.5442e-02, -3.0017e-02],\n                        [ 2.3190e-02, -2.9364e-02, -2.0647e-02]],\n              \n                       [[ 2.7435e-02, -1.5143e-02, -1.3978e-02],\n                        [-5.7340e-02, -5.8391e-02, -5.9976e-02],\n                        [-7.3531e-02, -9.8605e-02, -1.3735e-01]],\n              \n                       [[-2.5136e-02, -1.1725e-01, -2.1399e-01],\n                        [-4.8366e-02, -5.8681e-02, -1.8778e-01],\n                        [-7.1957e-03, -7.0026e-02, -1.1831e-01]]],\n              \n              \n                      [[[-2.9609e-02,  1.7721e-02, -8.7892e-03],\n                        [ 2.4571e-02,  4.4967e-02,  2.1334e-02],\n                        [-1.2201e-03,  7.0874e-02,  3.4855e-02]],\n              \n                       [[-7.7773e-02, -3.8472e-02, -3.9687e-02],\n                        [-3.1273e-02,  2.6547e-02,  4.1371e-02],\n                        [-8.4201e-02,  5.4351e-02, -3.8519e-03]],\n              \n                       [[-2.5769e-02,  7.7633e-02, -1.2388e-02],\n                        [-8.6781e-02, -3.1833e-02, -1.3389e-02],\n                        [-3.1013e-02,  4.9303e-02,  2.1719e-02]],\n              \n                       ...,\n              \n                       [[ 9.4139e-02,  1.0689e-01,  3.5377e-02],\n                        [ 1.9694e-01,  2.0188e-01,  1.4297e-01],\n                        [ 9.2458e-02,  1.2995e-01,  8.2292e-02]],\n              \n                       [[ 5.9039e-02,  9.1311e-02,  1.2756e-01],\n                        [-1.3392e-02,  4.7329e-02,  6.0264e-02],\n                        [-5.5405e-02,  2.0834e-02,  1.3508e-02]],\n              \n                       [[-6.0958e-03, -6.3472e-02, -1.0272e-01],\n                        [-7.3484e-02, -2.1088e-02, -4.6763e-02],\n                        [-2.8627e-02, -5.4465e-02, -2.0545e-02]]],\n              \n              \n                      [[[-3.7335e-02, -5.0477e-02,  1.9470e-02],\n                        [-2.2013e-02,  2.2216e-02,  7.6625e-02],\n                        [ 5.5680e-02,  6.9374e-02,  1.3401e-01]],\n              \n                       [[-1.2368e-01, -1.3362e-01, -1.1672e-01],\n                        [-9.5907e-02, -9.2261e-02, -9.4235e-02],\n                        [ 1.3727e-02, -4.2378e-02, -1.2508e-02]],\n              \n                       [[-4.3735e-01, -4.2919e-01, -2.6069e-01],\n                        [-2.1961e-01, -5.9207e-02, -3.2322e-02],\n                        [ 4.1721e-02,  1.6121e-01,  1.3916e-01]],\n              \n                       ...,\n              \n                       [[ 8.4298e-03, -6.1069e-03, -1.4984e-02],\n                        [-1.8673e-02, -1.4512e-02, -6.3853e-03],\n                        [ 8.8306e-02,  5.5857e-02,  1.0385e-01]],\n              \n                       [[-8.6048e-02, -8.0316e-02, -8.2980e-02],\n                        [-1.6614e-01, -1.7994e-01, -1.4721e-01],\n                        [-1.4991e-01, -1.5246e-01, -1.3982e-01]],\n              \n                       [[-2.5799e-02, -9.5269e-02, -1.3842e-01],\n                        [-5.7865e-02, -1.7566e-01, -2.0831e-01],\n                        [-1.2619e-01, -2.0076e-01, -2.1912e-01]]]])),\n             ('layer1.1.bn2.weight',\n              tensor([0.5853, 0.7962, 0.6266, 0.6098, 0.5427, 0.3592, 0.5351, 0.6672, 0.6376,\n                      0.5340, 0.6300, 0.5324, 0.5635, 0.6982, 0.4937, 0.4054, 0.3252, 0.7244,\n                      0.5679, 0.6713, 0.6353, 0.5469, 0.7590, 0.5669, 0.5069, 0.6584, 0.6861,\n                      0.4424, 0.4945, 0.6201, 0.4401, 0.7951, 0.4055, 0.6871, 0.5794, 0.5312,\n                      0.5514, 0.6907, 0.5919, 0.4635, 0.6970, 0.5962, 0.5534, 0.6407, 0.3306,\n                      0.5734, 0.7127, 0.7574, 0.5700, 0.5906, 0.5713, 0.7337, 0.3393, 0.7412,\n                      0.5462, 0.7349, 0.3571, 0.5443, 0.6986, 0.3934, 0.6391, 0.7731, 0.7809,\n                      0.8084])),\n             ('layer1.1.bn2.bias',\n              tensor([-1.6084e-01,  1.3331e-02, -2.6633e-01, -2.3827e-01, -1.6363e-01,\n                      -1.1999e-01,  3.2996e-02, -1.8809e-01,  3.0418e-02, -1.9465e-02,\n                      -1.9204e-01, -7.9198e-02, -1.2807e-01,  7.8097e-02, -2.9948e-01,\n                      -2.1686e-01, -2.7574e-02,  2.1039e-01, -1.6419e-01, -2.1994e-01,\n                      -2.9570e-02, -1.4765e-01, -1.0668e-01,  2.0624e-04, -2.8839e-01,\n                      -5.7896e-02,  1.6156e-03, -2.5170e-01,  9.5798e-03, -8.9474e-02,\n                       1.4511e-03, -5.8132e-02, -1.5916e-01, -1.2859e-01, -1.3411e-01,\n                      -1.2789e-01, -5.9391e-02, -3.7275e-02, -2.1508e-01, -2.3384e-02,\n                      -6.7812e-02, -4.9083e-02, -3.4069e-01, -5.2112e-02,  1.1239e-01,\n                       2.1271e-01, -6.0779e-02,  1.9551e-01, -4.4635e-01, -1.4012e-01,\n                      -2.3868e-01, -3.0713e-01, -6.7699e-02, -1.5322e-01, -8.5133e-02,\n                       4.5330e-02,  2.4062e-02, -1.6896e-01, -3.8636e-02, -2.6062e-01,\n                      -1.6756e-01, -1.1801e-01, -1.5975e-02, -1.3858e-01])),\n             ('layer1.1.bn2.running_mean',\n              tensor([ 0.1620, -0.5353,  1.8944,  1.0676,  1.9754, -4.2689,  6.9146, -5.9151,\n                       1.0939,  1.7994, -0.5874, -8.1906,  3.4763, -2.3103, -7.2067, -4.8841,\n                      -2.2350, -4.1327,  4.6967,  5.1462,  3.5675, -3.8158, -5.6233, -6.4272,\n                       2.9091, -3.5096,  7.5757,  2.6167, -4.3613, -7.3339, -7.6411, -1.1135,\n                       0.1561, -4.0016, -4.3820, -2.4273, -0.4133, -6.6981, -0.5529,  5.4499,\n                      -6.6641, -5.1650,  8.4368,  2.2442, -5.9059, -5.0386, -2.8592,  2.0941,\n                       3.0942,  1.3585,  0.4081, -1.6808, -6.5176, -1.9768, -5.8011,  0.8005,\n                      -6.7963, -2.5153, -5.5223,  3.3067, -6.6096, -7.8923, -0.7516, -0.5327])),\n             ('layer1.1.bn2.running_var',\n              tensor([ 56.7329,  49.1658,  49.7401,  12.6952,  63.8694,  82.4425,  78.3950,\n                       51.9339,  76.2795,  73.4010,  66.5539, 146.6521,  43.4154,  70.7590,\n                       45.8040,  77.1998,  76.1562, 107.8263,  51.5399,  84.6620,  71.4036,\n                      115.8776, 155.6268, 130.0637,  49.0107,  83.8351, 103.9734,  57.6394,\n                       99.3641, 124.6593, 119.4020,  54.9795,  42.7134,  82.6746,  62.6643,\n                       73.0952,  44.8882,  99.3973,  48.5837,  46.4395,  76.0659,  97.4581,\n                      138.3520,  49.3481, 150.2742,  86.1076,  75.7186,  65.2833,  39.6129,\n                       79.2775,  51.4576,  52.2411,  85.9064,  86.8067,  65.1871,  26.6049,\n                      113.4130,  51.6606, 116.2880, 100.1284, 124.5433, 117.5721,  63.3738,\n                       43.3388])),\n             ('layer1.1.bn2.num_batches_tracked', tensor(14770)),\n             ('layer2.0.conv1.weight',\n              tensor([[[[ 0.0284,  0.0562, -0.0152],\n                        [ 0.0484,  0.0906,  0.0168],\n                        [ 0.0392,  0.1034,  0.0073]],\n              \n                       [[-0.0498, -0.0559, -0.0298],\n                        [-0.1633, -0.1533, -0.1078],\n                        [-0.1645, -0.1186, -0.0532]],\n              \n                       [[-0.0657,  0.0059,  0.0118],\n                        [-0.0172,  0.0274,  0.0427],\n                        [ 0.0754,  0.0098,  0.0825]],\n              \n                       ...,\n              \n                       [[-0.1023, -0.0585, -0.0164],\n                        [ 0.0073, -0.0344, -0.0331],\n                        [ 0.0060, -0.0197, -0.0447]],\n              \n                       [[-0.1079, -0.0649, -0.1010],\n                        [-0.1056, -0.0558, -0.0811],\n                        [-0.0970, -0.0478, -0.0728]],\n              \n                       [[-0.0790, -0.0388, -0.0505],\n                        [ 0.0599,  0.0141, -0.0343],\n                        [ 0.0411,  0.0377, -0.0687]]],\n              \n              \n                      [[[ 0.0188,  0.0149, -0.0399],\n                        [-0.1013, -0.0796, -0.1173],\n                        [-0.1350, -0.0969, -0.1651]],\n              \n                       [[-0.0884, -0.0060, -0.0089],\n                        [-0.0370, -0.0214,  0.0071],\n                        [ 0.0036,  0.0292, -0.0236]],\n              \n                       [[ 0.5592,  0.5289,  0.3850],\n                        [ 0.4180,  0.2550, -0.0377],\n                        [ 0.2548, -0.0282, -0.1095]],\n              \n                       ...,\n              \n                       [[ 0.1501,  0.1054,  0.0093],\n                        [ 0.0932,  0.0197, -0.0121],\n                        [ 0.0567, -0.0461, -0.0707]],\n              \n                       [[-0.0040, -0.1142, -0.0992],\n                        [-0.0176, -0.0728, -0.0795],\n                        [-0.0162, -0.1046, -0.0990]],\n              \n                       [[ 0.0323, -0.0266, -0.0921],\n                        [-0.1300, -0.0942, -0.1742],\n                        [-0.1567, -0.1519, -0.0430]]],\n              \n              \n                      [[[-0.0589,  0.1067,  0.1990],\n                        [-0.0470,  0.0174,  0.1385],\n                        [-0.0521, -0.0570, -0.0268]],\n              \n                       [[ 0.0860,  0.0181, -0.0581],\n                        [ 0.0931, -0.0060, -0.0688],\n                        [ 0.1207,  0.0611, -0.0279]],\n              \n                       [[-0.0047,  0.0871,  0.2042],\n                        [-0.0775,  0.1696,  0.1601],\n                        [ 0.0121,  0.0602,  0.2199]],\n              \n                       ...,\n              \n                       [[-0.0090, -0.0961, -0.0539],\n                        [-0.0599, -0.0497, -0.0533],\n                        [ 0.0325,  0.0214,  0.0096]],\n              \n                       [[-0.0548, -0.0529, -0.0950],\n                        [-0.0845, -0.0793, -0.0967],\n                        [-0.1160, -0.1541, -0.1481]],\n              \n                       [[ 0.0740,  0.0808, -0.0537],\n                        [ 0.1303,  0.1156,  0.0998],\n                        [ 0.1413,  0.1270,  0.0944]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1010,  0.0313,  0.1106],\n                        [-0.1280, -0.0334,  0.0878],\n                        [-0.0857, -0.0388,  0.0225]],\n              \n                       [[ 0.1115,  0.0576,  0.0771],\n                        [ 0.0331, -0.0551, -0.0232],\n                        [-0.0393, -0.1310, -0.0962]],\n              \n                       [[-0.2037, -0.2962, -0.1780],\n                        [ 0.0604, -0.0794, -0.0568],\n                        [-0.0388, -0.1008, -0.0423]],\n              \n                       ...,\n              \n                       [[-0.1348, -0.1109, -0.0198],\n                        [-0.1069, -0.0897, -0.0334],\n                        [-0.0360,  0.0200,  0.0605]],\n              \n                       [[ 0.0041,  0.0128, -0.0351],\n                        [ 0.0361,  0.0160,  0.0057],\n                        [ 0.1155,  0.1052,  0.0313]],\n              \n                       [[ 0.1854,  0.3121,  0.2305],\n                        [ 0.0756,  0.1597,  0.1699],\n                        [ 0.1683,  0.1896,  0.1402]]],\n              \n              \n                      [[[-0.0114, -0.0183, -0.0210],\n                        [-0.0447, -0.0372,  0.0243],\n                        [-0.0395, -0.0435, -0.0175]],\n              \n                       [[-0.1809, -0.1481, -0.1263],\n                        [-0.0694, -0.0753, -0.0032],\n                        [ 0.0129,  0.0367,  0.0609]],\n              \n                       [[ 0.0511,  0.0194, -0.0465],\n                        [-0.1223, -0.2529, -0.2318],\n                        [-0.0266,  0.0240,  0.1270]],\n              \n                       ...,\n              \n                       [[-0.1562, -0.1172, -0.0211],\n                        [-0.1272, -0.1332, -0.0423],\n                        [ 0.0085,  0.0403,  0.0722]],\n              \n                       [[-0.0882, -0.0620,  0.0757],\n                        [-0.1120, -0.0772,  0.0692],\n                        [ 0.0330,  0.0752,  0.1718]],\n              \n                       [[-0.0020,  0.1348,  0.0182],\n                        [ 0.1368,  0.2147,  0.0265],\n                        [ 0.2279,  0.1143, -0.0174]]],\n              \n              \n                      [[[ 0.0751,  0.0139,  0.1204],\n                        [ 0.0495, -0.0239,  0.0299],\n                        [-0.0143, -0.0716, -0.0169]],\n              \n                       [[-0.0253, -0.0674, -0.1000],\n                        [-0.0538, -0.1034, -0.0410],\n                        [ 0.0406,  0.0268,  0.0532]],\n              \n                       [[-0.0818, -0.0242, -0.0627],\n                        [-0.0107, -0.0269,  0.0683],\n                        [ 0.0054,  0.0278,  0.0931]],\n              \n                       ...,\n              \n                       [[ 0.1202,  0.0717, -0.0609],\n                        [ 0.1188,  0.0804,  0.0371],\n                        [ 0.0763,  0.0638,  0.0139]],\n              \n                       [[-0.0638, -0.0406, -0.0238],\n                        [-0.0399, -0.0350, -0.0332],\n                        [-0.1266, -0.1118, -0.0187]],\n              \n                       [[ 0.1616,  0.0861, -0.0980],\n                        [-0.0388, -0.1303, -0.2731],\n                        [ 0.0042, -0.0964, -0.1940]]]])),\n             ('layer2.0.bn1.weight',\n              tensor([0.6134, 1.0222, 0.7810, 0.6919, 0.6470, 0.8107, 1.0776, 0.6298, 0.9038,\n                      0.7464, 0.7388, 1.0518, 0.7474, 1.0250, 0.7803, 0.8257, 0.6575, 0.9071,\n                      0.6775, 0.7289, 0.7458, 0.6416, 0.7021, 0.7125, 0.7500, 0.8240, 0.7277,\n                      0.7710, 1.1333, 0.7316, 0.6882, 0.5148, 0.7233, 0.5885, 0.7866, 0.9036,\n                      0.9153, 0.7353, 0.6999, 0.6205, 0.7878, 0.7633, 0.6241, 1.3738, 0.6837,\n                      0.8755, 1.5367, 0.8365, 0.9896, 0.9027, 0.8408, 0.8088, 0.6593, 0.6259,\n                      0.7996, 0.7499, 0.6147, 0.9858, 0.8826, 0.8869, 0.7261, 0.8439, 0.6719,\n                      0.7370, 0.6912, 0.7011, 0.8959, 0.7414, 0.9250, 0.8136, 1.0020, 0.9909,\n                      0.6981, 0.7384, 0.6092, 0.7691, 0.7286, 0.7018, 0.5435, 0.7402, 0.6938,\n                      0.9870, 0.8483, 0.7568, 0.5912, 0.8632, 0.5168, 0.8550, 0.9835, 0.7823,\n                      1.0838, 0.7457, 0.8271, 0.7876, 0.6697, 0.5428, 0.7810, 1.3615, 1.2917,\n                      0.6269, 0.8151, 0.5977, 0.7547, 0.8248, 0.7626, 0.6482, 0.8518, 0.8320,\n                      0.7526, 0.9518, 0.9044, 0.7707, 0.6270, 0.6197, 0.9252, 0.9539, 1.0490,\n                      0.6767, 0.8444, 0.8548, 0.8054, 0.7535, 0.5285, 0.6001, 0.7850, 0.9529,\n                      1.3857, 0.8955])),\n             ('layer2.0.bn1.bias',\n              tensor([-0.0790, -0.1869,  0.0069, -0.2567, -0.1319, -0.1998, -0.0789, -0.4573,\n                      -1.1788, -0.2088, -0.0072, -0.1729, -0.1227, -0.3269, -0.2406, -0.1514,\n                      -0.3158,  0.0796, -0.1578, -0.3390, -0.1464, -0.2579, -0.3104, -0.1529,\n                      -0.2419, -0.2497, -0.1347, -0.2045, -0.2320, -0.2525, -0.1859, -0.4288,\n                      -0.1280, -0.3147, -0.3238, -0.0785, -0.0359, -0.2253, -0.2823, -0.1628,\n                       0.0060, -0.0729, -0.1959, -0.1527, -0.7832, -0.4757, -0.1412, -0.2947,\n                      -0.1583,  0.1330, -0.2099, -0.3119, -0.2892, -0.1968, -0.1122, -0.2418,\n                      -0.2900,  0.0803, -0.0122, -0.1089, -0.5665, -0.2500, -0.3210, -0.2618,\n                      -0.1600, -0.2092,  0.1245, -0.2749,  0.0329, -0.1273, -0.0719,  0.1006,\n                      -0.3065, -0.2896, -0.2742, -0.2365, -0.5386, -0.2570, -0.3043, -0.0583,\n                      -0.3426, -0.0383, -0.1609, -0.1122, -0.3256, -0.2554, -0.2954, -0.3992,\n                       0.2561, -0.6131, -0.1494, -0.1756, -0.0291, -0.1786, -0.2210, -0.2954,\n                      -0.1727, -0.2467, -0.1815, -0.3385, -0.2624, -0.3486, -0.3186, -0.2363,\n                      -0.2972, -0.1348, -0.2308, -0.1306, -0.2540,  0.1093, -0.1789, -0.2122,\n                      -0.1817, -0.2754,  0.2875,  0.0888, -0.1883, -0.4364, -0.9711,  0.0651,\n                      -0.2127, -0.3821, -0.3432, -0.2402, -0.4033, -0.0636,  0.0577,  0.1489])),\n             ('layer2.0.bn1.running_mean',\n              tensor([ 4.1209e+00, -4.9623e+00, -6.7538e+00, -1.6548e+01, -8.6060e+00,\n                      -8.8064e+00,  8.3548e-01,  4.3883e+00,  3.5567e+00,  7.9184e+00,\n                       7.3052e+00,  2.0283e-03,  1.9387e+00, -1.6531e+00,  1.3187e+01,\n                       5.6720e+00, -1.5516e+01,  2.6099e+00, -1.2391e+01, -8.4602e-03,\n                       2.7594e+00, -1.9319e+01, -1.9178e+01,  1.2125e+01, -1.3229e+01,\n                       8.5604e+00, -1.2544e+01, -1.2616e+01, -8.1914e-01,  1.2840e+01,\n                      -1.4207e+01, -5.6873e+00,  2.2095e+00,  1.1393e+01,  7.5784e+00,\n                      -8.3773e+00,  3.8204e+00, -6.1223e+00, -6.0177e+00, -1.1427e-01,\n                       1.3264e+00,  1.0121e+01, -3.0730e+00, -1.9108e+00,  5.0074e+00,\n                       2.2534e+00, -2.8161e+00,  6.2437e+00,  1.0737e+01, -1.5390e+01,\n                       2.0764e+00, -5.8047e+00,  1.6070e+00, -1.5722e+01, -5.0595e-01,\n                      -2.0024e+01,  3.4287e+00, -1.0273e+01, -5.8228e+00,  2.6708e+00,\n                      -5.4524e+00,  5.4357e+00,  1.4702e+01, -1.9465e+01, -1.0418e+01,\n                      -4.5286e+00, -3.9648e+00, -4.8691e+00, -6.6531e+00, -5.5587e+00,\n                       1.8654e+00,  5.2721e+00, -2.0852e+01, -2.0774e+01,  9.1420e+00,\n                      -1.9691e+01,  1.1767e+01, -2.0574e+01, -1.8227e+01,  2.7902e+00,\n                       1.3663e+01,  9.4495e-01,  4.9125e+00,  3.8475e+00, -4.2114e-01,\n                      -8.3386e+00,  8.1251e+00, -3.7292e+00, -9.7518e+00,  4.1920e+00,\n                      -1.2169e+01,  4.4888e+00, -7.9631e+00, -1.4313e+01,  6.3952e+00,\n                      -5.7244e+00, -1.9192e+01,  1.5036e+00, -1.9621e+00, -1.6811e+01,\n                      -2.3281e+00, -1.9064e+00,  5.1790e-01, -1.0661e+00, -1.6199e+01,\n                      -3.9395e+00,  2.4505e+00,  4.2805e+00, -3.6224e-01, -5.0274e+00,\n                       9.1225e+00, -2.4576e+00, -1.2537e+01, -1.6472e+01, -1.7084e+01,\n                      -6.7556e+00, -1.6985e+00,  1.5181e+01,  5.3259e+00, -1.7506e+01,\n                      -3.2982e-01,  1.8987e+00, -5.8074e+00,  4.1931e+00, -2.5241e+00,\n                      -5.1846e+00, -2.8902e-01, -1.7992e+01])),\n             ('layer2.0.bn1.running_var',\n              tensor([ 55.1334,  62.4046,  50.9000, 134.9267,  69.4315,  46.9516,  32.2772,\n                       76.8259,  30.2142,  69.5650,  67.7346,  46.8043,  39.5476,  42.6293,\n                       70.5354,  46.7776, 125.9753,  32.9516,  63.0988,  66.8531,  73.1353,\n                      169.4707, 126.7761, 101.2628,  91.4809,  35.1977,  71.4174, 145.3392,\n                       37.7220,  78.6763, 115.0666,  57.6789,  32.2394,  73.9734,  40.6503,\n                       59.2973,  65.2972,  91.4036,  60.4229,  67.1025,  40.8267,  78.6450,\n                       40.4023,  45.6411,  48.9880,  44.7925,  43.8459,  55.1179,  41.8936,\n                       83.1100,  36.9099,  75.5162,  61.2036,  97.4159,  45.4378, 142.7254,\n                       92.1059,  49.4823,  55.1437,  50.6055,  47.4156,  30.2854,  63.0531,\n                      172.2724,  93.6593,  41.7690,  25.0404,  53.9234,  53.3205,  61.0522,\n                       35.3215,  40.4379, 172.7116, 196.4942,  88.4119, 133.9442,  42.9238,\n                      196.0559,  89.9364,  33.0453,  83.2306,  46.0734,  66.1666,  65.0001,\n                       43.9299,  97.8267,  41.9914,  49.1468,  34.0561,  48.4222,  49.8119,\n                       81.5165,  41.9542,  64.3640,  72.2772,  76.3811,  87.7729,  30.4699,\n                       41.7243, 163.7922,  53.5363,  79.4034,  35.3020,  42.4892,  99.6083,\n                       30.1569,  42.8563,  48.3182,  46.4494,  23.9211,  71.5127,  38.0303,\n                       64.0051, 124.7478,  41.6209,  35.4315,  31.5485, 109.2015,  32.5656,\n                       69.1497,  58.9025,  30.7067,  73.2602,  62.0791,  48.4799,  46.8389,\n                       31.6163,  62.6288])),\n             ('layer2.0.bn1.num_batches_tracked', tensor(14770)),\n             ('layer2.0.conv2.weight',\n              tensor([[[[ 3.5801e-02,  1.3240e-01, -5.4290e-02],\n                        [-1.8036e-03,  2.4940e-02, -1.3672e-02],\n                        [-1.1689e-01, -7.5107e-02, -1.3439e-01]],\n              \n                       [[-9.0310e-02, -1.0578e-01,  2.3978e-02],\n                        [-9.0807e-03, -1.6579e-02,  3.6473e-02],\n                        [-1.5790e-01, -8.4830e-02,  3.7259e-02]],\n              \n                       [[-4.9794e-02, -1.8431e-02, -6.1605e-02],\n                        [-8.8847e-03,  1.2568e-03,  3.0755e-02],\n                        [-4.8227e-02, -5.1341e-02, -1.7537e-02]],\n              \n                       ...,\n              \n                       [[ 4.6410e-02,  8.1207e-02, -3.0802e-03],\n                        [-8.3120e-02,  1.4837e-01, -1.4664e-02],\n                        [-1.6309e-01,  1.4790e-02,  5.7556e-02]],\n              \n                       [[-1.3220e-01, -3.7188e-02, -1.0852e-01],\n                        [-5.7340e-02,  5.0669e-02,  5.7932e-02],\n                        [ 6.5463e-02,  1.5733e-01,  1.8176e-01]],\n              \n                       [[-1.6333e-01, -2.7075e-01, -1.7215e-01],\n                        [-7.7638e-02, -1.2614e-01, -6.5702e-02],\n                        [-3.6707e-03,  4.6832e-02,  4.2311e-02]]],\n              \n              \n                      [[[-1.4180e-01, -1.6493e-01,  2.9355e-02],\n                        [-1.3146e-01, -6.4907e-02, -9.0453e-03],\n                        [ 1.4242e-01,  2.8171e-02, -5.7050e-04]],\n              \n                       [[-1.0268e-02,  1.2513e-01,  9.4676e-02],\n                        [ 3.4087e-02, -1.9172e-02, -2.9008e-02],\n                        [ 3.0332e-01,  1.7188e-01,  3.2988e-01]],\n              \n                       [[-1.7140e-02, -6.6407e-02, -1.0188e-01],\n                        [-1.4512e-01, -4.3981e-02, -1.0823e-01],\n                        [-8.2077e-02,  1.0352e-01,  4.4443e-03]],\n              \n                       ...,\n              \n                       [[ 2.0774e-01, -2.1379e-03, -9.2778e-02],\n                        [ 1.3649e-01, -6.6127e-02, -4.4744e-02],\n                        [ 2.9556e-02,  1.0244e-02,  7.7245e-02]],\n              \n                       [[-1.1152e-01,  6.5462e-03, -8.2931e-02],\n                        [-4.2178e-02, -2.3308e-01,  5.2366e-04],\n                        [ 7.2607e-02,  7.4133e-03,  1.1952e-01]],\n              \n                       [[ 1.7376e-02,  1.3439e-01,  1.4233e-01],\n                        [-1.3419e-01,  2.7399e-02,  1.0730e-01],\n                        [-1.8993e-02,  8.7700e-03,  1.2544e-01]]],\n              \n              \n                      [[[-1.2157e-01, -1.8317e-02,  8.1200e-02],\n                        [-2.0050e-01, -9.7537e-02, -8.2173e-02],\n                        [ 6.9067e-02, -6.8818e-02, -5.9485e-02]],\n              \n                       [[ 1.4536e-01,  1.9601e-01,  1.7496e-01],\n                        [-1.0737e-02,  3.6350e-02, -1.4850e-01],\n                        [-8.6807e-04, -9.4118e-02, -1.7735e-01]],\n              \n                       [[-1.7363e-02, -7.5268e-02, -1.7642e-01],\n                        [ 2.7279e-02, -9.3642e-02, -6.0860e-02],\n                        [-1.0044e-01, -2.4384e-02, -8.8497e-02]],\n              \n                       ...,\n              \n                       [[ 1.3081e-01, -9.9761e-02, -9.8485e-02],\n                        [ 2.4875e-01, -3.4535e-02,  2.8311e-02],\n                        [ 2.7220e-01,  1.5995e-01,  2.0722e-01]],\n              \n                       [[-3.9993e-03,  1.1203e-01, -4.3970e-02],\n                        [-1.9230e-01, -2.6667e-01,  3.7191e-02],\n                        [-8.8728e-02,  7.7421e-02,  1.6630e-01]],\n              \n                       [[-3.1391e-02,  5.0552e-02, -1.0317e-01],\n                        [-1.8551e-01,  1.3258e-01,  5.6104e-02],\n                        [-2.0102e-01, -4.4937e-02, -2.0744e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-9.4502e-02,  1.0528e-01, -1.7363e-03],\n                        [ 4.1220e-02,  2.0488e-01,  5.4001e-02],\n                        [-7.1703e-02, -1.9960e-02, -1.5586e-01]],\n              \n                       [[-5.0622e-03, -1.9191e-01, -1.1044e-01],\n                        [ 2.8142e-02, -1.6692e-01,  5.6854e-02],\n                        [ 3.3312e-02, -1.3556e-02,  1.5202e-01]],\n              \n                       [[ 1.7013e-02,  7.2815e-03, -1.4439e-02],\n                        [ 1.4262e-01,  1.4621e-02,  1.1073e-02],\n                        [ 6.1023e-02,  9.5766e-02,  4.3382e-02]],\n              \n                       ...,\n              \n                       [[ 3.6952e-02, -3.7726e-02, -3.9187e-02],\n                        [ 7.3322e-02,  6.6987e-02, -3.2104e-02],\n                        [ 5.2040e-02,  4.3732e-02,  7.4937e-02]],\n              \n                       [[-2.0813e-01, -1.7508e-01, -1.9377e-02],\n                        [-3.5919e-02,  2.5197e-01, -1.5524e-02],\n                        [ 1.2569e-01,  1.0979e-01,  1.2982e-01]],\n              \n                       [[-1.8244e-01, -2.9352e-01, -1.9207e-01],\n                        [-1.0618e-01, -1.0968e-01, -1.4454e-01],\n                        [-2.8780e-02, -1.3570e-01, -1.1764e-01]]],\n              \n              \n                      [[[-6.6411e-02, -1.6646e-02,  4.8790e-04],\n                        [-1.3186e-01,  3.2583e-02, -6.9171e-05],\n                        [-1.1551e-01, -9.4039e-02, -4.2402e-02]],\n              \n                       [[-1.2871e-01, -1.2930e-01, -5.0688e-03],\n                        [-2.0826e-01, -4.5276e-02,  4.5756e-02],\n                        [ 3.6496e-02,  1.6634e-01, -9.7669e-02]],\n              \n                       [[-2.7398e-03, -1.3537e-01, -1.5237e-01],\n                        [-1.3347e-01, -2.3328e-02, -1.1535e-01],\n                        [-1.1356e-01, -4.3647e-02, -7.8738e-04]],\n              \n                       ...,\n              \n                       [[ 8.9676e-02,  1.4685e-01,  4.5372e-02],\n                        [ 1.3160e-01,  4.3171e-02, -9.2407e-03],\n                        [-1.2996e-03, -2.7926e-02,  2.1524e-03]],\n              \n                       [[-1.8576e-01,  1.2650e-01,  7.1942e-02],\n                        [-2.8623e-02,  1.6864e-01,  7.9292e-02],\n                        [-1.5069e-01,  1.4628e-01,  1.1273e-02]],\n              \n                       [[-2.9922e-02, -3.5931e-02, -1.2499e-02],\n                        [ 4.0491e-02, -8.0361e-02,  1.0543e-01],\n                        [-9.2890e-02, -1.1158e-01, -1.2076e-01]]],\n              \n              \n                      [[[ 9.0650e-03, -1.4141e-01,  6.3999e-03],\n                        [-1.8173e-02, -8.9185e-02, -3.0831e-02],\n                        [ 1.4458e-02, -5.1872e-02,  7.4001e-02]],\n              \n                       [[-6.3484e-02, -2.4432e-02,  2.2120e-01],\n                        [-2.0744e-01,  1.8460e-02,  3.7766e-02],\n                        [-8.2261e-02, -5.9086e-02,  1.1641e-01]],\n              \n                       [[-7.1267e-02, -6.2843e-02, -4.0835e-02],\n                        [-5.7118e-02, -6.2611e-02,  8.2103e-02],\n                        [-9.5978e-02, -8.9341e-02,  2.1220e-01]],\n              \n                       ...,\n              \n                       [[ 1.2472e-01,  4.6290e-02, -1.1613e-01],\n                        [ 1.2086e-01,  1.0309e-01, -1.7575e-01],\n                        [ 8.3764e-02,  7.6414e-02, -1.3470e-01]],\n              \n                       [[ 1.2528e-01, -4.8583e-02, -1.6973e-02],\n                        [-3.4438e-03, -3.3502e-01, -2.7535e-01],\n                        [ 6.6604e-03, -4.4797e-02,  8.4230e-04]],\n              \n                       [[-5.0038e-02, -2.7921e-03,  2.7042e-01],\n                        [-5.3663e-03, -5.1382e-02,  2.1073e-01],\n                        [ 9.3974e-02,  1.0339e-01,  3.8057e-01]]]])),\n             ('layer2.0.bn2.weight',\n              tensor([0.6577, 0.5118, 0.8078, 0.5694, 0.6965, 0.5746, 0.7737, 0.6717, 0.8194,\n                      0.7064, 0.4945, 0.5833, 0.5435, 0.7800, 0.5933, 0.6866, 0.7126, 1.0045,\n                      0.5890, 0.6660, 0.8094, 0.6657, 0.7991, 0.8141, 0.7056, 0.6736, 0.5962,\n                      0.8255, 0.7576, 0.6523, 0.7532, 1.0810, 0.6127, 0.6299, 0.7645, 0.5347,\n                      0.6505, 0.7153, 0.4856, 0.6341, 0.6910, 0.6699, 0.7503, 0.6819, 0.9162,\n                      0.7519, 0.7908, 1.2308, 0.6911, 0.6552, 0.8100, 0.5959, 0.8930, 0.7586,\n                      0.7202, 0.7424, 0.9509, 0.7953, 0.5943, 0.6324, 0.6956, 0.8068, 0.7295,\n                      0.7074, 0.8315, 0.8682, 0.6716, 0.6816, 0.6579, 0.4632, 0.6663, 1.2411,\n                      0.6367, 0.7639, 0.6698, 0.6096, 0.6367, 0.8152, 0.7718, 0.7289, 1.1551,\n                      0.8006, 0.9205, 0.7174, 0.6325, 1.0895, 1.0311, 1.1262, 0.7004, 0.6115,\n                      0.7981, 0.7650, 1.0933, 0.7253, 0.5805, 0.6170, 0.7113, 0.8955, 0.7811,\n                      0.8537, 0.6820, 0.5815, 0.6565, 0.6634, 0.6738, 0.8298, 0.6158, 0.7774,\n                      0.5220, 0.6127, 0.7268, 0.9472, 0.7954, 0.6999, 0.6721, 0.7422, 0.5784,\n                      0.6993, 0.5309, 0.8259, 0.7583, 0.6628, 0.7361, 0.5603, 0.6798, 0.6898,\n                      0.7512, 0.9841])),\n             ('layer2.0.bn2.bias',\n              tensor([-0.0589, -0.1193, -0.0089, -0.1119,  0.0361, -0.1816,  0.1708,  0.0488,\n                      -0.2814, -0.0590, -0.0417,  0.0347, -0.2159,  0.0516, -0.0415, -0.1442,\n                       0.1872, -0.2314,  0.0340, -0.0484,  0.0088, -0.0565,  0.0416,  0.0141,\n                       0.0009, -0.0270,  0.0357,  0.0515, -0.2541, -0.0140,  0.1478, -0.0194,\n                      -0.0599,  0.0063, -0.1707, -0.0553, -0.0436, -0.0996,  0.1390,  0.0382,\n                      -0.2057,  0.0083, -0.1432, -0.0594, -0.1167,  0.0882, -0.0723, -0.1300,\n                      -0.0745, -0.1867,  0.1678,  0.1488, -0.2187,  0.0281,  0.0759,  0.0717,\n                       0.1589, -0.0977,  0.0126,  0.0972,  0.0086, -0.0478, -0.0640,  0.0453,\n                      -0.2356, -0.1474, -0.0453, -0.1086, -0.0457,  0.0395, -0.0317, -0.1708,\n                       0.0226, -0.0384, -0.1183, -0.2796,  0.0429,  0.0333, -0.0171, -0.0324,\n                      -0.2253, -0.1943, -0.1839, -0.1824, -0.1590, -0.1076, -0.0141, -0.0763,\n                      -0.1525, -0.0036, -0.1619, -0.1059, -0.1481,  0.0374, -0.0519, -0.0216,\n                      -0.0109, -0.0134, -0.0254, -0.1627,  0.0200, -0.1476, -0.0603,  0.0163,\n                       0.0326, -0.0509,  0.0157, -0.0305,  0.0275, -0.1200,  0.0329, -0.0174,\n                       0.0759, -0.0609, -0.1446, -0.1355, -0.0357,  0.0091, -0.0222, -0.0241,\n                      -0.1326, -0.0339, -0.1004,  0.1220,  0.1289,  0.0472, -0.1749, -0.1778])),\n             ('layer2.0.bn2.running_mean',\n              tensor([-13.5770,   4.2140,  -4.7748,  -8.2909,  -5.8646,   4.0038,  -9.5529,\n                        6.1094,  -0.6456,  -2.8429,  -9.5049,  -4.4268,  -2.7800,  -8.5688,\n                       -2.0064,  -5.6678,  -6.7595,   5.2602,   7.3776,  -6.6356,   6.0395,\n                       -3.4436,  -3.9014,   4.9311, -10.3050,   0.5134,  -7.1367,   3.4454,\n                       -0.4371,  -2.3029,  -3.2537,   0.8512,   6.6621,  -3.4748,   8.0398,\n                        6.9319,  -1.1044,   5.1875,  -9.3360,  -7.2005,   3.5278,  -6.6447,\n                        2.1923, -12.4781,  -5.6459,  -7.4121,   4.6116,   1.5467,  -1.0006,\n                        4.5802,  -5.0516,  -4.4139,  -1.7943,  -4.0864,   1.6912,  -5.9307,\n                       -1.4010,   2.6030,   2.7360,  -1.8157,  12.6657,  -0.7540,  -0.5401,\n                       -9.5814,  -1.1584,  -0.2269,  -9.1129,  -1.9534,  -0.3810,  -6.6774,\n                       -8.6777,  -1.6651,  -5.4415,  -3.1595,  -4.9190,   4.5150,  -0.9443,\n                        8.9201,  -8.6564,  -4.0799,  -3.4513,   0.7491,   2.5151,   1.1123,\n                        9.1904,  -3.4404,  -3.6951,  -1.2499,   6.2097,  -0.7321,  -1.7109,\n                        3.9846,  -1.7369, -10.5597,  -3.5835,   3.7829,  -0.0622,  -2.7304,\n                       -1.0503,   4.9344,   3.4011,  -4.7707,  -5.9463,  -2.2911, -10.6537,\n                        2.0938,  -6.0722, -10.2146,  -4.0509,  -4.8341,  -8.8447,  -7.3939,\n                       -7.4103,   7.0856,   0.2281,  -6.0793, -12.5681,  -5.1561, -10.3493,\n                       -0.3206,   2.3449,  -4.5123,  -2.6114,  -5.7357,  -2.7772,  -7.4424,\n                        0.4648,  -2.5017])),\n             ('layer2.0.bn2.running_var',\n              tensor([197.6742,  64.2352,  33.2637, 171.3137,  17.5549, 106.7100, 221.7563,\n                      190.0669,  48.4596,  65.8227, 235.1169, 111.4200, 136.3803,  40.1072,\n                       65.5406,  29.2109, 112.7780, 150.3714, 133.9124,  76.6826,  53.9462,\n                       79.5930,  17.8218, 104.6718, 134.4290, 152.0777, 174.4032,  78.6358,\n                       28.0101, 190.8961,  86.1515,  23.2568, 110.6282,  59.6461, 139.6725,\n                       86.1084, 140.5526, 114.3700, 227.9780, 128.0989,  51.8052, 117.1612,\n                       21.0112, 140.2381,  19.4945,  32.4074,  84.4341,  22.3890, 151.0523,\n                       62.6492,  16.3967, 101.6144,  18.2463,  52.0702,  62.8029, 122.1665,\n                       20.4848,  87.9352, 119.1143,  72.9450, 117.5448,  23.7911,  36.5578,\n                      143.0677,  52.9075,  63.2185,  98.5059,  44.4582, 110.5517, 167.9270,\n                      126.2828,  26.8916, 138.7760, 106.8816,  71.2996,  64.7501,  58.5670,\n                      157.1947,  50.8761,  43.7149,  28.0322,  71.7278,  15.0055,  65.6787,\n                      121.8646,  18.9348,  16.7670,  22.5916,  40.9791,  97.8768,  66.2227,\n                       50.6567,  27.1301, 101.8207, 142.7595,  40.1828,  57.4737,  16.0975,\n                       25.7872, 120.3929, 115.1394,  55.5642,  94.5754,  57.6179, 194.4453,\n                       34.8477,  20.2666,  68.6495, 132.7943, 163.6256, 111.8340,  53.2291,\n                      165.0342, 101.2292,  23.3091, 142.5979, 174.0289, 136.1597, 289.9181,\n                       13.4299,  53.3485,  75.3207,  98.2696, 205.8018,  33.8526, 174.0713,\n                       96.3458,  23.1842])),\n             ('layer2.0.bn2.num_batches_tracked', tensor(14770)),\n             ('layer2.0.shortcut.0.weight',\n              tensor([[[[ 0.0177]],\n              \n                       [[-0.3160]],\n              \n                       [[-0.1060]],\n              \n                       ...,\n              \n                       [[ 0.1394]],\n              \n                       [[-0.1693]],\n              \n                       [[ 0.1079]]],\n              \n              \n                      [[[ 0.0104]],\n              \n                       [[ 0.0029]],\n              \n                       [[-0.1573]],\n              \n                       ...,\n              \n                       [[-0.0838]],\n              \n                       [[ 0.0423]],\n              \n                       [[-0.0958]]],\n              \n              \n                      [[[-0.0823]],\n              \n                       [[-0.0181]],\n              \n                       [[-0.0436]],\n              \n                       ...,\n              \n                       [[-0.2236]],\n              \n                       [[ 0.0543]],\n              \n                       [[-0.1667]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.2573]],\n              \n                       [[-0.0814]],\n              \n                       [[ 0.2193]],\n              \n                       ...,\n              \n                       [[-0.0428]],\n              \n                       [[-0.2197]],\n              \n                       [[ 0.1457]]],\n              \n              \n                      [[[ 0.0707]],\n              \n                       [[-0.2219]],\n              \n                       [[ 0.0773]],\n              \n                       ...,\n              \n                       [[ 0.0802]],\n              \n                       [[-0.1531]],\n              \n                       [[ 0.1615]]],\n              \n              \n                      [[[-0.0734]],\n              \n                       [[-0.0819]],\n              \n                       [[ 0.0814]],\n              \n                       ...,\n              \n                       [[ 0.1435]],\n              \n                       [[ 0.1250]],\n              \n                       [[-0.0725]]]])),\n             ('layer2.0.shortcut.1.weight',\n              tensor([0.7368, 0.9680, 1.2037, 0.9564, 0.9493, 0.9577, 0.9916, 1.0672, 0.9494,\n                      0.8332, 1.0011, 1.1071, 1.0567, 1.0757, 1.1161, 0.8648, 1.1397, 0.9912,\n                      1.2053, 0.8574, 0.9652, 1.0982, 1.0216, 1.0663, 0.8281, 1.0882, 1.0989,\n                      0.8387, 0.9111, 0.9915, 1.1775, 0.9558, 0.9280, 1.0262, 1.0949, 0.9765,\n                      1.0226, 0.9611, 1.0517, 0.9731, 1.0375, 1.0633, 1.1329, 0.8013, 1.1629,\n                      1.1824, 1.0977, 0.8681, 0.9754, 1.1008, 1.1079, 1.2029, 1.1652, 0.8304,\n                      0.9350, 1.0614, 0.9982, 0.9342, 1.0483, 1.0610, 1.1489, 1.0674, 1.1209,\n                      1.0371, 0.9450, 0.8378, 0.8533, 0.9814, 1.1446, 1.0196, 1.0823, 0.7229,\n                      1.0540, 0.8670, 0.8691, 0.8650, 1.1184, 1.0176, 0.9209, 0.9220, 1.0640,\n                      1.0159, 1.1735, 0.8813, 1.0717, 0.9766, 0.8199, 0.9195, 0.8406, 1.2634,\n                      0.9488, 0.8924, 1.0015, 0.9561, 1.0412, 1.2393, 1.1079, 1.0895, 0.8962,\n                      1.0423, 1.0511, 1.1169, 1.0600, 0.9519, 1.0750, 0.7825, 1.1935, 1.0361,\n                      1.2503, 0.9696, 1.0715, 1.0065, 0.8901, 1.2161, 1.0652, 0.7200, 0.9543,\n                      1.0928, 0.7902, 1.0398, 1.1519, 1.0680, 1.0072, 1.1711, 1.1704, 1.0152,\n                      1.1213, 1.0036])),\n             ('layer2.0.shortcut.1.bias',\n              tensor([-0.0589, -0.1193, -0.0089, -0.1119,  0.0361, -0.1816,  0.1708,  0.0488,\n                      -0.2814, -0.0590, -0.0417,  0.0347, -0.2159,  0.0516, -0.0415, -0.1442,\n                       0.1872, -0.2314,  0.0340, -0.0484,  0.0088, -0.0565,  0.0416,  0.0141,\n                       0.0009, -0.0270,  0.0357,  0.0515, -0.2541, -0.0140,  0.1478, -0.0194,\n                      -0.0599,  0.0063, -0.1707, -0.0553, -0.0436, -0.0996,  0.1390,  0.0382,\n                      -0.2057,  0.0083, -0.1432, -0.0594, -0.1167,  0.0882, -0.0723, -0.1300,\n                      -0.0745, -0.1867,  0.1678,  0.1488, -0.2187,  0.0281,  0.0759,  0.0717,\n                       0.1589, -0.0977,  0.0126,  0.0972,  0.0086, -0.0478, -0.0640,  0.0453,\n                      -0.2356, -0.1474, -0.0453, -0.1086, -0.0457,  0.0395, -0.0317, -0.1708,\n                       0.0226, -0.0384, -0.1183, -0.2796,  0.0429,  0.0333, -0.0171, -0.0324,\n                      -0.2253, -0.1943, -0.1839, -0.1824, -0.1590, -0.1076, -0.0141, -0.0763,\n                      -0.1525, -0.0036, -0.1619, -0.1059, -0.1481,  0.0374, -0.0519, -0.0216,\n                      -0.0109, -0.0134, -0.0254, -0.1627,  0.0200, -0.1476, -0.0603,  0.0163,\n                       0.0326, -0.0509,  0.0157, -0.0305,  0.0275, -0.1200,  0.0329, -0.0174,\n                       0.0759, -0.0609, -0.1446, -0.1355, -0.0357,  0.0091, -0.0222, -0.0241,\n                      -0.1326, -0.0339, -0.1004,  0.1220,  0.1289,  0.0472, -0.1749, -0.1778])),\n             ('layer2.0.shortcut.1.running_mean',\n              tensor([ 0.4115, -0.9973, -3.8073, -0.5123,  1.0239,  1.1557,  0.9542,  1.5496,\n                      -0.7323, -2.1955,  0.7337,  0.3562, -3.5859, -3.3681, -4.0707, -1.0903,\n                      -0.4623, -2.4795, -2.4944, -1.4823, -0.7690,  0.9773,  1.6520, -0.1771,\n                      -0.9915,  1.0075, -0.7689,  0.7875,  1.6614, -0.3133, -2.4886,  3.3075,\n                      -0.4175, -1.3878,  1.0926,  1.5230,  0.1379, -0.2698,  1.2550,  0.0326,\n                       0.8788,  0.5574,  2.6353, -1.5197,  0.1169,  1.9813,  2.2427, -4.0162,\n                      -0.1949, -4.1639,  0.0348, -1.8801,  2.0094,  0.5792, -0.8278,  0.5751,\n                      -0.0289,  1.0693, -0.7418,  0.8068, -2.5509, -1.1869, -4.6845,  1.5307,\n                       0.5275, -0.5840,  0.9732, -1.9652, -0.1028,  0.9331,  1.6788,  0.1919,\n                      -2.7058, -0.6962, -0.3033, -1.9786, -1.6098, -3.1101, -0.3977, -0.6215,\n                       2.8716,  1.5397,  2.5187, -1.0662, -0.4044, -0.0120, -0.8375,  0.1305,\n                       0.9779, -2.7040, -1.0642,  0.2000, -0.4119, -0.4748, -0.9200, -1.8593,\n                       0.3621,  1.0665, -0.3082,  0.3654,  0.4915,  2.9277, -1.5849, -2.7957,\n                      -0.3085,  0.2778,  0.1716, -0.1562, -2.2252,  0.1360,  0.7876,  1.0335,\n                       0.5040,  1.8830, -0.2056,  1.1333,  1.5483, -0.1270,  0.2071,  1.7760,\n                      -4.1840,  0.1275,  0.9648, -0.7364,  1.2102,  3.8724,  1.7978,  0.6265])),\n             ('layer2.0.shortcut.1.running_var',\n              tensor([2.6287, 1.5078, 1.8648, 1.3138, 1.8051, 1.3020, 1.8047, 3.2533, 1.8073,\n                      3.2840, 1.3426, 1.5360, 4.3148, 2.4233, 4.6277, 1.3671, 2.0650, 2.6728,\n                      2.5693, 2.1015, 2.0433, 1.1779, 1.5382, 1.7146, 1.8396, 1.8207, 2.7035,\n                      3.1751, 1.5921, 1.7959, 2.9569, 3.0701, 1.1862, 1.8057, 1.6374, 1.5463,\n                      2.3446, 2.0070, 1.7803, 1.2819, 0.9331, 1.3540, 3.2095, 1.9765, 1.5608,\n                      1.3265, 2.4879, 3.6785, 1.6261, 3.3173, 2.0491, 3.3243, 2.0250, 1.6490,\n                      1.5968, 1.2423, 1.6132, 2.9618, 1.5451, 1.6178, 3.0535, 1.3765, 5.8159,\n                      2.3090, 1.9825, 1.1913, 1.5554, 1.7802, 1.8659, 2.1132, 2.1850, 1.8486,\n                      2.2446, 1.5916, 1.7518, 2.2578, 2.6792, 3.7670, 1.7671, 1.6695, 1.7014,\n                      1.1429, 2.5866, 2.1882, 0.9968, 1.8870, 1.4195, 2.0928, 2.5253, 3.5368,\n                      1.5969, 1.5955, 2.2376, 1.7224, 1.6044, 2.0210, 1.5657, 1.7393, 1.2774,\n                      1.0229, 1.1810, 1.9581, 0.8282, 2.2992, 1.7276, 1.8672, 1.6402, 1.5459,\n                      4.0103, 1.6050, 2.3439, 2.0357, 1.8463, 1.4181, 1.8961, 2.7484, 1.6487,\n                      1.6334, 1.8920, 1.7944, 4.4940, 1.7011, 1.3426, 1.8360, 2.0470, 4.3153,\n                      1.4311, 1.0909])),\n             ('layer2.0.shortcut.1.num_batches_tracked', tensor(14770)),\n             ('layer2.1.conv1.weight',\n              tensor([[[[ 3.2100e-02,  1.0460e-01,  9.5459e-02],\n                        [ 9.0663e-02,  7.3382e-02,  4.4730e-02],\n                        [ 4.7326e-02,  7.6245e-02,  7.1827e-02]],\n              \n                       [[ 4.8367e-02,  2.9622e-02, -6.8132e-02],\n                        [ 3.3837e-02,  4.2062e-03, -2.4448e-02],\n                        [ 4.5123e-02, -6.1435e-03,  3.2011e-02]],\n              \n                       [[-1.0970e-01,  9.7401e-04,  9.0467e-04],\n                        [-9.0642e-02,  6.0060e-02,  1.0182e-01],\n                        [-1.7669e-01, -3.3079e-02,  4.2465e-02]],\n              \n                       ...,\n              \n                       [[-1.0521e-01, -1.6188e-01, -2.8798e-01],\n                        [-6.7048e-02, -1.8152e-01, -2.4710e-01],\n                        [-6.6556e-02, -1.4099e-01, -9.3303e-02]],\n              \n                       [[-4.4892e-02,  1.1680e-02, -2.6722e-02],\n                        [-4.9371e-02, -6.1906e-02, -5.5995e-02],\n                        [-7.2131e-02, -7.8026e-02, -8.3362e-02]],\n              \n                       [[ 7.4698e-02,  1.8083e-01,  1.5758e-01],\n                        [ 7.1278e-04,  9.6384e-02,  1.5929e-01],\n                        [ 3.4511e-02,  8.6681e-02,  1.4734e-01]]],\n              \n              \n                      [[[ 8.0237e-02,  1.5541e-01,  1.5224e-01],\n                        [ 1.0092e-01,  1.5980e-01,  2.0517e-01],\n                        [ 8.3220e-02,  1.5977e-01,  1.4761e-01]],\n              \n                       [[ 1.3133e-01,  1.4817e-01,  1.2637e-01],\n                        [ 1.4177e-01,  1.8302e-01,  1.3073e-01],\n                        [ 1.7032e-01,  1.2736e-01,  1.7743e-01]],\n              \n                       [[-2.0783e-02,  1.5048e-02, -2.0250e-02],\n                        [ 2.7401e-02,  6.0909e-02,  8.7436e-02],\n                        [-2.3046e-02, -1.6914e-04, -2.2176e-02]],\n              \n                       ...,\n              \n                       [[-7.9825e-02, -5.9052e-03,  3.1849e-03],\n                        [-5.0999e-02,  3.4883e-02,  3.0436e-02],\n                        [-4.0058e-02,  3.0147e-02,  2.1094e-03]],\n              \n                       [[ 5.3402e-03, -9.1757e-02, -9.9656e-02],\n                        [-5.9672e-03, -4.8077e-02, -5.8059e-02],\n                        [-1.0637e-02, -1.5236e-02, -1.9705e-02]],\n              \n                       [[-1.0032e-01, -1.5350e-01, -1.5077e-01],\n                        [-1.0058e-01, -1.2052e-01, -8.3111e-02],\n                        [-9.6743e-02, -1.4515e-01, -1.1003e-01]]],\n              \n              \n                      [[[-1.0665e-01, -2.4064e-02,  1.1310e-01],\n                        [-1.0937e-01,  1.0504e-01,  1.6052e-01],\n                        [-1.5125e-01,  1.6300e-03,  7.6901e-02]],\n              \n                       [[-3.6275e-02, -5.8629e-02, -2.5732e-01],\n                        [-3.0183e-02, -8.1318e-02, -1.1427e-01],\n                        [-3.3282e-02, -4.1921e-03, -1.3704e-02]],\n              \n                       [[ 7.6188e-04,  8.0163e-03, -2.8835e-02],\n                        [ 2.8878e-02,  2.4115e-01,  1.2452e-01],\n                        [-1.4770e-01,  7.2152e-02, -3.3143e-02]],\n              \n                       ...,\n              \n                       [[-4.0436e-02,  6.0638e-02,  2.6638e-03],\n                        [-5.6797e-02,  6.5651e-02,  1.3601e-01],\n                        [-1.8702e-01, -2.2287e-02,  1.2792e-01]],\n              \n                       [[-1.7089e-01,  1.5321e-02,  1.0474e-01],\n                        [-1.7762e-01, -5.7380e-02,  2.0386e-01],\n                        [-2.2481e-01, -1.3774e-01,  1.8029e-01]],\n              \n                       [[-3.0477e-01,  6.6511e-02, -2.4691e-02],\n                        [-1.7370e-01,  6.8311e-02, -1.5986e-02],\n                        [-2.1973e-01, -1.0829e-01, -1.4451e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 5.0200e-02, -4.3333e-02,  1.1784e-02],\n                        [ 8.6298e-02,  2.2211e-02,  6.3304e-03],\n                        [ 5.1099e-02, -6.5949e-03,  3.5861e-02]],\n              \n                       [[-1.7469e-01, -1.0085e-01,  7.2674e-02],\n                        [-1.7695e-01, -1.9473e-01, -2.9625e-03],\n                        [-1.9431e-01, -2.2080e-01, -1.3132e-01]],\n              \n                       [[ 8.3710e-03, -1.0960e-01, -7.6957e-02],\n                        [ 5.9591e-02, -6.5013e-02, -7.1151e-02],\n                        [ 4.1561e-02,  4.7757e-03,  1.0576e-01]],\n              \n                       ...,\n              \n                       [[-7.8984e-03, -1.0063e-01,  5.5861e-02],\n                        [ 8.4683e-03, -1.4414e-01,  5.2356e-02],\n                        [-8.4835e-02, -1.2764e-01,  5.4194e-02]],\n              \n                       [[-8.0547e-02, -2.5891e-01,  1.7718e-02],\n                        [ 3.0224e-02, -1.7904e-01, -7.8708e-02],\n                        [-5.7783e-04, -3.7714e-02,  1.2775e-01]],\n              \n                       [[-2.4452e-01, -2.5193e-01,  8.3609e-02],\n                        [-1.0243e-01, -3.0191e-01, -1.0805e-01],\n                        [-3.0731e-02, -2.5908e-01, -8.1150e-02]]],\n              \n              \n                      [[[ 1.1518e-01,  9.1219e-03, -8.3510e-02],\n                        [ 7.1741e-02, -1.2788e-01, -3.3313e-02],\n                        [ 6.0717e-02,  9.0502e-02,  7.3017e-02]],\n              \n                       [[-7.5252e-02, -1.6819e-02, -5.6408e-02],\n                        [ 3.8645e-03,  4.2511e-02,  3.5238e-02],\n                        [ 8.8113e-02,  4.7184e-02, -2.3744e-02]],\n              \n                       [[-1.7529e-01, -1.2029e-02, -8.4835e-03],\n                        [-1.1002e-02,  1.0244e-01, -7.0212e-02],\n                        [-1.4540e-01, -2.0693e-01, -5.4344e-02]],\n              \n                       ...,\n              \n                       [[-1.0604e-01, -2.7860e-02, -1.2849e-01],\n                        [-5.1143e-02, -9.7617e-02, -6.4610e-04],\n                        [ 1.3028e-01,  2.0158e-01,  7.3412e-02]],\n              \n                       [[-2.7905e-02,  6.1720e-02, -1.7812e-01],\n                        [-6.5482e-02, -1.3683e-01, -9.5034e-03],\n                        [-2.0614e-02,  2.2796e-01,  5.7145e-02]],\n              \n                       [[ 3.8017e-02,  1.4266e-01, -4.9352e-02],\n                        [-1.3621e-01, -1.6608e-01, -1.0393e-01],\n                        [ 1.0645e-02,  9.8891e-02, -6.4978e-02]]],\n              \n              \n                      [[[ 2.6170e-02, -7.2580e-02, -7.7578e-02],\n                        [ 7.5079e-02, -6.5754e-03, -3.5813e-02],\n                        [ 1.1184e-02, -1.5603e-03, -1.1977e-01]],\n              \n                       [[ 9.2254e-04, -8.2536e-02, -2.7668e-02],\n                        [ 1.1162e-02,  6.0733e-02,  5.2416e-02],\n                        [ 1.6390e-02,  1.0675e-01,  5.6890e-02]],\n              \n                       [[-9.6441e-02, -1.8627e-01, -2.0884e-01],\n                        [ 2.5237e-02, -1.2389e-01, -1.5779e-01],\n                        [ 1.7628e-01,  1.5215e-02, -2.1954e-01]],\n              \n                       ...,\n              \n                       [[-3.2150e-02, -1.1502e-01, -1.3641e-01],\n                        [-6.9110e-02, -1.1061e-01, -1.4679e-01],\n                        [-6.6869e-02, -5.8758e-02, -6.5511e-02]],\n              \n                       [[-1.5214e-02, -1.0811e-01, -2.0467e-01],\n                        [-8.9036e-02, -8.7747e-02, -1.4613e-01],\n                        [-2.7041e-01, -1.8866e-01, -1.8857e-01]],\n              \n                       [[-1.6267e-01, -1.0286e-01,  9.1324e-02],\n                        [-3.6009e-01, -2.0878e-01,  1.8942e-02],\n                        [-2.3024e-01, -3.3132e-01, -3.6844e-02]]]])),\n             ('layer2.1.bn1.weight',\n              tensor([0.4356, 0.5756, 0.7505, 0.5556, 0.5874, 0.5211, 0.7126, 0.7033, 0.6665,\n                      0.5966, 0.5353, 0.5137, 0.4815, 0.5494, 0.5717, 0.7091, 0.6041, 0.5167,\n                      0.4166, 0.5100, 0.4532, 0.5089, 0.5142, 0.5645, 0.8289, 1.0617, 0.4137,\n                      0.6207, 0.7423, 0.5574, 0.5528, 0.5630, 0.4423, 0.5526, 0.7304, 0.8474,\n                      1.0810, 0.9543, 0.4738, 0.7264, 1.0551, 0.5753, 0.4816, 0.9185, 0.5856,\n                      1.0675, 0.5608, 1.2251, 0.6278, 0.5448, 0.4816, 1.1887, 0.3940, 0.5017,\n                      1.0063, 0.3121, 0.5049, 0.5874, 0.3269, 0.8712, 0.4708, 0.4795, 0.5910,\n                      0.5999, 0.5509, 1.0026, 0.4510, 0.4766, 0.7578, 0.7986, 0.5725, 0.3909,\n                      1.1046, 0.7149, 0.7298, 0.6268, 0.3907, 0.8249, 0.5118, 1.1274, 0.9583,\n                      0.7703, 0.4532, 0.4966, 0.7508, 0.5052, 0.9163, 0.5700, 0.3007, 0.9951,\n                      0.4176, 0.9864, 0.6715, 0.8978, 0.4856, 1.1442, 0.5408, 0.5928, 0.4899,\n                      0.5795, 0.5014, 0.5904, 0.6671, 0.6916, 0.8166, 0.4647, 0.8701, 0.6134,\n                      0.6235, 0.5505, 0.5398, 0.7222, 0.8543, 0.4983, 0.4603, 0.5243, 0.6645,\n                      0.4640, 0.9467, 1.0323, 0.6225, 0.6174, 0.5871, 0.6142, 0.4577, 0.5126,\n                      1.1263, 0.3671])),\n             ('layer2.1.bn1.bias',\n              tensor([-0.3208, -1.0936, -0.2658, -0.3771, -0.3500, -0.3456, -0.3076, -0.2372,\n                      -0.2690, -0.0779, -0.3206, -0.4324, -0.4042, -0.3013, -0.4151, -0.0125,\n                      -0.4395, -0.4833, -0.7671, -0.4073, -0.4144, -0.3500, -0.5866, -0.5609,\n                      -0.5866,  0.1858, -0.3339, -0.8448, -0.1532, -0.5693, -0.6048, -0.3139,\n                      -0.3707, -0.3363, -0.1819, -0.4624, -0.4027, -0.1545, -0.3221, -0.2892,\n                      -0.4221, -0.3426, -0.3263, -0.3976, -0.2760, -0.4975, -0.4397, -0.4281,\n                      -0.4775, -0.8100, -0.7551, -0.1611, -0.3306, -0.3099, -0.4266, -0.3439,\n                      -0.3774, -0.3317, -0.4247, -0.3416, -0.2812, -0.3438, -0.3988, -0.6705,\n                      -0.2539, -0.3986, -0.4044, -0.3923, -0.1646, -0.4458, -0.2924, -0.4443,\n                      -0.4214, -0.4408, -0.7768, -0.4034, -0.3280,  0.1345, -0.3768, -0.3804,\n                      -0.2707, -0.2827, -0.3512, -0.3410, -0.2031, -0.4638, -0.4297, -0.3800,\n                      -0.3252, -0.3739, -0.4251, -0.3567, -0.5449, -0.2019, -0.4413, -0.4142,\n                      -0.8236, -0.4248, -0.3963, -0.3047, -0.2410, -0.4625, -0.4523, -0.4530,\n                      -0.1970, -0.5291, -0.3191, -0.2288, -0.2217, -0.9179, -0.5598, -0.4929,\n                      -0.2667, -0.7343, -0.3325, -0.2370, -0.4588, -0.3389, -0.3472, -0.2807,\n                      -0.3707, -0.2753, -0.3016, -0.3685, -0.4254, -0.2884, -0.3108, -0.4838])),\n             ('layer2.1.bn1.running_mean',\n              tensor([ -2.9914,   5.3729,  -6.6058,  -7.5374,  -5.7578, -16.9152,  -7.0496,\n                      -11.4428, -10.4798, -12.5508, -17.1054, -12.9622,  -6.2252, -13.3874,\n                        3.2875, -11.3983,   9.3357,  -3.9378,   9.0035, -11.6709,  -9.7575,\n                      -11.2287,   7.4381,  -6.7324,  -3.4599,  -1.3677, -16.0966,   6.9675,\n                       -4.4756,   7.3564,   0.1732,   3.4160, -13.6393,  -9.9802,  -8.4695,\n                       -9.7289,  -4.5834,  -4.7855, -18.4498,  -9.5403,  -4.8336, -10.2722,\n                      -13.5009,  -2.9896,  -6.4254,  -3.8180,  -7.6466,  -2.9324,  -0.0379,\n                        6.9348,   8.7782,  -4.9289,  -4.4766, -21.4587,  -2.8826,  -3.3510,\n                      -15.6848, -11.7615, -15.3389,  -8.4710,  -7.4211,  -0.1338,  -0.4698,\n                        2.7968, -19.1107,  -2.6618,  12.4181,  -0.5895,  -8.9214,   0.1909,\n                       -1.6791,  -2.0515,  -3.1951,   5.2639,  -0.1193, -12.4147,  -1.4307,\n                       -7.2267, -16.7902,  -3.1046,   7.2441,  -7.5529,  -1.5602, -18.3273,\n                       -1.0047,   7.8184,  -4.2843, -10.6311,  -9.4324,  -3.7237, -11.9006,\n                       -3.0763,  -0.8937,  -3.2523, -12.9935,  -5.0277,   7.9081,  -8.0249,\n                      -12.6994, -12.9502, -13.5463,  -4.9925,   0.6948,   0.6843,   2.0493,\n                       -1.8491,  -9.7782, -21.9901, -22.6004,   7.9404,   3.8367,  -1.4392,\n                       -2.6530,   8.4717,   7.0174,  -8.7730,  -2.7807, -13.9131,  -2.9489,\n                       -2.8165,  -6.6884, -14.3631, -17.4145, -10.8456,   2.7374, -12.5456,\n                       -2.0380,   1.6780])),\n             ('layer2.1.bn1.running_var',\n              tensor([187.4923,  44.1066,  47.0195, 166.0863,  81.8629, 144.9843,  77.0317,\n                       61.7646,  97.8380,  91.1052, 175.0229, 114.3929, 117.7481, 164.2838,\n                      155.8471,  68.8138,  96.1879,  73.7954,  50.2926, 148.0430, 137.9835,\n                      208.8202,  92.3815,  70.0871,  44.2039,  37.8201, 184.8106,  61.3119,\n                       56.7242,  63.5696,  58.4789, 145.3827, 133.5018,  76.3722,  81.3037,\n                       65.7816,  45.3335,  43.1884, 293.7784,  59.4241,  61.2569, 215.8085,\n                      221.8314,  55.4479, 159.2079,  49.0686, 131.5776,  44.7065,  49.8837,\n                       64.8757,  60.5100,  53.6958, 188.9828, 417.8442,  68.5185, 176.6050,\n                      265.1370, 169.2311, 168.0508,  49.5502, 103.7380, 151.6292, 103.9017,\n                       49.0189, 148.2655,  35.2972, 120.9762, 135.8076,  55.0914,  43.7488,\n                      147.0845,  65.3014,  67.0709,  92.7103,  45.6702,  66.9743, 136.3248,\n                       37.9550, 132.5719,  63.2586,  39.5429,  56.6656, 124.6722, 239.3772,\n                       64.1997,  85.6163,  65.8065,  87.1908,  78.0509,  34.2527,  59.7685,\n                       60.0301,  44.5836,  45.4388, 168.3071,  65.2178,  69.0676,  67.9502,\n                      293.7263, 124.7222,  91.9348,  68.8894,  65.0726,  51.0472,  46.9632,\n                       40.8907,  62.0024, 176.8055, 223.9406,  55.6153,  86.8886,  46.8400,\n                       62.7876,  67.0682, 190.5755, 127.5010,  58.5601, 312.7769,  62.3703,\n                       34.1510,  87.8710, 177.7585,  58.8068,  50.1411,  87.8356, 178.3601,\n                       58.9371,  73.3815])),\n             ('layer2.1.bn1.num_batches_tracked', tensor(14770)),\n             ('layer2.1.conv2.weight',\n              tensor([[[[-0.0552, -0.1326, -0.1188],\n                        [-0.0486, -0.1657, -0.1817],\n                        [ 0.0227, -0.1140, -0.1416]],\n              \n                       [[ 0.0281, -0.0281,  0.0050],\n                        [-0.0094, -0.0486, -0.0344],\n                        [-0.0271, -0.0233, -0.0305]],\n              \n                       [[ 0.1040,  0.1210,  0.0722],\n                        [ 0.1055,  0.0279,  0.1175],\n                        [ 0.1730,  0.1576,  0.0470]],\n              \n                       ...,\n              \n                       [[-0.0582, -0.1466,  0.0634],\n                        [-0.0117, -0.1156,  0.1081],\n                        [-0.0403, -0.0780,  0.0569]],\n              \n                       [[ 0.0014,  0.0476, -0.0929],\n                        [ 0.1100,  0.0428, -0.1219],\n                        [ 0.0086,  0.0170,  0.0181]],\n              \n                       [[-0.1210, -0.1276, -0.1507],\n                        [-0.1368, -0.0726, -0.0657],\n                        [-0.1086, -0.0122, -0.0638]]],\n              \n              \n                      [[[-0.0659, -0.1396, -0.1186],\n                        [-0.1102, -0.1572, -0.1422],\n                        [-0.0600, -0.0969, -0.0772]],\n              \n                       [[ 0.1421,  0.1024,  0.1567],\n                        [ 0.1499,  0.1317,  0.1826],\n                        [ 0.1690,  0.2009,  0.2011]],\n              \n                       [[ 0.0404,  0.2369,  0.2625],\n                        [-0.0511,  0.1429,  0.0156],\n                        [-0.0100,  0.0240,  0.1229]],\n              \n                       ...,\n              \n                       [[-0.0178, -0.0742, -0.0756],\n                        [-0.0393, -0.1630, -0.0961],\n                        [-0.0380, -0.0940, -0.0656]],\n              \n                       [[ 0.1194,  0.1740,  0.1989],\n                        [ 0.1863,  0.1625,  0.2413],\n                        [ 0.1684,  0.1183,  0.1183]],\n              \n                       [[ 0.0519,  0.0035,  0.0169],\n                        [ 0.1050, -0.0361, -0.1236],\n                        [ 0.0905, -0.0534, -0.0826]]],\n              \n              \n                      [[[-0.1066, -0.0771, -0.0558],\n                        [-0.1800, -0.1837, -0.0812],\n                        [-0.1327, -0.1686, -0.1213]],\n              \n                       [[-0.1138, -0.1558, -0.1496],\n                        [-0.1439, -0.1783, -0.1721],\n                        [-0.1548, -0.1750, -0.2020]],\n              \n                       [[-0.0413,  0.0672,  0.0648],\n                        [-0.1985, -0.0149,  0.0209],\n                        [-0.2060, -0.2093,  0.1430]],\n              \n                       ...,\n              \n                       [[-0.0936, -0.0312, -0.1668],\n                        [-0.0591, -0.0868, -0.0782],\n                        [-0.1604, -0.0211, -0.0871]],\n              \n                       [[-0.0562, -0.0829,  0.1641],\n                        [-0.1232, -0.0748, -0.0106],\n                        [-0.0736, -0.0136, -0.1268]],\n              \n                       [[-0.1686, -0.0579, -0.0689],\n                        [-0.0130, -0.1873, -0.1322],\n                        [ 0.0848, -0.1212, -0.0054]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0491, -0.1295, -0.1577],\n                        [-0.1030, -0.1989, -0.1465],\n                        [-0.0167, -0.0809,  0.0100]],\n              \n                       [[-0.2207, -0.1912, -0.1764],\n                        [-0.1857, -0.1996, -0.1436],\n                        [-0.1730, -0.1619, -0.1074]],\n              \n                       [[ 0.1364,  0.1112,  0.0471],\n                        [ 0.1197,  0.0833, -0.0294],\n                        [ 0.1619,  0.0741, -0.0748]],\n              \n                       ...,\n              \n                       [[-0.0461, -0.0285,  0.0069],\n                        [ 0.0272,  0.0253, -0.1515],\n                        [ 0.0577,  0.0050, -0.0153]],\n              \n                       [[ 0.0347, -0.0226,  0.0765],\n                        [ 0.0556,  0.1349,  0.1025],\n                        [-0.0016, -0.0721, -0.0293]],\n              \n                       [[-0.1551, -0.0646, -0.1112],\n                        [-0.1459, -0.1046, -0.1086],\n                        [-0.2680, -0.0210,  0.0147]]],\n              \n              \n                      [[[-0.0290, -0.2208, -0.1972],\n                        [-0.1657, -0.2986, -0.2667],\n                        [-0.0842, -0.2241, -0.2103]],\n              \n                       [[-0.0471, -0.0338, -0.0106],\n                        [-0.0745, -0.0095,  0.0078],\n                        [ 0.0015,  0.0393,  0.0932]],\n              \n                       [[ 0.1350, -0.0034, -0.1354],\n                        [ 0.3116,  0.1102, -0.1061],\n                        [ 0.3173,  0.0708, -0.0988]],\n              \n                       ...,\n              \n                       [[ 0.0551,  0.1954, -0.0204],\n                        [ 0.1261,  0.1587, -0.1777],\n                        [ 0.1538,  0.0969, -0.0996]],\n              \n                       [[ 0.0057,  0.1099,  0.0726],\n                        [ 0.1141,  0.0901,  0.0685],\n                        [-0.1578, -0.1788,  0.0406]],\n              \n                       [[-0.1768, -0.2028, -0.2518],\n                        [-0.2602, -0.2498, -0.2598],\n                        [-0.1808, -0.1869, -0.1683]]],\n              \n              \n                      [[[-0.1114, -0.1770, -0.2615],\n                        [-0.1251, -0.1926, -0.1828],\n                        [-0.1089, -0.2326, -0.1695]],\n              \n                       [[-0.0305, -0.0595, -0.0749],\n                        [-0.0202, -0.0339, -0.0697],\n                        [ 0.0450, -0.0213, -0.0547]],\n              \n                       [[-0.0777,  0.0777,  0.0900],\n                        [-0.0008,  0.0896,  0.0898],\n                        [ 0.1368,  0.0852,  0.2173]],\n              \n                       ...,\n              \n                       [[ 0.1865,  0.0991,  0.1006],\n                        [ 0.2892,  0.1402,  0.1514],\n                        [ 0.2771,  0.1292,  0.1647]],\n              \n                       [[ 0.0401,  0.0124, -0.2062],\n                        [-0.0892, -0.2205, -0.2236],\n                        [-0.2420, -0.1395,  0.0529]],\n              \n                       [[ 0.0236, -0.1176, -0.1286],\n                        [ 0.0366, -0.0069, -0.0212],\n                        [ 0.1204,  0.0193, -0.0322]]]])),\n             ('layer2.1.bn2.weight',\n              tensor([0.7586, 0.4767, 0.7533, 0.8638, 0.9320, 0.4318, 0.8025, 0.7066, 0.6021,\n                      0.3353, 0.6976, 0.7904, 0.5828, 0.5197, 0.7474, 0.6190, 0.5637, 0.6336,\n                      0.6225, 0.7315, 0.4117, 0.5382, 0.7399, 0.6754, 0.7072, 0.7617, 0.7246,\n                      0.5364, 0.7279, 0.5410, 0.6281, 0.8496, 0.6862, 0.5739, 0.6928, 0.3981,\n                      0.7371, 0.6829, 0.4922, 0.5038, 0.4684, 0.7822, 0.7028, 1.1200, 0.7915,\n                      0.7209, 0.7388, 0.7760, 0.6243, 0.2054, 0.9323, 0.5426, 0.7078, 0.4604,\n                      0.6082, 0.7681, 0.9024, 0.5436, 0.8447, 0.8538, 0.7146, 0.6687, 0.4600,\n                      0.5795, 0.8597, 0.6661, 0.7353, 0.7760, 0.6453, 0.5912, 0.5563, 0.7161,\n                      0.5222, 0.4128, 0.4970, 0.4709, 0.7300, 0.6673, 0.4673, 0.6886, 0.8585,\n                      0.6546, 0.5578, 0.8018, 0.4422, 0.5930, 0.9003, 0.6749, 0.7041, 0.5263,\n                      0.6215, 0.7235, 0.6999, 0.4642, 0.8237, 0.4708, 0.6645, 0.9164, 0.7427,\n                      0.7114, 0.6791, 0.7862, 0.7353, 0.4521, 0.5689, 0.3985, 0.7498, 0.6827,\n                      0.4210, 0.5898, 0.4655, 0.5740, 0.4633, 0.7300, 0.4009, 0.3798, 0.5365,\n                      0.5854, 0.6713, 0.9064, 0.4728, 0.7260, 0.6775, 0.5922, 0.7951, 0.5600,\n                      0.7067, 0.7294])),\n             ('layer2.1.bn2.bias',\n              tensor([-0.0433, -0.3191, -0.1435, -0.1163, -0.0252, -0.2258,  0.1135,  0.0323,\n                      -0.2597, -0.1237, -0.0015,  0.0318, -0.0322, -0.0154, -0.1760, -0.4023,\n                      -0.0656,  0.1719, -0.1577,  0.1230, -0.0481,  0.0427, -0.3025,  0.1522,\n                       0.0980,  0.0386,  0.1562,  0.1198, -0.1523, -0.0183,  0.0111,  0.0320,\n                       0.1274, -0.3366, -0.0971, -0.2622,  0.0316,  0.0126, -0.1806,  0.1396,\n                      -0.3731,  0.0630, -0.1112,  0.1979, -0.0697, -0.1235,  0.0045, -0.1598,\n                      -0.0512, -0.2252,  0.0869, -0.1168, -0.0890,  0.0600, -0.0114, -0.0940,\n                      -0.0441,  0.0426, -0.0611,  0.1194,  0.3384, -0.4177, -0.2181, -0.1040,\n                      -0.2463,  0.1304,  0.1471, -0.0742,  0.0385,  0.1719, -0.2485, -0.1964,\n                      -0.0146, -0.1920, -0.0874, -0.2586, -0.1009,  0.3307, -0.0056, -0.2087,\n                      -0.1070,  0.1940,  0.0297,  0.0695, -0.2606, -0.3888, -0.1124,  0.0755,\n                       0.0725,  0.0968, -0.1451,  0.0591, -0.0646, -0.0689, -0.0123, -0.2975,\n                       0.0993, -0.4270,  0.1499, -0.0498,  0.1280,  0.0879,  0.0536, -0.2294,\n                      -0.0096, -0.1546,  0.0904,  0.0349, -0.1528, -0.2085,  0.0587, -0.2581,\n                      -0.1718,  0.1518, -0.3097, -0.2111, -0.0217, -0.1344, -0.0671, -0.1001,\n                      -0.1114,  0.0568,  0.0217, -0.1464, -0.1422,  0.0353,  0.1020, -0.2430])),\n             ('layer2.1.bn2.running_mean',\n              tensor([-1.6624e+00,  3.5998e+00,  7.3550e-01, -3.7406e+00, -4.7826e-01,\n                       2.3208e+00, -2.4162e+00, -2.5200e+00,  3.7687e-03,  2.7348e+00,\n                      -2.7486e+00, -3.2830e+00, -2.0198e+00,  1.4902e+00,  3.9107e-01,\n                       9.9806e-01,  2.7448e+00, -2.1054e+00,  6.8070e-01, -1.7833e+00,\n                       4.0599e+00, -2.3764e+00,  1.7198e+00, -2.3076e+00, -6.8378e-01,\n                      -2.6260e+00, -3.0705e+00, -4.8149e+00, -1.1684e+00, -4.0539e+00,\n                      -1.2265e+00, -2.8356e+00, -2.9899e+00,  2.6660e+00, -1.3433e+00,\n                       2.5864e+00, -2.8111e+00, -4.8168e+00,  9.5934e-01, -3.4366e+00,\n                       1.0326e+00, -3.2742e+00, -6.1248e-01, -4.1560e+00, -2.5461e+00,\n                       1.1510e-01, -1.4168e+00, -1.0485e+00, -2.7257e+00,  3.3153e-01,\n                      -2.9656e+00,  1.8900e+00, -1.0697e+00, -3.6305e+00, -2.9537e+00,\n                      -2.9288e+00,  1.9356e+00, -4.1912e+00, -1.7210e+00, -2.5694e+00,\n                      -3.3218e+00,  9.0464e-01, -8.0472e-01,  6.0838e-01,  5.2665e-01,\n                      -3.2080e+00, -2.6699e+00, -3.1584e+00, -3.2025e+00, -3.5133e+00,\n                       1.6827e+00, -3.5227e+00,  2.8538e+00,  2.1386e+00,  2.1009e+00,\n                       5.0843e+00, -2.3707e+00, -2.9119e+00, -8.0075e-01, -1.1547e+00,\n                      -2.4209e+00, -3.7459e+00, -4.0258e+00, -3.0305e+00,  1.3017e+00,\n                       2.3976e+00, -2.8597e+00, -4.3497e+00, -3.1578e+00, -4.2294e+00,\n                       2.3963e-01, -1.8992e+00, -2.8183e+00,  1.6122e+00, -2.8699e+00,\n                       1.3935e+00, -3.7099e+00, -1.0468e+00, -2.2452e+00, -5.6597e-01,\n                      -2.9221e+00, -1.8159e+00, -2.6699e+00,  1.3291e+00, -1.6799e+00,\n                       3.8011e+00, -4.3114e+00, -3.7383e+00,  7.6891e-01, -1.8369e+00,\n                      -3.3344e+00,  4.9725e-01,  2.0404e+00, -4.2490e+00,  9.0059e-01,\n                      -4.7857e-02, -3.4583e+00,  2.1497e+00, -3.6176e+00, -2.1512e+00,\n                      -1.7295e+00, -2.6414e+00, -4.7849e+00,  3.1932e-01, -2.2942e+00,\n                      -4.2175e+00, -2.6955e+00, -5.6036e-01])),\n             ('layer2.1.bn2.running_var',\n              tensor([ 55.6316,  11.1235,  45.9619,  10.8224,   7.4370,  18.1218,  24.1684,\n                       35.7316,   9.9934,  41.1662,  80.8597,  16.5943,  97.8401,  28.4167,\n                       10.3845,   6.0246,  44.9958,  46.2160,   8.8632,  85.1578,   8.4439,\n                       96.9223,   8.5367,  41.4254,  40.2144,  35.9221,  75.1623, 112.5336,\n                       10.6912, 100.7587,  13.9405,  59.2917, 100.8086,   7.4178,  32.9399,\n                       31.4975,  53.5862,  72.2030,  29.7016, 163.3720,   8.1017,  55.3283,\n                        7.3288,  11.2161,  87.7732,   8.2870,  44.0427,  73.7823,  65.4461,\n                       15.7189,   7.9143,  43.0173,   8.9720, 123.9696,  93.8435,  16.9782,\n                        7.2156, 125.8533,  21.6296,  64.0767,  85.7345,   5.5284,  22.0313,\n                       31.5312,   7.2965,  47.5088,  38.5117,  13.1549, 172.9533,  79.9798,\n                       66.6725,  85.8944,  37.4634,  56.4343,  60.0023,  13.8363,  45.5918,\n                       78.9016,  21.2536,  10.5953,  41.7110,  82.2148, 188.4568,   9.1097,\n                       24.4402,  33.9116,  40.4274,  65.4259,  27.5838, 146.1155,   8.2523,\n                       59.0545, 122.4041,  56.8355,  67.7836,   8.2755,  95.0565,   8.8389,\n                       39.0218,  37.2255,  47.5586,  56.1477,  52.0816,  14.5118,  72.6473,\n                       16.1293,  66.0114,  92.1299,  66.5150, 112.1300, 220.1171,  25.4779,\n                       59.4169,  82.2730,  11.7281,  21.2563, 132.3648,  32.7768,  19.4573,\n                        7.1949,  21.4716,  12.7654,  39.6464,  12.9573,   8.9472, 126.0912,\n                       42.9023,  30.1468])),\n             ('layer2.1.bn2.num_batches_tracked', tensor(14770)),\n             ('layer3.0.conv1.weight',\n              tensor([[[[ 8.1738e-03,  7.4881e-02,  4.7689e-02],\n                        [ 1.4988e-01,  1.2948e-01,  1.0625e-01],\n                        [ 1.5110e-01,  1.5900e-01,  1.7663e-01]],\n              \n                       [[-4.7533e-02, -4.7580e-02, -9.0165e-03],\n                        [-2.6064e-02,  4.1465e-03, -1.0236e-02],\n                        [-1.0075e-02, -3.7328e-02,  7.2981e-02]],\n              \n                       [[ 1.3200e-01,  9.5192e-02,  1.5394e-01],\n                        [ 1.3471e-01,  8.6250e-02,  3.3452e-02],\n                        [ 2.5429e-01,  9.3311e-02,  2.8199e-01]],\n              \n                       ...,\n              \n                       [[-1.3079e-01, -5.3320e-02, -5.0396e-02],\n                        [-3.7889e-02,  1.8292e-01,  1.9686e-01],\n                        [-1.5943e-02,  2.4361e-01,  7.8162e-02]],\n              \n                       [[ 8.7987e-02,  1.0074e-01, -3.0857e-02],\n                        [ 1.5467e-01,  1.3091e-01,  1.2077e-01],\n                        [-6.8091e-02,  1.3872e-02, -1.1781e-01]],\n              \n                       [[ 6.9361e-02,  1.9425e-01,  1.4426e-01],\n                        [ 1.1213e-01,  2.5917e-01,  1.6139e-01],\n                        [-1.4701e-01,  1.3630e-01,  6.8111e-02]]],\n              \n              \n                      [[[-6.5769e-02,  8.7266e-02, -7.4145e-03],\n                        [ 6.2888e-02,  7.9954e-03,  3.5992e-02],\n                        [ 4.6989e-02,  1.1730e-01,  2.2473e-02]],\n              \n                       [[ 1.5878e-01, -7.6180e-03, -1.4964e-01],\n                        [-7.3997e-02,  6.9409e-03, -6.9883e-02],\n                        [-1.6275e-01, -1.1296e-01, -2.5233e-01]],\n              \n                       [[-2.4961e-02,  5.4539e-02,  1.2622e-01],\n                        [-4.6888e-02,  1.5180e-02,  3.0845e-01],\n                        [-2.3575e-01, -2.0016e-02,  5.2831e-02]],\n              \n                       ...,\n              \n                       [[-3.6088e-02,  3.8770e-02,  7.4572e-02],\n                        [-4.8476e-02,  8.0621e-02,  1.4107e-02],\n                        [ 4.6648e-02,  1.4406e-01,  7.5712e-02]],\n              \n                       [[-7.6157e-02, -3.3630e-02,  1.1029e-01],\n                        [-1.1736e-01,  1.0154e-01, -5.4820e-03],\n                        [ 5.1754e-03, -1.0260e-02,  7.9151e-02]],\n              \n                       [[ 1.1145e-01,  2.5186e-02, -6.9634e-02],\n                        [ 8.2182e-02,  3.8181e-02,  6.2372e-02],\n                        [ 1.7126e-01,  4.9202e-02,  1.6998e-01]]],\n              \n              \n                      [[[-1.6732e-01,  1.0684e-01,  1.3392e-01],\n                        [-1.2912e-01,  5.5619e-02,  1.3397e-01],\n                        [-6.3187e-02,  1.6628e-01,  1.2644e-01]],\n              \n                       [[ 1.7489e-01,  2.2443e-01,  6.8899e-02],\n                        [ 6.9011e-02,  5.4519e-02, -8.9937e-02],\n                        [-5.9985e-02, -2.2676e-02, -4.0257e-02]],\n              \n                       [[ 1.3419e-01, -1.8057e-01,  2.2990e-01],\n                        [-3.0987e-01, -1.5495e-01, -4.1263e-02],\n                        [-2.2231e-01, -2.5279e-01, -1.7261e-01]],\n              \n                       ...,\n              \n                       [[-9.3414e-02,  1.3991e-01,  1.1538e-01],\n                        [ 2.0074e-01,  8.0185e-02,  4.9083e-01],\n                        [ 1.1684e-01,  3.8778e-01,  3.2394e-01]],\n              \n                       [[-2.0734e-02,  9.7231e-02,  1.2184e-01],\n                        [ 1.9185e-01, -5.9938e-02,  9.2514e-02],\n                        [ 4.4164e-02,  3.6163e-02,  4.7280e-02]],\n              \n                       [[ 2.3255e-01,  2.4320e-02,  1.8560e-01],\n                        [ 2.9445e-01, -7.1427e-02,  1.1121e-01],\n                        [ 1.0546e-01,  9.7247e-02,  1.3539e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.2138e-01, -1.4759e-01, -1.5230e-01],\n                        [-1.7880e-01, -1.3925e-01, -9.9617e-02],\n                        [ 1.4134e-01,  7.2623e-02,  1.6825e-01]],\n              \n                       [[ 1.0134e-02,  2.1436e-01,  4.7006e-02],\n                        [-1.4291e-01, -7.5040e-03, -1.4633e-01],\n                        [ 2.0891e-02, -3.7696e-02, -1.9754e-01]],\n              \n                       [[ 2.2685e-01,  7.3727e-02,  9.6871e-02],\n                        [ 6.1597e-02, -8.5141e-02,  1.4998e-01],\n                        [-1.7007e-01, -1.3281e-01, -4.9408e-02]],\n              \n                       ...,\n              \n                       [[-1.4634e-01, -1.0079e-01,  7.6670e-02],\n                        [-1.0563e-01,  1.7565e-02, -1.5336e-01],\n                        [ 1.1820e-01,  5.6015e-02,  8.4180e-02]],\n              \n                       [[-1.6152e-01, -7.6283e-02, -1.6025e-02],\n                        [-2.8926e-02,  1.8111e-02, -3.8017e-02],\n                        [-5.2809e-02,  9.2867e-03, -3.9913e-02]],\n              \n                       [[-6.9280e-02,  1.6732e-02,  2.7091e-01],\n                        [-2.1224e-01, -2.3200e-02,  2.0560e-02],\n                        [-1.9232e-01, -1.2428e-01,  2.4191e-04]]],\n              \n              \n                      [[[ 1.6758e-01,  8.0787e-02,  3.2055e-02],\n                        [ 5.5533e-02, -2.6013e-02, -3.2318e-02],\n                        [ 5.9283e-02,  2.3899e-02,  2.4754e-02]],\n              \n                       [[ 2.1083e-02, -9.5962e-02, -7.2103e-02],\n                        [-2.1320e-02, -1.0576e-01,  6.5877e-02],\n                        [-9.3117e-02, -3.1103e-02,  1.5871e-01]],\n              \n                       [[ 2.2477e-01,  1.6000e-02, -2.9913e-02],\n                        [ 2.0378e-01,  1.6928e-02, -2.4971e-01],\n                        [ 2.6943e-01,  1.8239e-01,  1.6242e-01]],\n              \n                       ...,\n              \n                       [[-3.2820e-02, -1.6244e-02, -1.9235e-02],\n                        [-3.9855e-02,  4.0165e-02,  1.1503e-01],\n                        [-9.0133e-02,  6.7191e-02, -1.6783e-02]],\n              \n                       [[-1.0843e-01,  7.8786e-02,  1.1252e-01],\n                        [ 7.0067e-02,  2.1737e-01,  1.5118e-01],\n                        [ 4.5545e-02, -5.3982e-02, -1.2164e-01]],\n              \n                       [[-3.9328e-02, -1.7452e-01, -1.9392e-01],\n                        [ 8.1549e-02, -2.3823e-01, -8.6558e-02],\n                        [ 6.4797e-02,  1.6698e-03, -1.0183e-01]]],\n              \n              \n                      [[[ 1.0033e-01,  1.6302e-01,  1.4356e-01],\n                        [ 1.8178e-01,  6.6560e-02, -4.9186e-02],\n                        [ 9.3703e-02, -1.3616e-01, -1.7931e-02]],\n              \n                       [[-4.0774e-02, -6.8715e-02, -1.6876e-01],\n                        [ 2.1084e-02,  9.4454e-02, -1.4440e-01],\n                        [-8.5345e-02, -2.7278e-02, -6.8161e-02]],\n              \n                       [[-3.8777e-02,  1.3068e-04,  2.2459e-02],\n                        [-2.2714e-02, -2.0030e-01, -6.8124e-02],\n                        [ 3.7002e-02, -4.3847e-02,  2.2052e-01]],\n              \n                       ...,\n              \n                       [[ 7.8278e-02,  1.2561e-01,  9.5536e-03],\n                        [ 3.6091e-02,  9.8848e-02,  1.2085e-01],\n                        [-7.9747e-02, -1.2379e-01, -1.6664e-01]],\n              \n                       [[-7.8045e-02,  1.0164e-01, -8.6448e-02],\n                        [-3.5174e-02,  4.0172e-02,  3.0932e-01],\n                        [-1.1361e-01,  3.2649e-02,  8.0714e-02]],\n              \n                       [[-1.9571e-01,  9.4192e-02, -3.4215e-01],\n                        [-9.7128e-02,  3.8245e-02,  1.7164e-01],\n                        [-8.1533e-02,  1.1810e-01,  7.0117e-02]]]])),\n             ('layer3.0.bn1.weight',\n              tensor([0.6378, 0.7781, 0.7039, 0.7459, 0.6477, 1.1584, 0.6934, 0.6711, 0.9222,\n                      0.7141, 1.0242, 0.9098, 0.5375, 0.7394, 0.4423, 0.6776, 0.5019, 0.6955,\n                      0.5877, 0.6263, 0.5862, 0.8127, 1.1151, 0.6463, 0.5494, 0.8327, 0.9572,\n                      0.7358, 0.8427, 0.7215, 1.0087, 0.6054, 1.1867, 0.6983, 0.8158, 0.6843,\n                      1.0763, 1.1891, 0.6294, 0.7596, 0.8136, 0.5432, 0.7049, 0.6472, 0.4526,\n                      0.5221, 0.5513, 0.8816, 0.5775, 0.6159, 0.9333, 0.6263, 0.6555, 0.6105,\n                      0.8492, 0.8127, 0.4739, 0.8716, 0.9486, 1.0707, 0.8834, 0.6674, 0.6900,\n                      0.5664, 0.7293, 1.1602, 0.5696, 0.4802, 0.8106, 0.7208, 0.5336, 1.2305,\n                      0.9766, 0.6077, 0.6706, 0.8731, 0.6027, 0.6773, 0.6759, 0.6138, 0.5143,\n                      0.9512, 0.6013, 0.5848, 0.9333, 1.3623, 0.6640, 0.4161, 1.1078, 0.7575,\n                      0.5870, 0.7638, 1.2373, 0.7125, 0.6145, 0.2842, 0.7778, 1.0818, 1.1278,\n                      1.1606, 0.4836, 0.7034, 0.7295, 0.4871, 0.4832, 0.4376, 0.5520, 0.6615,\n                      0.7348, 0.7828, 0.6507, 0.6169, 0.6541, 0.6941, 1.1764, 0.6041, 0.4733,\n                      0.8854, 0.6271, 0.8305, 0.6441, 0.7374, 0.7427, 0.7853, 0.6421, 0.9475,\n                      1.1086, 0.6023, 0.6581, 0.4450, 0.6528, 0.6033, 0.5120, 0.8402, 0.6407,\n                      0.6088, 0.7410, 0.9918, 0.9069, 0.9096, 0.6420, 0.8177, 0.4343, 0.5977,\n                      0.6516, 0.4884, 0.6306, 0.4652, 0.5941, 0.9853, 0.5094, 0.5035, 1.1796,\n                      0.6404, 0.5083, 0.5543, 0.8570, 1.1379, 0.6837, 1.0402, 0.6723, 0.7037,\n                      0.9243, 0.6410, 0.5227, 0.7656, 0.5745, 0.8004, 0.6552, 0.5691, 0.6535,\n                      0.5541, 0.8248, 0.6926, 0.6200, 0.6979, 0.9114, 0.6566, 1.0739, 1.0380,\n                      0.5304, 0.6042, 0.5351, 0.7053, 1.0473, 0.9283, 0.7128, 0.7375, 0.6611,\n                      0.5220, 0.6391, 0.5745, 0.5914, 0.4554, 0.6115, 0.5320, 0.5943, 0.7023,\n                      0.6816, 0.5565, 0.4912, 0.7742, 0.4862, 1.3542, 0.6612, 0.6443, 0.9545,\n                      0.7160, 0.8222, 0.6798, 0.4781, 0.7160, 0.5548, 1.2063, 0.9028, 0.5802,\n                      0.9126, 0.5311, 0.6083, 0.4562, 0.4117, 0.9539, 0.6870, 0.5366, 1.1471,\n                      0.7904, 1.0073, 0.7606, 0.8047, 0.6684, 0.7022, 0.6127, 0.6728, 0.8706,\n                      0.7014, 0.6702, 0.6364, 0.8549, 0.8185, 0.8976, 0.6722, 0.5829, 0.7668,\n                      0.9144, 0.4160, 0.5695, 0.8750, 0.4820, 1.2988, 0.6330, 0.8075, 0.5679,\n                      0.6410, 0.8807, 0.4412, 0.7812])),\n             ('layer3.0.bn1.bias',\n              tensor([-0.4310, -0.4292, -0.3757, -0.2408, -0.4055, -0.3533, -0.4467, -0.4778,\n                      -0.5254, -0.3071, -0.3711, -0.5043, -0.5814, -0.5175, -0.4406, -0.3454,\n                      -0.4834, -0.4200, -0.3654, -0.2781, -0.2766, -0.4943, -0.5037, -0.3006,\n                      -0.3435, -0.4613, -0.2618, -0.5420, -0.4765, -0.2410, -0.4741, -0.3607,\n                      -0.2107, -0.1793, -0.1987, -0.4567, -0.4324, -0.3889, -0.4364, -0.4062,\n                      -0.5143, -0.4662, -0.1483, -0.3507, -0.3139, -0.3324, -0.5042, -0.3678,\n                      -0.4090, -0.3953, -0.3562, -0.4093, -0.3347, -0.5342, -0.4475, -0.3399,\n                      -0.4107, -0.4366, -0.2828, -0.3153, -0.4009, -0.2905, -0.3357, -0.3381,\n                      -0.2661, -0.1868, -0.2515, -0.5151, -0.4142, -0.5074, -0.3692, -0.2683,\n                      -0.4313, -0.2815, -0.4238, -0.3828, -0.2743, -0.5533, -0.1233, -0.4116,\n                      -0.4111, -0.2546, -0.5592, -0.3360, -0.3845, -0.3613, -0.4174, -0.1833,\n                      -0.4891, -0.4817, -0.3511, -0.3543, -0.3411, -0.1810, -0.3569, -0.2169,\n                      -0.4288, -0.2483, -0.2090, -0.3161, -0.3603, -0.2815, -0.2463, -0.4628,\n                      -0.5040, -0.2155, -0.0771, -0.2887, -0.3790, -0.5103, -0.3751, -0.3910,\n                      -0.3650, -0.5999, -0.1349, -0.3960, -0.3585, -0.2148, -0.3919, -0.3926,\n                      -0.3503, -0.4353, -0.4750, -0.3295, -0.4338, -0.3920, -0.4213, -0.4530,\n                      -0.4165, -0.4218, -0.4024, -0.4545, -0.3627, -0.1340, -0.3742, -0.4552,\n                      -0.1671, -0.3136, -0.3093, -0.3887, -0.3607, -0.3637, -0.3897, -0.5272,\n                      -0.3537, -0.3635, -0.3719, -0.3777, -0.2954, -0.1855, -0.3671, -0.4548,\n                      -0.1630, -0.3089, -0.3961, -0.5333, -0.4925, -0.3197, -0.2454, -0.3085,\n                      -0.3789, -0.6122, -0.1261, -0.3274, -0.4836, -0.2946, -0.3319, -0.1163,\n                      -0.3462, -0.3471, -0.2718, -0.3742, -0.2053, -0.3135, -0.3528, -0.3316,\n                      -0.3047, -0.2976, -0.2720, -0.3251, -0.3567, -0.1705, -0.4056, -0.4466,\n                      -0.2455, -0.3360, -0.2879, -0.3670, -0.4585, -0.5361, -0.3161, -0.2473,\n                      -0.4248, -0.3728, -0.3928, -0.1559, -0.6863, -0.5738, -0.2390, -0.4882,\n                      -0.1112, -0.1555, -0.5181, -0.3424, -0.3197, -0.3231, -0.2612, -0.4670,\n                      -0.4179, -0.8488, -0.3444, -0.3509, -0.2664, -0.3764, -0.3005, -0.1705,\n                      -0.3188, -0.3608, -0.5607, -0.4016, -0.2135, -0.2278, -0.4426, -0.3942,\n                      -0.1042, -0.3605, -0.3798, -0.3095, -0.3732, -0.2582, -0.4772, -0.4073,\n                      -0.1344, -0.5410, -0.3779, -0.3766, -0.3784, -0.4948, -0.3579, -0.4640,\n                      -0.4420, -0.4022, -0.4619, -0.3709, -0.4026, -0.1139, -0.2038, -0.3394,\n                      -0.3142, -0.4983, -0.4309, -0.3847, -0.3916, -0.4159, -0.4487, -0.3137])),\n             ('layer3.0.bn1.running_mean',\n              tensor([ 1.2839e+00,  8.9185e-01, -1.2073e+01, -1.4475e+01, -5.2532e+00,\n                      -2.9621e+00, -6.6999e+00,  4.6639e+00, -7.0041e+00, -1.9942e+01,\n                      -9.5403e+00, -1.1886e+01,  4.3570e+00, -2.9135e+00,  9.5713e-01,\n                      -1.8463e+01,  7.5520e+00, -3.1711e-01, -1.4433e+01, -2.3498e+01,\n                      -1.5902e+01,  3.9851e+00, -6.1451e+00, -2.3525e+01, -1.9466e+00,\n                       3.5529e+00, -6.9217e+00,  5.1779e+00, -3.4033e+00, -1.6838e+01,\n                      -6.0762e-01, -1.5460e+01, -2.8387e+00, -1.3420e+01, -1.2046e+01,\n                      -1.2705e+01, -9.2241e+00,  2.1710e+00, -3.1724e+00,  4.5444e-01,\n                      -3.1164e+00, -7.4188e+00, -8.0994e+00, -1.1780e+01, -3.5665e+00,\n                      -7.3307e+00,  2.4226e-01,  1.9733e+00, -1.0157e+01, -1.5234e+01,\n                      -4.8155e+00, -9.0185e+00, -1.0035e+01, -5.7559e+00, -5.4582e+00,\n                      -1.8663e-03, -1.0501e+00,  2.4222e+00, -1.7209e-01, -1.2468e+00,\n                      -9.5781e+00, -2.0329e+01, -1.3045e+00, -2.8022e+00, -1.1403e+01,\n                      -2.4913e+00, -1.8948e+01, -6.8543e+00, -1.3075e+01, -5.7898e+00,\n                       6.0743e+00, -3.2113e+00, -7.8868e+00, -1.6465e+01, -1.0830e+01,\n                      -3.9740e+00, -1.9477e+01, -1.0610e+00, -1.7064e+01, -1.4531e+01,\n                       3.4992e+00, -5.9962e+00,  1.5125e+00, -1.4465e+01, -2.7148e+00,\n                      -4.4111e+00, -4.1646e+00, -1.8677e+01, -6.1986e+00, -7.4673e+00,\n                       1.5960e+00, -1.2091e+01, -6.2026e+00, -1.3770e+01, -1.3789e+01,\n                      -9.1771e-01, -2.7487e+00, -3.3776e+00, -4.2278e+00, -1.4701e+00,\n                      -8.2524e-01, -1.9285e+01, -1.4729e+01, -7.8379e+00,  6.2172e+00,\n                      -1.5890e+01, -9.5287e+00, -2.0844e+01, -1.3129e+01,  4.2829e+00,\n                      -1.4372e+01, -2.4436e+01, -1.5111e+01,  1.7132e-01, -1.5792e+00,\n                      -2.3269e+01, -1.7547e+01, -1.1632e+01, -1.8368e+01, -1.0401e+01,\n                      -9.5308e+00, -7.8333e+00, -4.1268e+00,  3.4021e+00, -3.5546e+00,\n                      -4.9633e-01, -6.8476e+00,  4.1311e+00,  3.9512e+00,  1.8108e+00,\n                      -1.2907e+01, -6.2888e-01,  8.6513e-01, -9.4917e+00, -6.1824e+00,\n                      -8.7623e+00, -7.5169e+00, -6.1608e-01, -2.9917e+00, -8.5068e+00,\n                      -4.8846e+00, -3.9740e+00,  7.9496e+00,  3.3313e+00, -1.5573e+01,\n                      -3.1532e+00, -1.8467e+01, -1.7374e+01, -1.6429e+01, -9.7934e+00,\n                       2.6582e+00, -2.4596e-01, -9.0926e-01, -1.2461e+01, -1.3595e+01,\n                       6.1019e-01, -8.0547e+00, -5.7024e+00, -1.7895e+01, -3.7056e+00,\n                      -1.5997e+01,  3.2213e-01, -5.2331e+00, -8.2199e+00,  5.0431e+00,\n                      -1.0461e+01, -2.9100e+00, -8.1618e+00,  5.5293e+00, -1.2101e+01,\n                      -1.7914e+01, -9.0164e+00, -7.6529e+00, -1.4485e+01, -2.1354e+01,\n                      -7.1479e-01, -5.2891e+00, -1.9757e+01, -6.5409e+00,  6.2044e-01,\n                      -2.6745e+00, -5.8540e+00, -1.4406e+00, -9.2520e+00, -1.2951e+00,\n                      -2.2200e+00, -2.9667e+00, -3.4373e+00, -3.6998e+00,  1.7599e-01,\n                      -2.1174e+01, -1.6455e+01, -9.2237e+00, -6.2965e-02, -5.8544e+00,\n                      -2.3277e+01,  4.4711e+00, -8.6603e+00, -1.7151e+01, -3.1412e+00,\n                      -1.7776e+01, -1.3730e+01,  3.9918e+00, -4.5941e+00, -1.3424e+01,\n                      -9.3894e+00, -4.2700e+00, -3.6034e+00,  9.6518e-01,  1.4900e+01,\n                       6.9763e+00, -1.3253e+01, -1.3238e+01, -3.5861e+00, -1.0664e+01,\n                      -2.0831e+01, -5.9978e+00,  2.4725e+00,  4.8214e+00,  8.8301e+00,\n                      -1.8248e+01, -6.6506e+00, -7.3913e+00, -2.0594e+00, -2.5111e+00,\n                       4.6962e+00, -6.2469e+00, -1.0344e+01,  2.1904e+00, -2.1597e+01,\n                       1.4294e+00, -1.3997e+01, -1.3339e+01, -5.7735e+00, -1.1303e+00,\n                      -5.9318e+00, -1.3878e+00, -1.0967e+01, -1.4008e+01, -3.0335e+00,\n                       5.7803e+00, -1.1693e+01, -1.3725e+00, -7.8739e+00,  3.5857e+00,\n                      -1.9740e+01, -9.2474e+00,  2.2133e-01, -2.2387e+00, -1.0592e+01,\n                      -4.0906e+00, -1.5012e+01,  9.2338e+00, -1.8624e+00,  6.8409e+00,\n                      -8.3413e+00])),\n             ('layer3.0.bn1.running_var',\n              tensor([ 81.6014,  70.5841,  75.0253, 129.9203,  63.0262,  72.1341,  67.4458,\n                       39.2971,  60.9233,  86.3770,  84.6027,  61.8352,  75.8440,  53.3904,\n                       90.5744,  95.0482,  60.8688,  50.5970, 187.1561, 178.9630, 275.6024,\n                       53.1565,  86.6453, 141.8303,  75.8523,  49.0868,  61.6984,  47.4192,\n                       52.4079, 164.4723,  58.3561, 100.4829,  64.9408,  91.5853,  84.8204,\n                       79.3898,  61.7190,  59.1684,  97.1255,  59.3168,  69.0690, 144.9687,\n                       96.4655, 159.0768, 125.6903, 196.9534,  76.4036,  65.1031, 217.1739,\n                      100.0146,  72.4488,  63.5340, 142.3384, 112.1499,  63.4041,  46.2524,\n                       66.2960,  59.0886,  54.2430,  62.3304,  62.1912, 149.0256, 102.3050,\n                       80.5138,  89.7814,  67.2635, 171.1418,  46.4154,  74.2141,  54.5302,\n                      119.6417,  58.2830,  53.1772, 161.8208,  57.0633,  57.7449, 160.2213,\n                       63.7186, 161.6663, 101.6300,  78.6277,  58.7284,  79.8477, 218.4859,\n                       63.7375,  65.7651,  65.7564, 175.2395,  85.4774,  58.9357,  97.7566,\n                       76.4254,  95.9536, 108.1620, 126.3405, 138.3850,  64.9346,  72.6442,\n                       54.4324,  55.3477,  80.4810, 139.8722, 185.7724, 147.1340,  76.1102,\n                      134.1411, 130.4865, 101.7534,  65.6141,  59.4602, 155.9810, 371.9248,\n                       80.9355,  61.4046,  47.5647, 180.6447, 253.7202,  70.3966, 132.5861,\n                       49.9793, 122.1777, 111.9612,  50.0878,  62.1856,  81.0361,  65.4432,\n                       66.4547,  45.9027,  82.3396, 132.1529, 103.3950,  53.8671, 124.6583,\n                       60.6233, 116.6186,  77.1569,  68.0329,  68.5349,  56.7849,  87.3188,\n                      154.5958,  76.3745,  78.5349,  52.5176, 128.1083, 106.5923, 224.2180,\n                      179.7560, 111.3425,  74.5154,  80.9136,  62.0455,  64.1578, 155.0788,\n                      221.1229,  83.4330,  67.7156,  58.2974, 148.2114,  85.0268, 106.5580,\n                       71.5144,  87.0229,  62.6243,  67.2320,  72.1234, 101.2007,  80.2116,\n                       85.1728,  87.9710, 144.9162, 118.3269, 107.6725, 148.0800, 152.3080,\n                       82.5872,  80.8215, 130.8120,  62.0709,  70.8030, 114.3065, 100.6261,\n                      165.6789,  59.0859,  46.4845,  54.5786,  78.7786,  78.2238,  68.6728,\n                       65.3488,  87.8800, 134.7726, 124.8892,  99.0027, 136.8211, 180.8557,\n                       63.5733,  81.8138, 174.0091, 100.6636, 176.3534, 130.9315,  92.1265,\n                       64.8380, 164.1332,  99.0029,  83.5720,  76.4805,  54.9575,  60.5111,\n                       99.5364,  90.6149, 121.6773,  63.0286,  77.1218, 159.2509,  69.2303,\n                       66.0957,  66.8611,  86.4192, 204.8656,  60.8716,  96.3946,  73.9347,\n                       63.4738,  62.0630,  45.5814, 109.7402,  75.1032, 200.1954,  72.0212,\n                      116.3489,  96.6357,  60.3099,  70.3161, 135.3094,  89.8590,  72.2814,\n                       68.7525,  52.8142,  70.5259, 108.5154,  89.4019,  48.9733,  61.1727,\n                      112.8959,  73.0281, 120.1583,  81.2637,  96.3199,  55.5137, 169.3639,\n                       79.0384,  53.6083,  65.3241,  62.6224])),\n             ('layer3.0.bn1.num_batches_tracked', tensor(14770)),\n             ('layer3.0.conv2.weight',\n              tensor([[[[-9.5308e-03,  3.5560e-02, -1.2737e-01],\n                        [ 5.8482e-02,  9.4662e-02,  8.7114e-02],\n                        [-8.2109e-02, -4.8046e-03, -2.3842e-02]],\n              \n                       [[-1.1722e-01, -7.9455e-02, -4.0110e-02],\n                        [-1.1995e-01, -9.8558e-02, -2.4163e-02],\n                        [-1.2472e-01,  5.3165e-02,  1.0627e-01]],\n              \n                       [[-9.5587e-02,  1.5387e-01,  1.0231e-01],\n                        [-1.4842e-01,  8.6482e-02,  7.1464e-02],\n                        [ 1.2179e-01,  3.4430e-01,  1.3869e-01]],\n              \n                       ...,\n              \n                       [[-7.1628e-02,  9.4703e-04,  1.1905e-01],\n                        [ 1.3707e-01,  1.7960e-01, -1.2585e-01],\n                        [-3.7628e-02,  3.9579e-03,  4.8828e-02]],\n              \n                       [[ 2.3325e-01,  9.8863e-03, -1.4603e-01],\n                        [ 2.9889e-02, -3.2507e-02, -1.5560e-02],\n                        [-1.0531e-01, -4.5844e-02, -2.3791e-01]],\n              \n                       [[ 4.9626e-02,  8.2789e-02, -5.4705e-02],\n                        [ 1.6250e-02, -1.0113e-01, -8.3363e-02],\n                        [ 1.3537e-02,  5.4175e-02,  1.5393e-01]]],\n              \n              \n                      [[[ 1.2473e-02,  7.4696e-02,  3.7027e-02],\n                        [-4.1633e-02, -2.3453e-01, -1.6443e-01],\n                        [-5.9714e-02, -1.0571e-01,  1.7342e-02]],\n              \n                       [[ 3.8342e-02,  1.7607e-01, -6.0428e-03],\n                        [ 2.4191e-01,  1.2405e-01,  1.6885e-01],\n                        [-3.8870e-02,  1.0635e-02, -6.7080e-02]],\n              \n                       [[ 1.0069e-01,  2.0599e-01, -3.3912e-02],\n                        [ 1.0094e-01, -4.5811e-02, -1.6083e-01],\n                        [-3.6124e-02, -1.3191e-01, -4.5062e-01]],\n              \n                       ...,\n              \n                       [[ 8.1099e-04,  1.2809e-01,  5.0984e-02],\n                        [ 1.5312e-01,  1.6377e-01,  1.1731e-01],\n                        [ 1.5035e-01, -2.8466e-02,  2.0433e-02]],\n              \n                       [[-4.2316e-02,  4.2772e-02, -1.9033e-01],\n                        [ 2.3761e-02, -1.0518e-01, -1.0387e-01],\n                        [ 8.8062e-02,  9.3258e-03,  1.0409e-01]],\n              \n                       [[-1.6280e-01, -2.1234e-01,  6.3403e-02],\n                        [-4.7015e-02, -2.9913e-01,  3.6697e-03],\n                        [ 2.6396e-01, -1.8845e-02, -4.8239e-02]]],\n              \n              \n                      [[[ 2.8920e-01,  1.8718e-01,  2.6416e-01],\n                        [ 2.3886e-02,  2.0323e-01,  2.4531e-01],\n                        [ 7.5244e-02,  4.1016e-02,  2.7627e-01]],\n              \n                       [[-1.4806e-01, -2.6489e-01, -1.5127e-02],\n                        [-1.0331e-01, -1.9468e-01, -1.4998e-02],\n                        [-1.3503e-01, -2.4312e-01,  3.8068e-02]],\n              \n                       [[ 1.1039e-01,  5.2745e-02,  1.1633e-01],\n                        [-7.2188e-02, -3.4737e-02,  3.1917e-02],\n                        [-1.7930e-02, -4.1207e-02, -6.7198e-03]],\n              \n                       ...,\n              \n                       [[ 3.5089e-02,  2.8591e-02,  1.6276e-01],\n                        [ 6.4275e-02,  1.9973e-01,  1.9074e-01],\n                        [ 1.4361e-01,  9.9906e-02,  1.5353e-01]],\n              \n                       [[ 2.8793e-01,  7.6412e-02,  1.4833e-01],\n                        [-3.2402e-03,  8.3632e-02,  1.3911e-03],\n                        [ 8.0030e-02,  6.3052e-02,  1.4946e-01]],\n              \n                       [[ 2.0002e-01,  9.4750e-02,  1.9203e-02],\n                        [-4.6609e-02,  3.4544e-02, -4.7142e-03],\n                        [-2.3943e-01, -2.6703e-01,  1.2765e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 2.7661e-02, -4.3581e-02, -1.4086e-02],\n                        [-1.2761e-01, -1.7279e-01, -1.0709e-01],\n                        [-7.9220e-02, -2.2687e-01, -1.4756e-01]],\n              \n                       [[ 4.7685e-02,  4.8688e-02,  1.3971e-01],\n                        [-1.3608e-01, -8.3628e-02,  6.9632e-02],\n                        [ 9.1915e-02,  4.5217e-03,  1.1531e-02]],\n              \n                       [[-5.7376e-02,  7.1628e-02, -3.6217e-02],\n                        [ 1.2146e-03,  1.3340e-01, -8.4881e-03],\n                        [-1.2917e-01,  8.2631e-02,  8.8910e-02]],\n              \n                       ...,\n              \n                       [[-1.0486e-01,  3.9948e-02,  8.3973e-02],\n                        [-1.3131e-01, -1.5182e-02,  1.9836e-02],\n                        [ 7.4269e-02,  9.6436e-03,  1.2247e-01]],\n              \n                       [[ 9.2583e-02,  3.1173e-02,  1.9368e-02],\n                        [ 1.4132e-01, -5.4591e-02,  1.0564e-02],\n                        [ 7.5344e-05, -1.8119e-02, -6.8161e-02]],\n              \n                       [[-5.4142e-03, -5.1753e-02, -1.4532e-01],\n                        [ 7.8464e-02,  1.6945e-01,  4.0065e-02],\n                        [ 1.3242e-01, -7.8796e-02,  4.3361e-02]]],\n              \n              \n                      [[[ 8.6407e-02, -2.0485e-02, -4.1478e-02],\n                        [-8.1664e-03,  1.6171e-01,  3.2499e-01],\n                        [-6.1992e-03,  1.7203e-01,  6.7128e-02]],\n              \n                       [[-7.2360e-02, -1.8068e-02, -8.3238e-02],\n                        [ 6.7195e-02, -8.3139e-02,  8.8158e-02],\n                        [ 4.6776e-02, -5.1923e-02,  4.1569e-02]],\n              \n                       [[-2.6398e-01, -2.2950e-02,  6.0595e-02],\n                        [-8.1655e-02, -8.4466e-02, -6.8850e-02],\n                        [ 8.0754e-02, -9.9144e-02, -1.2725e-01]],\n              \n                       ...,\n              \n                       [[-3.1667e-02, -5.7766e-02, -6.0434e-02],\n                        [-5.3645e-03, -1.4186e-01,  2.6282e-02],\n                        [-4.4796e-02, -1.2199e-01,  2.6505e-02]],\n              \n                       [[ 2.7835e-01,  1.6449e-01,  1.9190e-01],\n                        [ 2.6077e-01,  2.4747e-01,  1.8488e-01],\n                        [ 1.0598e-02,  2.5463e-01,  1.6594e-01]],\n              \n                       [[ 2.5156e-02,  2.1771e-01, -1.8196e-02],\n                        [-1.0949e-01, -4.1238e-02,  5.7142e-02],\n                        [-3.0551e-01, -1.9752e-01, -8.1494e-02]]],\n              \n              \n                      [[[-7.9886e-02,  1.6805e-02,  1.0457e-01],\n                        [ 1.5730e-01,  2.7993e-03,  4.3450e-02],\n                        [ 4.2932e-02, -6.8590e-02, -1.1252e-02]],\n              \n                       [[-3.0173e-01, -1.5897e-01, -1.7037e-01],\n                        [-1.5876e-01, -1.3726e-02, -1.9559e-01],\n                        [-2.5998e-01, -1.9566e-01, -2.6876e-01]],\n              \n                       [[-1.7469e-01,  4.1239e-02, -2.9417e-01],\n                        [-7.5146e-03,  3.0580e-01, -5.5075e-03],\n                        [-2.7282e-01,  7.7060e-02, -2.4311e-01]],\n              \n                       ...,\n              \n                       [[ 1.0166e-01,  6.0584e-02,  7.3718e-02],\n                        [-1.2162e-01, -1.3124e-01, -6.8151e-02],\n                        [ 4.0134e-02,  1.2167e-01, -3.2655e-02]],\n              \n                       [[ 1.0428e-01,  1.2578e-01,  7.0375e-02],\n                        [-1.5123e-02,  8.4134e-03,  1.3399e-01],\n                        [-9.2118e-02, -8.7077e-02,  1.4237e-01]],\n              \n                       [[ 5.0121e-03,  5.7338e-02,  6.2209e-02],\n                        [ 1.0156e-01,  1.1208e-01, -3.9040e-02],\n                        [-7.8542e-02, -1.1320e-01, -1.3093e-01]]]])),\n             ('layer3.0.bn2.weight',\n              tensor([1.0208, 0.9587, 0.6802, 0.6858, 0.7839, 0.7227, 0.6571, 0.6171, 0.6901,\n                      0.7654, 0.6553, 0.8217, 0.9072, 1.2739, 0.8368, 0.8587, 0.9361, 0.7447,\n                      0.7818, 0.5964, 0.7326, 0.8206, 1.0730, 0.7203, 0.9855, 1.0868, 0.7978,\n                      0.6918, 0.9850, 0.6245, 1.0045, 0.6975, 0.7941, 0.7888, 0.8163, 0.8344,\n                      0.8455, 1.0071, 0.8091, 0.8077, 0.6550, 0.6179, 0.7887, 1.1800, 0.8239,\n                      0.7152, 0.7942, 0.6959, 0.6452, 0.7457, 0.6845, 0.6892, 0.8850, 0.9453,\n                      1.1256, 1.1879, 0.7788, 0.8621, 0.8755, 0.9579, 0.6099, 0.6464, 0.9873,\n                      0.8411, 0.9122, 0.9414, 1.0045, 0.7369, 0.7845, 0.6681, 1.1932, 1.1224,\n                      0.7267, 0.6301, 0.8421, 0.9457, 0.8734, 0.6845, 0.7823, 0.6815, 0.8649,\n                      0.8159, 0.8684, 0.6737, 0.7712, 1.0795, 0.8967, 0.6045, 0.7168, 1.0670,\n                      0.7670, 0.7133, 0.7763, 0.8339, 0.7488, 0.7173, 0.6798, 0.7263, 0.6420,\n                      0.8506, 0.8370, 0.7346, 0.5794, 1.1696, 0.9321, 1.0746, 0.8071, 0.8447,\n                      0.9406, 0.6323, 0.7604, 0.7636, 0.9056, 0.7188, 0.8877, 0.8869, 0.6959,\n                      0.7182, 0.6896, 0.8204, 0.8618, 0.8249, 1.3500, 0.7257, 0.6985, 0.7103,\n                      0.7569, 0.8753, 0.6182, 0.6025, 0.8256, 1.3125, 0.6631, 0.8024, 0.9257,\n                      0.7334, 0.9517, 0.9582, 0.6189, 0.7545, 0.7702, 0.7548, 0.6263, 0.7405,\n                      0.6348, 0.8860, 0.9238, 0.9175, 0.7969, 0.9564, 0.5791, 0.8843, 0.8097,\n                      0.7414, 0.8908, 0.6228, 1.1029, 0.8648, 0.7721, 0.8652, 1.0092, 0.8120,\n                      0.7779, 0.7429, 0.9945, 1.0320, 0.7101, 0.5310, 0.7294, 0.7128, 0.7982,\n                      1.1035, 0.9077, 0.5775, 0.8393, 0.8170, 0.8159, 0.8591, 0.8181, 0.9417,\n                      1.1474, 0.8016, 0.6700, 0.9826, 0.8269, 0.5408, 1.0075, 0.6204, 0.5883,\n                      0.7232, 0.8765, 0.8691, 0.8644, 1.2661, 0.9978, 0.9365, 0.6838, 0.6764,\n                      0.7520, 0.7907, 0.7122, 0.6203, 0.8594, 0.5654, 0.6816, 0.8095, 0.7492,\n                      0.9598, 0.7443, 1.0474, 0.6588, 0.8434, 0.6371, 0.7355, 0.8268, 0.7459,\n                      0.7187, 0.5939, 0.7767, 0.6567, 0.7327, 0.7931, 0.6276, 0.8829, 0.7646,\n                      1.0630, 0.8046, 0.7946, 0.7553, 0.7871, 0.6947, 1.2216, 0.9677, 0.6274,\n                      1.1133, 0.7990, 0.8746, 0.7567, 0.9706, 0.6880, 0.8725, 0.5775, 1.1380,\n                      0.9675, 0.7486, 0.8837, 1.1639, 0.9134, 0.7276, 1.0991, 0.6179, 0.7353,\n                      0.8766, 0.6983, 0.6067, 1.0409])),\n             ('layer3.0.bn2.bias',\n              tensor([-0.3777, -0.1877, -0.2183, -0.1577, -0.2501, -0.2719, -0.2954, -0.0819,\n                      -0.3835, -0.2421, -0.1571, -0.0432, -0.2204, -0.3408, -0.3004, -0.1336,\n                      -0.0952, -0.1301, -0.1674, -0.1878, -0.0930, -0.0047, -0.2586, -0.0797,\n                      -0.1544, -0.2502, -0.3816, -0.2988, -0.2173, -0.2304, -0.2683, -0.2006,\n                      -0.2384, -0.1636, -0.1285, -0.1281, -0.2618, -0.2140, -0.1584, -0.1797,\n                      -0.1932, -0.2291, -0.1121, -0.3726, -0.1850, -0.1184, -0.1427, -0.2114,\n                      -0.2017, -0.2580, -0.2042, -0.2420, -0.0942, -0.1073, -0.2207, -0.3517,\n                      -0.2533, -0.2838, -0.2240, -0.1909, -0.1561, -0.2264, -0.0740, -0.3993,\n                      -0.1789, -0.1406, -0.0948, -0.1401, -0.1895, -0.1572, -0.2993, -0.1897,\n                      -0.1872, -0.2171, -0.3780, -0.0106, -0.2905, -0.3231, -0.1638, -0.1486,\n                      -0.2544, -0.1411, -0.1936, -0.3628, -0.2064,  0.0223, -0.3248, -0.2069,\n                      -0.2340, -0.2302, -0.1232, -0.2421, -0.2215, -0.3008, -0.2534, -0.1041,\n                      -0.3014, -0.2069, -0.1031, -0.2101, -0.1937, -0.1756, -0.2367, -0.1216,\n                      -0.2349, -0.1975, -0.1946, -0.0877, -0.2878, -0.1340, -0.2178, -0.1478,\n                      -0.2864, -0.1600, -0.3437, -0.0407, -0.1879, -0.3033, -0.2105, -0.2163,\n                       0.0236, -0.1361, -0.2798, -0.2346, -0.2107, -0.1331, -0.1500, -0.2855,\n                      -0.2239, -0.1957, -0.2463, -0.4201, -0.2282, -0.2545, -0.2152, -0.2302,\n                      -0.1724, -0.2982, -0.1912, -0.1970, -0.2937, -0.1763, -0.1579, -0.2073,\n                      -0.2151, -0.3083, -0.1937, -0.1903, -0.2749, -0.1564, -0.1924, -0.1951,\n                      -0.1976, -0.2060, -0.1664, -0.1630, -0.1719, -0.2098, -0.0743, -0.1794,\n                      -0.2192, -0.1084, -0.1681, -0.3133, -0.2439, -0.2415, -0.2013, -0.2012,\n                      -0.2420, -0.2582, -0.1563, -0.0391, -0.2035, -0.2403, -0.1610, -0.0706,\n                      -0.1818, -0.2718, -0.1469, -0.2144, -0.3050, -0.2072, -0.1225, -0.1925,\n                      -0.2133, -0.1676, -0.1741, -0.1586, -0.1836, -0.1322, -0.2144, -0.2607,\n                      -0.1332, -0.3027, -0.2520, -0.1680, -0.1838, -0.1338, -0.3254, -0.2845,\n                      -0.1544, -0.2438, -0.0361, -0.2755, -0.0657, -0.1811, -0.1564, -0.0234,\n                      -0.2723, -0.2562, -0.1185, -0.3003, -0.2037, -0.1541, -0.2839, -0.0846,\n                      -0.1882, -0.1814, -0.2058, -0.1413, -0.2912, -0.2219, -0.2816, -0.2513,\n                      -0.2951, -0.3562, -0.0315, -0.2404, -0.2035, -0.2631, -0.2130, -0.2869,\n                      -0.2344, -0.2237, -0.0695, -0.2035, -0.1620, -0.1782, -0.2095, -0.1864,\n                      -0.1469, -0.3003, -0.4605, -0.1256, -0.1670, -0.2805, -0.2407, -0.1774,\n                      -0.1454, -0.1491, -0.2927, -0.2411, -0.2430, -0.2170, -0.1504, -0.3711])),\n             ('layer3.0.bn2.running_mean',\n              tensor([ -1.6524,  -2.7671,   2.1591,   3.4830,  -2.4282,   0.3685,   2.7102,\n                       -4.3822,   1.3342,   1.1851,  -8.2512,  -3.1820,  -2.5780,  -3.0267,\n                        0.3570,  -5.0583,  -3.8727,  -6.8682,  -4.1593,  -4.3876,   4.0838,\n                       -6.2356,  -0.4974,  -5.0145,  -2.8154,  -2.7125,   2.2623,  -3.8636,\n                       -1.7593,   1.2525,   2.2541,  -1.4657,   2.3978,   0.1985,  -4.3358,\n                       -3.6055,  -2.3142,  -3.6561,  -3.6388,  -2.8271,   1.7787,  -5.5080,\n                       -7.3583,  -2.4238,  -4.4431,  -4.3769,   0.3046,  -7.4090,   4.7513,\n                        0.7589,  -4.8125,  -3.5939,  -5.2473,  -1.2743,  -2.5756,  -5.7122,\n                        0.8094,  -4.2426,  -6.0670,  -3.0689,  -1.7540,   0.5680,  -4.9720,\n                       -4.4697,  -5.9402,  -5.2842,  -3.6539,  -4.8153,  -3.2790,  -0.0425,\n                       -4.3329,  -1.9858,  -3.3800, -11.1369,  -0.8082,  -4.5922,  -4.8769,\n                       -0.3327,  -1.3572,  -6.8242,  -4.5573,  -0.4867,  -4.0544,   0.2787,\n                       -2.4907,  -3.5464,  -5.0783,   1.7891,   0.2882,  -5.0541,  -2.2339,\n                        3.7536,  -8.7388,  -0.8756,  -2.3498,  -4.6347,   0.3829,   5.8677,\n                       -2.9249,  -7.7603,  -6.1750,  -3.9294,  -5.2496,  -5.1854,  -2.6903,\n                       -1.1464,  -1.7087,  -3.2292,  -0.9359,  -5.8305,  -1.8626,  -1.0982,\n                       -1.7734,   4.7712,  -0.0769, -10.6986,  -5.1955,  -1.5949,  -1.2565,\n                        0.7219,  -4.1539,  -0.9426,  -0.3364,   2.3347,  -1.6235,   2.5753,\n                       -0.0741,  -5.7667,  -3.6338,  -5.0335,  -3.8862,  -2.8389,   3.9841,\n                        1.1434,  -1.1547,   0.4497,   0.9501,  -2.0331,  -4.8971,   0.7387,\n                       -4.4550,   0.0220,  -3.4548,  -4.0135,   0.8851,  -1.8999,  -2.4489,\n                        3.4601,  -4.2689,  -4.7864,  -3.1723,  -1.5284,  -2.3705,  -6.9281,\n                       -3.2039,  -6.6131,  -2.3683,  -3.1531,  -4.7890,   4.6794,   0.8352,\n                       -5.9745,  -1.2797,  -2.0160,  -3.8530,  -2.8089,  -2.3315,  -0.0701,\n                        4.6393,  -5.7093,   1.0945,  -3.5814,  -2.9737,   0.9903,  -2.2882,\n                       -3.4228,  -2.2093,   2.4564,  -0.1087,  -6.4520,  -3.6966,   1.5198,\n                       -5.4298,  -3.9350,  -1.9144,  -1.9907,  -6.0076,  -7.4479,  -3.8139,\n                       -6.7735,  -4.5762,  -0.4562,  -4.2038,  -0.9457,  -0.1088,  -5.8646,\n                       -8.1168,  -4.8045,   0.5364,   2.5892,  -1.5317,   2.8171,  -3.4481,\n                        5.3094,   2.0406,  -0.7719,  -2.4673,  -2.3453,  -5.7339,  -3.1471,\n                       -1.9215,  -3.3315,   2.5242,  -6.2615,  -0.2609,  -4.5276,   1.3231,\n                        0.7994,  -0.3845,   1.3468,  -1.6448,  -0.7292,  -1.2417,   3.0528,\n                       -1.1260,  -0.7183,  -3.6242,  -3.3460,   1.1598,   0.3776,  -1.3899,\n                       -0.1050,  -3.3603,  -5.5021,  -2.5742,  -4.5704,  -4.4430,  -1.2001,\n                       -3.9192,  -3.0524,  -6.0476,   1.1884,  -4.2590,  -5.3374,  -6.5311,\n                       -2.3770,  -4.6627,  -4.5328,  -2.3760,  -2.3353,   2.6527,  -1.2884,\n                        3.0861,  -3.0629,   2.9937,  -3.1178])),\n             ('layer3.0.bn2.running_var',\n              tensor([14.7198, 16.7772, 41.1826, 55.0354, 15.8135, 23.3128, 40.7639, 49.0547,\n                      38.2545, 20.8499, 33.7592, 18.4794, 18.6190, 17.4677, 14.3755, 19.5534,\n                      26.8010, 23.4417, 25.0946, 14.8937, 28.8605, 27.6835, 17.1865, 25.0609,\n                      23.8732, 15.1957, 16.1546, 25.5300, 15.6343, 23.7612, 19.1866, 22.3018,\n                      20.3156, 20.7927, 38.2249, 19.4293, 16.0712, 20.8078, 13.5783, 17.8769,\n                      40.2484, 36.7660, 25.5427, 17.3990, 28.7954, 27.2188, 25.2501, 21.4823,\n                      49.5417, 17.7173, 27.7496, 24.8220, 21.6301, 15.5423, 20.1228, 24.7355,\n                      23.8384, 17.2194, 21.3174, 17.9198, 34.3222, 20.6622, 22.5448, 20.2202,\n                      25.4387, 16.7229, 17.5115, 32.1878, 18.8468, 37.8526, 18.5781, 19.5274,\n                      24.3782, 34.6726, 19.7659, 29.7001, 22.7832, 17.1097, 18.0491, 28.7431,\n                      18.9439, 21.6217, 17.2658, 32.8390, 23.6144, 38.5752, 24.0672, 43.3215,\n                      44.0362, 15.8340, 23.5645, 31.0011, 27.0316, 20.2687, 19.3712, 19.9189,\n                      29.6498, 37.3654, 17.5759, 29.6512, 21.3947, 23.0446, 35.9175, 21.1972,\n                      18.9139, 17.9943, 17.1033, 24.2415, 19.4268, 29.4942, 36.3021, 17.9403,\n                      19.1644, 26.5905, 18.2997, 55.3988, 22.8487, 17.0965, 28.5644, 19.2509,\n                      20.5682, 16.2762, 14.6717, 24.0195, 26.5937, 24.4469, 48.8718, 20.2101,\n                      35.5945, 34.2377, 15.4571, 17.6930, 53.4563, 18.7446, 25.9388, 27.5279,\n                      15.5272, 14.8992, 46.1166, 58.4746, 24.1541, 26.4863, 39.3272, 17.2144,\n                      26.9310, 18.2429, 19.2187, 25.6797, 24.2718, 19.7446, 38.8415, 18.6382,\n                      20.6189, 26.9104, 17.9165, 45.5329, 23.9175, 18.2194, 20.9005, 20.4583,\n                      25.3546, 25.1518, 22.1249, 20.6241, 22.1571, 17.9514, 24.1975, 36.4976,\n                      43.1688, 20.3527, 21.4784, 35.9345, 22.8816, 38.8602, 36.7865, 22.5594,\n                      23.9258, 66.2662, 45.6898, 25.7337, 20.5001, 49.4249, 31.1578, 18.5363,\n                      20.6848, 16.6512, 26.1315, 36.5424, 41.1595, 24.9358, 19.9346, 17.1603,\n                      15.2407, 14.1107, 19.2886, 27.2095, 32.3180, 34.8345, 16.9990, 26.1843,\n                      27.9507, 24.8523, 22.0644, 26.1543, 44.3288, 19.1487, 23.5155, 31.1652,\n                      25.3295, 15.7136, 22.2229, 14.2088, 44.7021, 33.0957, 16.6948, 34.8160,\n                      41.1234, 53.0282, 16.5947, 40.8299, 18.3028, 21.5201, 19.0924, 14.6514,\n                      20.5015, 13.4260, 23.3999, 23.1923, 20.7941, 26.1544, 25.5914, 13.6908,\n                      25.5746, 24.0995, 22.4462, 18.7114, 17.0443, 31.5755, 19.0764, 21.7020,\n                      21.7243, 17.3905, 17.4693, 30.3994, 34.2354, 17.0130, 23.1031, 21.6364,\n                      28.9137, 18.9167, 20.8497, 29.1782, 18.1608, 23.3489, 56.3904, 17.8710])),\n             ('layer3.0.bn2.num_batches_tracked', tensor(14770)),\n             ('layer3.0.shortcut.0.weight',\n              tensor([[[[ 0.0013]],\n              \n                       [[ 0.1228]],\n              \n                       [[-0.0125]],\n              \n                       ...,\n              \n                       [[ 0.1407]],\n              \n                       [[-0.0206]],\n              \n                       [[-0.0081]]],\n              \n              \n                      [[[ 0.0272]],\n              \n                       [[-0.0351]],\n              \n                       [[-0.0220]],\n              \n                       ...,\n              \n                       [[ 0.1312]],\n              \n                       [[-0.2121]],\n              \n                       [[-0.1830]]],\n              \n              \n                      [[[-0.2125]],\n              \n                       [[ 0.0577]],\n              \n                       [[-0.1027]],\n              \n                       ...,\n              \n                       [[-0.0793]],\n              \n                       [[ 0.2158]],\n              \n                       [[ 0.1750]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0342]],\n              \n                       [[-0.0929]],\n              \n                       [[-0.1371]],\n              \n                       ...,\n              \n                       [[ 0.1015]],\n              \n                       [[-0.0835]],\n              \n                       [[-0.1275]]],\n              \n              \n                      [[[-0.1507]],\n              \n                       [[ 0.0416]],\n              \n                       [[ 0.2940]],\n              \n                       ...,\n              \n                       [[-0.1440]],\n              \n                       [[ 0.1242]],\n              \n                       [[ 0.1862]]],\n              \n              \n                      [[[-0.0121]],\n              \n                       [[-0.0483]],\n              \n                       [[ 0.2507]],\n              \n                       ...,\n              \n                       [[ 0.1186]],\n              \n                       [[ 0.1290]],\n              \n                       [[ 0.2134]]]])),\n             ('layer3.0.shortcut.1.weight',\n              tensor([0.6045, 0.8491, 1.0249, 0.9906, 0.9218, 0.8394, 0.8583, 0.9548, 0.6404,\n                      0.9378, 0.9760, 0.8299, 0.7944, 0.7151, 0.9084, 0.9203, 0.6327, 0.8991,\n                      0.7250, 1.0048, 0.9354, 0.6638, 0.8250, 0.9311, 0.6478, 0.8575, 0.7877,\n                      0.7770, 0.9160, 0.9851, 0.8422, 0.9107, 0.9093, 0.9072, 0.7262, 0.9837,\n                      0.8882, 0.9146, 0.9417, 0.7542, 0.9534, 0.8645, 0.9740, 0.7166, 0.8894,\n                      0.9034, 0.8462, 0.9812, 0.8543, 0.9277, 0.9533, 0.8637, 0.8336, 0.7606,\n                      0.7046, 0.7054, 0.9517, 0.9604, 0.7744, 0.9426, 0.9301, 0.8744, 0.7912,\n                      0.8244, 0.6355, 0.9435, 0.8025, 0.8532, 0.9369, 1.0420, 0.7626, 0.8368,\n                      0.9057, 0.8622, 0.8869, 0.7879, 0.8351, 0.9141, 0.9847, 0.9495, 0.8884,\n                      0.7609, 0.8471, 1.0184, 0.9591, 0.7790, 0.8857, 0.9559, 0.7647, 0.8730,\n                      0.8017, 0.9475, 0.8308, 0.9060, 0.8949, 0.8382, 0.9944, 0.8805, 0.8710,\n                      0.8444, 0.8292, 0.8194, 0.8743, 0.5687, 0.8406, 0.9534, 0.9301, 0.7609,\n                      0.6920, 1.0082, 0.8606, 0.7621, 0.8660, 0.8486, 0.9197, 0.6840, 0.9222,\n                      0.9696, 0.9023, 0.9656, 0.8171, 0.8691, 0.9610, 0.8839, 0.7669, 0.8383,\n                      0.8807, 0.8614, 1.0003, 0.9223, 0.9605, 0.7455, 1.0314, 0.8322, 0.6650,\n                      0.8604, 0.7590, 1.0230, 0.8579, 0.7884, 0.9481, 0.9141, 0.9550, 0.8722,\n                      0.9028, 0.8563, 0.8698, 0.9178, 0.8471, 0.5788, 0.9254, 0.8526, 0.9625,\n                      0.7085, 0.7551, 0.8085, 0.7489, 0.9107, 0.9767, 0.8489, 0.6408, 0.7302,\n                      0.9009, 1.0475, 0.8362, 0.8082, 0.8533, 0.9457, 1.0657, 0.9281, 1.0497,\n                      0.7920, 0.8691, 0.9822, 1.0799, 0.9677, 0.8241, 0.7650, 0.9727, 0.6774,\n                      0.8811, 0.8192, 0.8277, 0.9622, 0.6706, 0.9699, 0.7024, 0.9160, 1.0470,\n                      1.0596, 0.6622, 0.8057, 0.8998, 0.7830, 0.9019, 0.8965, 0.8740, 0.9653,\n                      0.8323, 0.9906, 0.9009, 0.9643, 0.8795, 0.9341, 0.8893, 1.0191, 1.0443,\n                      0.6019, 0.9057, 0.7966, 0.9890, 0.9193, 0.9579, 0.7596, 0.7940, 0.9202,\n                      0.8300, 0.9162, 0.9120, 0.8830, 1.1002, 0.7423, 0.8995, 0.9143, 1.0019,\n                      0.8394, 0.7222, 0.9983, 0.8239, 0.8849, 0.8652, 0.6792, 0.7422, 0.9342,\n                      0.7893, 0.7834, 0.9361, 0.8312, 0.9622, 0.8280, 0.7783, 0.9595, 0.7285,\n                      0.8286, 0.8983, 0.7738, 0.8543, 0.8584, 0.8629, 0.7483, 0.9047, 0.8631,\n                      0.7700, 0.9474, 0.9880, 1.0353])),\n             ('layer3.0.shortcut.1.bias',\n              tensor([-0.3777, -0.1877, -0.2183, -0.1577, -0.2501, -0.2719, -0.2954, -0.0819,\n                      -0.3835, -0.2421, -0.1571, -0.0432, -0.2204, -0.3408, -0.3004, -0.1336,\n                      -0.0952, -0.1301, -0.1674, -0.1878, -0.0930, -0.0047, -0.2586, -0.0797,\n                      -0.1544, -0.2502, -0.3816, -0.2988, -0.2173, -0.2304, -0.2683, -0.2006,\n                      -0.2384, -0.1636, -0.1285, -0.1281, -0.2618, -0.2140, -0.1584, -0.1797,\n                      -0.1932, -0.2291, -0.1121, -0.3726, -0.1850, -0.1184, -0.1427, -0.2114,\n                      -0.2017, -0.2580, -0.2042, -0.2420, -0.0942, -0.1073, -0.2207, -0.3517,\n                      -0.2533, -0.2838, -0.2240, -0.1909, -0.1561, -0.2264, -0.0740, -0.3993,\n                      -0.1789, -0.1406, -0.0948, -0.1401, -0.1895, -0.1572, -0.2993, -0.1897,\n                      -0.1872, -0.2171, -0.3780, -0.0106, -0.2905, -0.3231, -0.1638, -0.1486,\n                      -0.2544, -0.1411, -0.1936, -0.3628, -0.2064,  0.0223, -0.3248, -0.2069,\n                      -0.2340, -0.2302, -0.1232, -0.2421, -0.2215, -0.3008, -0.2534, -0.1041,\n                      -0.3014, -0.2069, -0.1031, -0.2101, -0.1937, -0.1756, -0.2367, -0.1216,\n                      -0.2349, -0.1975, -0.1946, -0.0877, -0.2878, -0.1340, -0.2178, -0.1478,\n                      -0.2864, -0.1600, -0.3437, -0.0407, -0.1879, -0.3033, -0.2105, -0.2163,\n                       0.0236, -0.1361, -0.2798, -0.2346, -0.2107, -0.1331, -0.1500, -0.2855,\n                      -0.2239, -0.1957, -0.2463, -0.4201, -0.2282, -0.2545, -0.2152, -0.2302,\n                      -0.1724, -0.2982, -0.1912, -0.1970, -0.2937, -0.1763, -0.1579, -0.2073,\n                      -0.2151, -0.3083, -0.1937, -0.1903, -0.2749, -0.1564, -0.1924, -0.1951,\n                      -0.1976, -0.2060, -0.1664, -0.1630, -0.1719, -0.2098, -0.0743, -0.1794,\n                      -0.2192, -0.1084, -0.1681, -0.3133, -0.2439, -0.2415, -0.2013, -0.2012,\n                      -0.2420, -0.2582, -0.1563, -0.0391, -0.2035, -0.2403, -0.1610, -0.0706,\n                      -0.1818, -0.2718, -0.1469, -0.2144, -0.3050, -0.2072, -0.1225, -0.1925,\n                      -0.2133, -0.1676, -0.1741, -0.1586, -0.1836, -0.1322, -0.2144, -0.2607,\n                      -0.1332, -0.3027, -0.2520, -0.1680, -0.1838, -0.1338, -0.3254, -0.2845,\n                      -0.1544, -0.2438, -0.0361, -0.2755, -0.0657, -0.1811, -0.1564, -0.0234,\n                      -0.2723, -0.2562, -0.1185, -0.3003, -0.2037, -0.1541, -0.2839, -0.0846,\n                      -0.1882, -0.1814, -0.2058, -0.1413, -0.2912, -0.2219, -0.2816, -0.2513,\n                      -0.2951, -0.3562, -0.0315, -0.2404, -0.2035, -0.2631, -0.2130, -0.2869,\n                      -0.2344, -0.2237, -0.0695, -0.2035, -0.1620, -0.1782, -0.2095, -0.1864,\n                      -0.1469, -0.3003, -0.4605, -0.1256, -0.1670, -0.2805, -0.2407, -0.1774,\n                      -0.1454, -0.1491, -0.2927, -0.2411, -0.2430, -0.2170, -0.1504, -0.3711])),\n             ('layer3.0.shortcut.1.running_mean',\n              tensor([-0.4047, -0.9015, -1.8414, -3.2604,  0.5449, -1.4932, -0.6691, -2.1912,\n                      -2.5344, -0.6695, -2.2830, -4.2605, -1.9206, -0.1111, -2.4968, -0.0319,\n                      -1.6769, -0.1396,  0.3176, -1.1248, -1.8909, -3.0771, -1.8510, -1.8659,\n                      -2.1461,  1.0769, -0.4715,  0.3776, -2.0066, -0.8343,  0.4238, -0.6968,\n                      -1.6349, -1.5340, -1.4748, -1.4181, -0.9210, -3.1599, -2.8403, -0.6435,\n                      -1.7176, -1.5885, -2.4023, -0.7705, -1.2651, -1.2371, -0.4291, -1.6731,\n                      -2.1196, -4.0307,  1.0895, -1.2606, -0.6197, -3.2863, -0.9960,  1.4896,\n                      -1.4249, -0.0283, -1.7362,  0.9893, -3.3494,  0.1249, -0.9034,  1.3762,\n                      -1.5285, -2.9590, -1.5652,  1.4918, -2.3116, -2.5468, -0.5885, -1.0089,\n                      -2.1437, -0.6135, -0.4522, -1.7270, -0.2595, -1.8308, -1.3524, -1.2344,\n                      -0.1265, -1.7753, -1.7263, -4.0901,  0.2214, -2.0204, -0.7519, -1.3928,\n                      -0.9204, -2.2180, -2.4523, -0.1103, -0.8438, -0.6858, -0.8421, -1.9864,\n                      -0.7677, -1.4027, -0.6231, -0.3870,  0.4685, -2.3573, -0.5470, -1.5394,\n                      -0.1627, -0.3944,  0.4667, -3.5182, -1.9361, -3.1432, -1.4378, -0.9072,\n                      -1.0690, -1.7500,  1.3952, -1.1453, -1.7338, -2.0761, -2.7465,  0.6223,\n                      -2.0324,  1.0748,  0.0075, -2.9086, -1.9750, -1.5459, -2.1056,  0.7014,\n                      -2.8937, -0.6837, -1.7182,  2.9996, -1.7492, -0.5305, -1.9844, -0.6550,\n                      -4.0104, -2.2009, -0.9259, -2.5410, -0.6000, -3.0361, -1.2200, -0.7264,\n                      -0.2129, -0.4557, -0.4743, -0.8905, -1.4066, -0.6870, -1.0538, -2.0799,\n                      -1.6621, -1.9658, -3.1231, -1.4577,  0.6507, -0.9725, -2.6961, -1.7651,\n                       1.4185, -1.5548, -2.0023, -0.9665,  1.2047, -1.7254, -0.6032, -2.9482,\n                      -2.7863, -0.9540, -3.8340, -3.3684,  2.4685, -1.3065,  0.9981,  0.1736,\n                      -1.2165, -1.6962, -3.2822,  2.2083, -1.2652, -3.6513, -0.6441, -0.8762,\n                      -2.4549, -0.8661,  1.6035, -0.7378, -0.6992, -1.1249, -1.8991, -0.4053,\n                      -1.1478, -0.3730, -3.8725, -1.0706,  0.3116, -0.1345, -1.5625, -0.9560,\n                      -1.0900, -0.8407, -3.1220, -2.6113, -2.4434,  0.4301, -1.3231, -1.1239,\n                       2.1942, -0.3708, -0.4990, -0.9771, -2.2503,  0.6921, -1.0958, -2.2592,\n                      -3.9416, -1.2999, -0.0174, -1.2612, -0.9177, -1.7779, -3.2087, -1.9964,\n                      -1.1496, -2.0551, -2.1300, -1.4086, -0.1212, -0.5980, -1.2995, -4.5321,\n                      -1.6119, -1.0117, -2.7613,  1.0635, -0.5016, -1.4911, -0.9216, -1.9364,\n                      -0.7886, -0.6700,  0.1205, -1.3843, -0.3865,  0.3061,  1.4881, -1.8291,\n                      -3.8087, -0.5713,  0.8626,  0.2672,  0.1390, -1.1836, -0.3602,  0.4790])),\n             ('layer3.0.shortcut.1.running_var',\n              tensor([4.2383, 2.8979, 2.7852, 3.2285, 2.6188, 3.1345, 2.3833, 4.1511, 6.4461,\n                      2.7137, 2.2247, 3.6809, 3.8394, 3.5426, 4.1386, 2.9334, 3.6417, 3.1151,\n                      4.4774, 3.2379, 3.6771, 7.5859, 2.5854, 4.4370, 5.2250, 2.8202, 2.5397,\n                      2.8259, 2.4860, 2.3191, 4.6229, 3.2032, 6.3911, 2.9305, 2.9150, 3.1212,\n                      2.4578, 3.4575, 3.7479, 5.6017, 2.3816, 2.4566, 3.5557, 3.4019, 3.5373,\n                      3.8396, 3.2540, 4.0713, 4.4203, 4.1736, 2.4256, 2.2368, 3.8966, 5.5028,\n                      5.3372, 5.4826, 2.5657, 2.8153, 3.9743, 3.3241, 4.7447, 3.0164, 3.5054,\n                      2.7691, 4.4622, 2.5745, 5.0768, 3.1281, 2.6925, 3.0261, 3.5266, 4.7106,\n                      3.8266, 2.7361, 3.4781, 4.4290, 2.3422, 2.9522, 2.6588, 2.7405, 2.2107,\n                      4.4153, 2.6239, 3.4523, 1.8832, 6.8118, 3.9317, 3.2882, 4.1802, 3.8403,\n                      4.7796, 2.5836, 4.0903, 2.6982, 3.8700, 2.9497, 3.0169, 3.6283, 2.8144,\n                      2.1963, 3.8031, 3.9705, 2.5526, 4.0301, 2.6694, 4.3004, 2.3580, 5.9074,\n                      3.5063, 2.2090, 3.7027, 3.2846, 3.6652, 4.0565, 2.7671, 4.3469, 2.5841,\n                      3.1080, 5.1139, 2.1280, 4.0106, 2.3176, 4.0831, 3.9025, 2.3869, 2.4972,\n                      5.3870, 3.0506, 2.7645, 3.9811, 3.4987, 5.4738, 4.2524, 3.3662, 4.6437,\n                      3.2115, 5.1261, 3.5545, 4.1007, 6.4443, 2.9657, 3.6171, 5.7570, 3.0091,\n                      2.7825, 2.8460, 3.2183, 3.7961, 3.7557, 5.4813, 2.5346, 3.0978, 3.5375,\n                      5.0535, 3.6769, 2.5902, 2.7668, 2.9637, 2.8187, 3.8315, 5.7415, 4.8188,\n                      2.5925, 3.1376, 3.6462, 2.7869, 3.6379, 2.8448, 2.5386, 2.6419, 4.1104,\n                      6.3989, 3.4668, 3.7835, 3.3099, 3.1197, 6.8336, 4.8789, 3.5392, 4.5006,\n                      2.9965, 6.5979, 2.5183, 3.9569, 3.8333, 1.8982, 4.4603, 4.9839, 2.8773,\n                      2.3032, 5.1910, 3.5717, 3.7102, 3.3281, 3.9140, 4.0264, 3.9032, 3.1203,\n                      6.1218, 3.5562, 2.9762, 3.2089, 3.9710, 2.6418, 4.4725, 3.0329, 1.9800,\n                      6.5378, 2.2294, 3.7311, 2.4882, 2.4208, 3.9259, 4.4104, 2.8069, 5.5502,\n                      8.1292, 4.5637, 3.0645, 4.7025, 2.4325, 4.7177, 3.0771, 5.2931, 3.1083,\n                      2.6506, 4.0823, 3.6766, 3.4614, 2.9203, 3.2858, 3.7021, 4.0990, 2.1558,\n                      5.5638, 3.8971, 2.4531, 4.1298, 2.6696, 2.7041, 4.7782, 2.6969, 3.0109,\n                      3.7418, 2.5153, 3.6325, 4.7008, 3.5413, 4.1993, 6.3625, 2.3924, 3.6849,\n                      3.3848, 3.4721, 2.5477, 2.2565])),\n             ('layer3.0.shortcut.1.num_batches_tracked', tensor(14770)),\n             ('layer3.1.conv1.weight',\n              tensor([[[[-0.0535,  0.0313, -0.1264],\n                        [-0.1016, -0.2603, -0.3367],\n                        [-0.1388, -0.1416, -0.2632]],\n              \n                       [[ 0.1106,  0.1014,  0.1314],\n                        [ 0.2046,  0.0914,  0.0690],\n                        [-0.1765, -0.1693, -0.0423]],\n              \n                       [[ 0.1020,  0.1831, -0.0144],\n                        [-0.0147, -0.0267,  0.1098],\n                        [ 0.1039,  0.0728,  0.1753]],\n              \n                       ...,\n              \n                       [[-0.0353,  0.0453,  0.0317],\n                        [ 0.1052,  0.1342,  0.2003],\n                        [ 0.0300,  0.0370,  0.1054]],\n              \n                       [[ 0.0323, -0.0295,  0.0888],\n                        [-0.0789, -0.1961,  0.0790],\n                        [ 0.0959,  0.1017,  0.1164]],\n              \n                       [[ 0.0796, -0.0782,  0.0602],\n                        [ 0.0944, -0.1741, -0.3104],\n                        [-0.0242,  0.0225,  0.0924]]],\n              \n              \n                      [[[-0.2362, -0.1320, -0.1664],\n                        [-0.1348, -0.1417, -0.0347],\n                        [ 0.0150,  0.0885, -0.2333]],\n              \n                       [[ 0.0171, -0.1969, -0.0141],\n                        [-0.0990,  0.0222,  0.1215],\n                        [-0.1487,  0.1282,  0.2454]],\n              \n                       [[-0.0030,  0.0813, -0.0029],\n                        [ 0.2043,  0.0308, -0.1618],\n                        [ 0.1547, -0.2242, -0.1372]],\n              \n                       ...,\n              \n                       [[-0.1408, -0.1020,  0.2262],\n                        [-0.0963, -0.1066,  0.1400],\n                        [-0.1020,  0.0589,  0.1084]],\n              \n                       [[ 0.0209,  0.1273,  0.1201],\n                        [ 0.1612,  0.1585,  0.0973],\n                        [ 0.2072,  0.0126, -0.1539]],\n              \n                       [[-0.0751, -0.1373,  0.0227],\n                        [-0.0952, -0.2800, -0.1190],\n                        [-0.1024, -0.1120, -0.0206]]],\n              \n              \n                      [[[ 0.1198,  0.0575,  0.0414],\n                        [-0.0007,  0.1382,  0.1379],\n                        [-0.2612, -0.1335, -0.2367]],\n              \n                       [[-0.0568,  0.0779,  0.1869],\n                        [-0.0353,  0.0415,  0.1360],\n                        [ 0.0968, -0.0055, -0.0075]],\n              \n                       [[ 0.0755,  0.0890, -0.0351],\n                        [ 0.1227,  0.0411, -0.1515],\n                        [-0.0200, -0.0332, -0.1315]],\n              \n                       ...,\n              \n                       [[ 0.0396,  0.0681,  0.0907],\n                        [-0.0523, -0.0708, -0.0684],\n                        [-0.1738, -0.2886, -0.2889]],\n              \n                       [[ 0.1095,  0.1618, -0.0600],\n                        [ 0.0554,  0.0953,  0.0243],\n                        [-0.1203, -0.0035, -0.0326]],\n              \n                       [[ 0.0110, -0.0239, -0.0743],\n                        [ 0.0703,  0.0013, -0.1099],\n                        [ 0.1150,  0.0008,  0.0011]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0052, -0.1562, -0.0871],\n                        [-0.1170, -0.0360, -0.0633],\n                        [ 0.0502,  0.0110, -0.1849]],\n              \n                       [[-0.0650, -0.0106, -0.2873],\n                        [ 0.0620,  0.0505,  0.0480],\n                        [-0.1186, -0.0858,  0.0409]],\n              \n                       [[ 0.1782,  0.1596,  0.2037],\n                        [ 0.0809,  0.1443,  0.1540],\n                        [ 0.0164,  0.0808,  0.1832]],\n              \n                       ...,\n              \n                       [[-0.0936, -0.1369, -0.0332],\n                        [-0.2858, -0.4047, -0.2437],\n                        [-0.2330, -0.2684, -0.2594]],\n              \n                       [[-0.0263,  0.0142,  0.0212],\n                        [-0.0161,  0.0572,  0.0757],\n                        [-0.0409,  0.0150,  0.0309]],\n              \n                       [[ 0.1302,  0.0550,  0.2168],\n                        [ 0.0051,  0.0404,  0.1235],\n                        [-0.1594, -0.0304,  0.0828]]],\n              \n              \n                      [[[ 0.0887,  0.1424, -0.0693],\n                        [ 0.0719,  0.3049,  0.1132],\n                        [ 0.0521, -0.1477, -0.2163]],\n              \n                       [[-0.0886, -0.0017,  0.1932],\n                        [-0.0731,  0.0034,  0.1311],\n                        [-0.1535, -0.0256, -0.0169]],\n              \n                       [[ 0.2055,  0.1418, -0.2113],\n                        [ 0.0569,  0.0236, -0.2049],\n                        [-0.1779, -0.2362, -0.1685]],\n              \n                       ...,\n              \n                       [[-0.0741,  0.0725, -0.0934],\n                        [ 0.1004,  0.0250,  0.0355],\n                        [ 0.1569,  0.0758, -0.0051]],\n              \n                       [[ 0.1570,  0.0421, -0.1855],\n                        [ 0.1264,  0.0649, -0.1302],\n                        [-0.0732, -0.1392, -0.1948]],\n              \n                       [[-0.0074,  0.1865, -0.1799],\n                        [ 0.0538, -0.0692, -0.2387],\n                        [ 0.1415, -0.0007,  0.0734]]],\n              \n              \n                      [[[-0.0216, -0.0557,  0.0261],\n                        [-0.0151, -0.0407, -0.2752],\n                        [ 0.1356,  0.0615,  0.0849]],\n              \n                       [[ 0.0401, -0.1461, -0.0704],\n                        [-0.0247, -0.1628,  0.1064],\n                        [ 0.1090,  0.1368,  0.1868]],\n              \n                       [[ 0.1897,  0.1171,  0.0432],\n                        [ 0.1379,  0.1691,  0.1111],\n                        [ 0.1194,  0.0602, -0.0708]],\n              \n                       ...,\n              \n                       [[ 0.1917,  0.0518, -0.0530],\n                        [-0.1979, -0.1521, -0.0422],\n                        [-0.1186, -0.0047,  0.0659]],\n              \n                       [[ 0.1450,  0.1193, -0.0017],\n                        [ 0.0447,  0.0141, -0.1374],\n                        [-0.0206, -0.1641, -0.2272]],\n              \n                       [[ 0.2059,  0.0540, -0.1189],\n                        [-0.2280, -0.0183, -0.0488],\n                        [-0.3469, -0.0238, -0.1553]]]])),\n             ('layer3.1.bn1.weight',\n              tensor([0.5807, 0.8537, 0.4624, 0.5548, 0.6428, 0.7017, 0.5515, 0.6022, 0.6592,\n                      0.3591, 0.2948, 0.9627, 0.8568, 0.5980, 0.3795, 0.3746, 0.9468, 0.8858,\n                      0.7534, 0.8600, 0.4420, 0.6631, 0.7532, 0.6008, 0.7329, 0.8642, 0.7867,\n                      0.8392, 0.4543, 0.5494, 0.8934, 0.2843, 0.7179, 0.8027, 0.6386, 0.6467,\n                      0.5072, 0.6608, 0.4333, 0.5782, 0.3663, 0.5577, 0.6758, 0.8388, 0.7608,\n                      0.6162, 0.7399, 0.9205, 0.5866, 0.5504, 0.5211, 0.5538, 0.9092, 0.8634,\n                      0.5197, 0.5607, 0.4731, 0.5470, 0.5408, 0.3216, 1.0417, 0.5546, 0.9265,\n                      0.8042, 0.5390, 0.8658, 0.6616, 0.5378, 0.8471, 0.3892, 0.5699, 0.4186,\n                      0.7254, 0.8220, 0.8355, 0.6532, 0.8801, 0.5079, 0.9705, 0.2879, 0.4804,\n                      0.8985, 0.9989, 1.0149, 0.7264, 0.3506, 0.8785, 0.3982, 0.4920, 0.7253,\n                      0.7033, 0.9798, 0.5563, 0.4558, 0.4862, 0.6202, 0.8216, 0.8466, 0.5879,\n                      0.5625, 0.8713, 0.5025, 1.0879, 1.0215, 0.9014, 0.4070, 0.6539, 0.4737,\n                      0.3777, 0.5754, 0.9598, 0.7453, 0.5913, 0.9181, 0.6397, 1.0448, 0.9276,\n                      0.8659, 0.6315, 0.7583, 1.0133, 0.3330, 0.9918, 0.5306, 0.9204, 0.7372,\n                      1.0096, 0.5454, 0.8092, 0.7614, 0.7121, 0.5823, 0.6906, 0.4560, 0.9643,\n                      0.5983, 0.6882, 0.5814, 0.7939, 0.7328, 0.7173, 0.6443, 0.9373, 0.8582,\n                      0.8752, 0.5613, 0.9040, 0.5820, 0.4091, 0.4730, 0.7589, 0.6213, 0.6886,\n                      0.8160, 0.9046, 0.4189, 0.6057, 0.6957, 1.0476, 0.7360, 0.7729, 0.4584,\n                      0.6998, 0.9261, 0.5475, 0.9638, 0.4459, 0.8974, 0.5828, 0.5205, 0.8883,\n                      0.8981, 0.5370, 0.5962, 0.4076, 0.5122, 0.7087, 0.5661, 0.4763, 0.8163,\n                      0.4247, 0.5722, 0.3386, 0.4741, 0.6644, 0.6013, 0.5826, 0.3936, 0.4893,\n                      0.5724, 1.0949, 0.4461, 0.8234, 0.8199, 0.5537, 0.8771, 0.6113, 0.4293,\n                      0.6084, 0.7853, 0.8827, 0.5708, 0.7866, 0.6275, 0.4990, 1.0512, 0.5910,\n                      0.6950, 0.4709, 0.9079, 0.9762, 0.6929, 0.6274, 0.4730, 0.5976, 0.4096,\n                      1.1237, 0.4562, 0.4583, 0.4034, 0.9170, 0.5824, 0.5457, 0.3512, 0.3763,\n                      0.3718, 0.5112, 0.7072, 0.8230, 0.5117, 0.7322, 0.5684, 0.4439, 0.4716,\n                      0.6951, 0.7484, 0.5601, 0.8544, 0.5016, 0.3953, 0.4218, 0.4132, 0.5578,\n                      0.6734, 0.6157, 0.6974, 0.8254, 0.6637, 0.8044, 0.7569, 1.0602, 0.7054,\n                      0.4876, 0.6456, 0.6674, 0.7065])),\n             ('layer3.1.bn1.bias',\n              tensor([-0.4323, -0.4084, -0.3338, -0.4250, -0.4346, -0.3920, -0.5569, -0.4677,\n                      -0.4784, -0.4242, -0.4430, -0.3395, -0.4018, -0.3585, -0.3803, -0.4941,\n                      -0.4321, -0.4712, -0.3788, -0.4829, -0.3910, -0.3589, -0.4333, -0.4746,\n                      -0.5619, -0.5020, -0.4263, -0.4013, -0.4752, -0.4514, -0.5814, -0.3924,\n                      -0.4501, -0.5195, -0.3245, -0.3984, -0.4311, -0.4657, -0.4617, -0.4497,\n                      -0.4579, -0.4656, -0.4984, -0.5001, -0.3238, -0.4220, -0.4740, -0.5366,\n                      -0.4605, -0.4239, -0.3916, -0.3879, -0.4411, -0.4872, -0.4433, -0.2920,\n                      -0.4672, -0.4148, -0.4669, -0.4300, -0.3094, -0.4673, -0.5195, -0.4691,\n                      -0.4262, -0.2925, -0.4236, -0.4350, -0.4233, -0.4524, -0.4619, -0.4087,\n                      -0.4923, -0.5440, -0.4000, -0.4622, -0.3606, -0.4816, -0.5147, -0.4259,\n                      -0.4409, -0.3655, -0.5315, -0.2765, -0.4657, -0.4814, -0.5007, -0.4158,\n                      -0.4183, -0.3650, -0.4642, -0.3622, -0.4303, -0.4297, -0.4147, -0.4749,\n                      -0.5722, -0.5742, -0.4507, -0.3915, -0.4959, -0.4139, -0.4907, -0.3295,\n                      -0.4459, -0.4714, -0.4374, -0.3722, -0.4375, -0.4498, -0.5039, -0.4752,\n                      -0.3575, -0.4063, -0.4987, -0.4972, -0.3796, -0.3456, -0.5539, -0.3908,\n                      -0.4696, -0.4007, -0.1989, -0.4641, -0.4830, -0.3621, -0.5379, -0.4919,\n                      -0.3756, -0.5235, -0.5110, -0.3224, -0.3334, -0.4465, -0.4909, -0.4871,\n                      -0.3036, -0.3948, -0.4760, -0.4411, -0.4264, -0.4161, -0.4858, -0.3903,\n                      -0.4891, -0.4255, -0.3439, -0.3913, -0.4224, -0.5072, -0.5042, -0.4212,\n                      -0.5167, -0.2634, -0.5599, -0.4274, -0.4015, -0.3657, -0.4051, -0.3540,\n                      -0.3598, -0.4514, -0.4310, -0.5484, -0.4256, -0.3687, -0.4769, -0.5404,\n                      -0.3713, -0.4491, -0.3460, -0.2799, -0.4555, -0.4641, -0.3804, -0.4388,\n                      -0.4234, -0.4923, -0.4718, -0.4852, -0.4009, -0.3448, -0.4484, -0.4817,\n                      -0.4958, -0.4294, -0.4235, -0.4309, -0.4447, -0.4317, -0.5309, -0.4533,\n                      -0.3757, -0.4465, -0.3647, -0.6197, -0.3405, -0.4403, -0.4268, -0.4740,\n                      -0.4995, -0.4637, -0.3150, -0.3997, -0.3704, -0.5576, -0.4290, -0.3024,\n                      -0.4690, -0.3019, -0.5692, -0.4182, -0.3351, -0.3777, -0.3904, -0.4376,\n                      -0.5555, -0.4073, -0.4365, -0.3839, -0.4592, -0.4554, -0.3694, -0.4890,\n                      -0.3737, -0.3182, -0.4116, -0.3759, -0.4712, -0.4604, -0.5938, -0.4659,\n                      -0.4427, -0.3365, -0.5226, -0.5266, -0.4473, -0.6344, -0.4216, -0.4789,\n                      -0.4495, -0.3941, -0.3293, -0.4527, -0.4189, -0.4884, -0.5310, -0.4497,\n                      -0.4729, -0.4786, -0.3824, -0.3711, -0.4011, -0.4468, -0.6261, -0.4063])),\n             ('layer3.1.bn1.running_mean',\n              tensor([-12.7123, -15.1434,   5.3942,   7.4946,  -5.9029,  -7.5774,   1.7560,\n                       -9.8324,  -9.1418,   3.7772,   6.0199, -17.9432, -10.3740,  -3.3602,\n                      -19.2959,   5.5764, -24.0600,  -3.4121, -17.0730,  -8.6444, -23.2720,\n                       -3.5863,  -3.0540, -12.3530,  -6.2896,  -9.1250, -13.3063,  -7.8516,\n                        3.0805,  10.2285,  -2.7267,   3.9070,  -9.8419, -16.0567, -22.9168,\n                      -17.6599, -19.1023, -18.2150,   3.0644, -16.3603,   3.1692,  -8.5879,\n                       -6.4324,  -2.0837, -15.2027, -10.8095,  -3.1608, -12.7886,  -1.9585,\n                      -16.4087, -21.6926, -11.7416,  -1.3559,  -5.1639, -15.9448, -20.5088,\n                       -8.3612, -23.6412,  -6.6381,   7.8424,  -8.7535, -19.4846,  -7.2342,\n                       -5.6820,   5.5722,  -4.2679, -16.7728,   1.3669, -13.4919,   8.4444,\n                      -20.9076,   8.3376,  -7.5829,  -8.6100,  -2.0024, -16.8394,  -0.0896,\n                      -14.0367,  -5.2940,  12.4887,   4.1538,  -8.8815,  -2.4520, -10.8454,\n                       -6.8593,  -0.0968,  -9.9086, -18.9288, -17.1394, -22.7076,   0.5929,\n                      -12.4371, -15.8080,  -4.3934, -14.0051,   1.8412,   1.5814, -13.1850,\n                        0.3901, -17.3443,  -8.6311, -13.2887,  -3.5027,  -9.0140,  -7.1612,\n                       -2.8347, -10.9594,  -8.5872,   3.4282, -11.1402,  -8.8423, -17.4228,\n                      -20.9417,  -0.0291,  -4.4040,  -9.5789,  -4.7806, -10.5474, -11.1271,\n                       -7.4824,  -7.6104,   2.1480,  -0.1415, -19.0235,  -4.8700,  -7.7383,\n                       -7.0391,   6.9526, -15.1371, -12.8348,   4.7256, -18.4427, -19.6870,\n                       -6.8980,  -5.2624,   7.0944, -16.9974,  -9.6095,  -9.5115,  -6.9379,\n                       -7.5060, -16.7507,  -7.8930,  -7.4755,  -3.3836, -19.9287,  -6.1300,\n                      -13.4666,  -4.1135,   8.4831,  -3.7948, -11.4269,   2.1837, -18.8766,\n                       -8.7903,   2.9875, -17.3681, -19.6581, -10.6190, -16.1248, -20.9016,\n                        7.4267,  -9.2176,  -9.6797, -18.4971,  -0.6492, -18.7475,   1.9846,\n                      -28.9953,   4.2703,   0.7546, -13.0213, -11.6458, -16.9874,   0.1322,\n                        7.9229,   7.2062,   1.0895,   6.5593,  -3.1480,  -5.8743, -16.7662,\n                       12.3122,   9.9011,  -6.6926, -11.6907, -14.9271,   4.9764, -12.8039,\n                      -12.3092,  -5.2416, -15.2526, -11.8359,  -3.7754, -20.8885,  -3.3536,\n                      -18.6334,  -7.7319, -19.1880, -10.8661, -18.9630, -26.9493, -12.3571,\n                        9.4161, -18.5613,  -6.0090,   1.4900, -10.7619,   1.2870,  -4.7588,\n                       -2.9043,  -6.5137,  -8.9254, -10.6025, -11.8246,  14.0654,  -8.0692,\n                       11.1370,   4.2261,   5.0604,  -7.2983,  -3.9548, -11.7902,  11.4105,\n                       -2.2299,  12.1959,   4.1054,   0.8082,  -3.0293, -12.8183,  -5.8464,\n                      -17.4656, -13.7390,  -0.1352,  -8.8331,   0.0993, -12.6120, -10.4786,\n                      -14.0140,  21.8952,   0.9933,  -2.5601, -19.6447, -12.7671,  -0.0315,\n                       -7.8117,  -5.2658, -21.0836,  -0.7378,  -7.2157,  -3.8128, -18.4673,\n                       -1.4920, -12.5887,  -5.1097, -15.8328])),\n             ('layer3.1.bn1.running_var',\n              tensor([ 96.6572,  86.8736, 102.3951,  94.5308,  99.1402, 104.5153, 110.5645,\n                       94.5547,  81.1202, 103.2498,  92.9824, 113.4991,  78.9570, 113.4824,\n                      187.1758, 125.4558, 101.7803, 115.7220,  99.8554, 138.2629, 262.4030,\n                      101.1885,  79.8853, 140.0045,  90.2959,  94.0002, 100.9024,  96.9551,\n                      101.0658,  87.1775,  68.2834, 125.6551, 109.7274,  96.6585, 189.1900,\n                      120.2205, 185.5325, 110.4345,  86.5652, 128.7204,  66.5790, 130.2088,\n                      200.6765,  66.7850, 124.4488, 100.4874, 103.8382,  77.9101,  88.5706,\n                      116.0784, 257.4100, 121.4846,  91.6664, 103.7174, 174.1113, 310.8594,\n                      125.6643, 153.6417, 198.8163,  77.2199,  87.2486, 209.3991, 127.9582,\n                      117.1711, 109.3217,  76.9171, 103.3101, 121.6434,  88.9278,  81.0212,\n                      163.8900, 105.4931, 113.6795,  69.7678,  90.3774, 110.0894, 125.6723,\n                      152.5669, 123.9719, 111.9949,  82.5676, 101.7854,  90.7888, 112.6875,\n                      110.1403,  88.5031, 100.2894, 131.5844, 175.8442, 111.7747,  92.4614,\n                      110.4560, 132.6060,  97.2608, 239.3743,  84.7962,  99.2416,  86.6864,\n                       83.2673, 193.9027,  90.6351, 202.4155,  99.1573, 111.2252, 101.5201,\n                      191.2090,  85.0293, 114.4833,  92.2354, 174.7505,  91.4119, 104.9696,\n                      170.5750,  92.4385,  86.6549, 127.0354,  88.2082, 118.8078,  93.4680,\n                       86.2410, 136.8383,  73.2185,  72.8343, 153.9451,  74.7779,  64.9924,\n                       78.2504,  89.9621, 105.3379,  97.5356,  78.4160, 143.0430, 132.2785,\n                      108.5571, 120.0732,  86.3136, 110.1783, 169.5883, 100.4099,  95.0072,\n                       88.0966, 106.6883,  91.6331, 110.2677, 107.5910, 232.3993,  84.6254,\n                      146.3559, 155.1229,  74.5028,  94.2390, 167.5296,  87.2606, 111.1835,\n                       77.1840, 119.8129, 125.8461, 115.9155, 113.3225, 108.2875, 140.9962,\n                      110.4205, 140.7871, 115.2892, 171.6153,  86.4561, 189.8306, 104.4032,\n                      276.4703,  82.6974,  86.6958, 105.0077, 110.1666, 175.4138, 108.2754,\n                      118.3873,  83.7411,  65.0299,  64.7659, 104.8116, 100.8059, 127.8345,\n                       76.8651, 110.2692,  72.1219,  94.0792, 140.2168,  79.8336, 130.8481,\n                      106.9479, 125.3031, 200.2451, 104.0361,  72.2519, 167.7463,  90.3748,\n                      146.2544, 185.5724, 133.0603,  88.7760,  80.4902,  98.5730,  80.3328,\n                       82.8983, 178.0956,  95.3303,  93.1189, 110.5755, 101.3609,  81.2767,\n                       89.8384,  74.1202, 120.1218, 111.8603, 167.7071,  99.5192,  89.4955,\n                       76.9001,  81.7727,  76.0728, 115.1215, 130.8233, 162.0501, 104.3093,\n                      134.3392, 150.5004,  98.7853, 100.6757, 134.8941, 152.7933,  69.3419,\n                      136.4367, 199.0758, 132.3124, 100.2362,  93.0000, 122.8700,  80.7203,\n                      171.6641, 103.0679,  66.4615, 109.7698, 175.2566, 125.7942,  96.5268,\n                       89.9037,  74.1191, 125.1407,  88.2417,  75.4986,  88.2335, 106.4226,\n                      114.3806, 106.8594,  66.1304, 112.9003])),\n             ('layer3.1.bn1.num_batches_tracked', tensor(14770)),\n             ('layer3.1.conv2.weight',\n              tensor([[[[-0.1473, -0.2085, -0.2386],\n                        [-0.0193, -0.1494, -0.1411],\n                        [-0.1708, -0.0962, -0.2876]],\n              \n                       [[-0.0695,  0.0686,  0.0710],\n                        [ 0.0595,  0.0901,  0.1196],\n                        [-0.0442,  0.1128,  0.1777]],\n              \n                       [[ 0.0290, -0.0212,  0.0992],\n                        [ 0.0475,  0.0456, -0.0664],\n                        [-0.0588,  0.0083, -0.1610]],\n              \n                       ...,\n              \n                       [[-0.0022,  0.0105,  0.0246],\n                        [ 0.1175, -0.0295, -0.0267],\n                        [-0.0417,  0.0107, -0.0720]],\n              \n                       [[-0.1621, -0.1039,  0.1100],\n                        [ 0.3098,  0.2217,  0.1293],\n                        [ 0.1922,  0.0413,  0.0811]],\n              \n                       [[ 0.1303,  0.1646,  0.0134],\n                        [ 0.1167, -0.0329, -0.0914],\n                        [-0.0918, -0.1127, -0.2591]]],\n              \n              \n                      [[[-0.3624, -0.3476, -0.0441],\n                        [-0.0349, -0.0755,  0.1559],\n                        [ 0.0170,  0.1034,  0.2048]],\n              \n                       [[ 0.0894,  0.2281, -0.1706],\n                        [ 0.1551,  0.3190, -0.1300],\n                        [ 0.1042,  0.2033, -0.1441]],\n              \n                       [[-0.0850, -0.1823, -0.1449],\n                        [ 0.0541,  0.0305,  0.0035],\n                        [ 0.0064, -0.0361, -0.0699]],\n              \n                       ...,\n              \n                       [[ 0.1516,  0.1357,  0.0400],\n                        [ 0.0067,  0.0554,  0.0930],\n                        [ 0.0220,  0.0862,  0.1783]],\n              \n                       [[ 0.0605, -0.1214, -0.1628],\n                        [ 0.0148,  0.1766,  0.1040],\n                        [-0.0560,  0.0532, -0.0548]],\n              \n                       [[ 0.0904, -0.0246, -0.1329],\n                        [ 0.0339, -0.0160, -0.1646],\n                        [-0.1155, -0.1132,  0.0529]]],\n              \n              \n                      [[[ 0.1747, -0.1221, -0.1437],\n                        [ 0.0909,  0.1747, -0.1037],\n                        [ 0.1247,  0.0596, -0.1484]],\n              \n                       [[-0.0675,  0.0339, -0.0209],\n                        [-0.2409, -0.2045,  0.0243],\n                        [-0.0386, -0.1493,  0.0899]],\n              \n                       [[ 0.0584, -0.0160,  0.0004],\n                        [-0.1629, -0.1069,  0.0703],\n                        [-0.1196, -0.2358, -0.0630]],\n              \n                       ...,\n              \n                       [[ 0.0605,  0.0996,  0.0060],\n                        [ 0.0824,  0.1346,  0.0066],\n                        [-0.1775, -0.1026, -0.1711]],\n              \n                       [[-0.0144,  0.2840, -0.0678],\n                        [-0.0234, -0.0525,  0.0576],\n                        [ 0.0401, -0.0148,  0.0672]],\n              \n                       [[-0.0249,  0.1163,  0.0956],\n                        [ 0.2073,  0.1914, -0.0036],\n                        [ 0.0353,  0.0334, -0.0781]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0852,  0.1081,  0.0137],\n                        [-0.0327,  0.1587, -0.0563],\n                        [-0.0975, -0.0464, -0.1663]],\n              \n                       [[ 0.0220,  0.0018, -0.0084],\n                        [ 0.0327,  0.0325, -0.0536],\n                        [-0.1058, -0.0442, -0.2196]],\n              \n                       [[-0.0543, -0.0351,  0.0030],\n                        [ 0.0107,  0.1167, -0.0332],\n                        [-0.0068, -0.1129, -0.0972]],\n              \n                       ...,\n              \n                       [[-0.0531, -0.1276, -0.0992],\n                        [ 0.0898, -0.0189,  0.0046],\n                        [-0.0758, -0.0512, -0.1174]],\n              \n                       [[ 0.1989, -0.0066,  0.1016],\n                        [ 0.0346,  0.0898, -0.0984],\n                        [ 0.0851, -0.2438, -0.1426]],\n              \n                       [[ 0.1137, -0.0283, -0.1901],\n                        [ 0.0574,  0.1174,  0.0445],\n                        [-0.1062,  0.0226, -0.1578]]],\n              \n              \n                      [[[ 0.1111,  0.0604,  0.2261],\n                        [ 0.2029,  0.1644,  0.2053],\n                        [ 0.1086,  0.0014, -0.0667]],\n              \n                       [[ 0.0766, -0.0758,  0.1899],\n                        [-0.2096, -0.0402,  0.0327],\n                        [ 0.1734,  0.2523,  0.1805]],\n              \n                       [[-0.0105, -0.0009, -0.0297],\n                        [-0.1364, -0.2503, -0.1921],\n                        [ 0.0555, -0.0405,  0.0596]],\n              \n                       ...,\n              \n                       [[ 0.0415, -0.0466, -0.0441],\n                        [-0.0110,  0.0070, -0.1418],\n                        [ 0.0580,  0.0278, -0.1133]],\n              \n                       [[ 0.2034,  0.1852, -0.0717],\n                        [ 0.0820,  0.0413, -0.1086],\n                        [-0.0766, -0.0239, -0.1347]],\n              \n                       [[ 0.1175,  0.0337,  0.1569],\n                        [ 0.1547,  0.0505,  0.2632],\n                        [-0.0462, -0.0166,  0.0372]]],\n              \n              \n                      [[[-0.2496, -0.1889,  0.0187],\n                        [-0.1205,  0.0099, -0.0861],\n                        [ 0.0888,  0.0896, -0.3513]],\n              \n                       [[ 0.1277,  0.0260,  0.1314],\n                        [ 0.1286,  0.0822,  0.0262],\n                        [-0.1185, -0.0680, -0.0553]],\n              \n                       [[ 0.0639,  0.0498,  0.0499],\n                        [-0.0052, -0.0462, -0.0025],\n                        [-0.0362, -0.0883,  0.0165]],\n              \n                       ...,\n              \n                       [[-0.0033,  0.0439, -0.0443],\n                        [-0.0418,  0.0547, -0.0075],\n                        [ 0.0586,  0.1111, -0.0242]],\n              \n                       [[ 0.0497,  0.0219,  0.0753],\n                        [-0.0973,  0.0721, -0.1889],\n                        [ 0.0405, -0.1381, -0.1274]],\n              \n                       [[-0.0047,  0.1032,  0.0061],\n                        [-0.0257,  0.0611,  0.1085],\n                        [-0.2539, -0.2061, -0.0333]]]])),\n             ('layer3.1.bn2.weight',\n              tensor([0.7663, 0.8345, 0.6825, 0.6713, 0.8286, 1.0002, 0.6503, 0.6604, 1.1021,\n                      0.6127, 0.7047, 0.6676, 0.7710, 0.9891, 0.6273, 0.6957, 0.7913, 0.6275,\n                      0.8952, 0.7090, 1.0028, 0.6831, 0.8057, 0.6724, 0.7060, 0.7291, 0.7524,\n                      0.6163, 0.7441, 0.7528, 1.1065, 0.7508, 0.9854, 0.5507, 0.5472, 0.7469,\n                      0.7314, 0.6301, 0.8869, 0.9908, 0.6178, 0.6084, 0.8000, 1.0595, 0.7608,\n                      0.7373, 0.6052, 0.7259, 0.7585, 0.9076, 0.6600, 0.8148, 0.7602, 0.7328,\n                      0.8296, 0.9513, 0.7746, 0.7032, 0.9124, 0.7162, 0.6519, 0.5979, 0.6061,\n                      0.8910, 0.9995, 0.8420, 0.7693, 0.7139, 0.7171, 0.6811, 0.9505, 0.8230,\n                      0.8582, 0.6650, 0.6825, 0.9278, 0.6618, 0.6961, 0.8027, 0.6314, 0.8126,\n                      0.5599, 0.5866, 0.7951, 0.6597, 0.7668, 0.7888, 0.6059, 1.1899, 0.7480,\n                      0.8646, 0.6496, 0.6822, 0.7295, 0.9519, 0.6265, 0.7714, 0.7238, 0.8288,\n                      0.7082, 0.7264, 0.6480, 0.5879, 0.9810, 0.7450, 0.7540, 0.9752, 0.7565,\n                      0.8583, 0.6562, 0.5892, 0.6567, 0.7529, 0.7020, 1.0405, 0.7306, 0.7482,\n                      0.6719, 0.6053, 0.6590, 0.6293, 0.7680, 0.8832, 0.7229, 0.7492, 0.5591,\n                      0.6533, 0.6974, 0.5654, 0.6910, 0.7849, 0.8366, 0.6046, 0.7712, 0.9056,\n                      0.6519, 0.8777, 0.7363, 0.5598, 0.7220, 0.7046, 0.6683, 0.7598, 0.6053,\n                      0.8135, 0.8646, 0.6356, 0.7661, 0.5708, 1.2057, 0.6668, 1.0525, 0.5699,\n                      0.6892, 0.8343, 0.5431, 0.7387, 0.7570, 0.6469, 0.8131, 0.8927, 0.8464,\n                      0.7201, 0.7382, 0.7369, 0.9250, 0.6293, 0.4786, 0.6589, 0.7658, 0.6512,\n                      0.8936, 0.6928, 0.8082, 0.7060, 0.7113, 0.7075, 0.8414, 0.5059, 1.0500,\n                      0.8010, 0.7720, 0.7513, 0.9943, 0.9030, 0.5256, 1.0915, 0.7246, 0.6471,\n                      0.5929, 0.9731, 0.5047, 0.8752, 0.9652, 0.7467, 0.7571, 0.5437, 0.6014,\n                      0.6907, 0.5997, 0.6380, 0.9643, 0.7284, 0.5389, 0.6858, 0.7053, 0.7483,\n                      0.9285, 0.5821, 0.9749, 0.6048, 0.9132, 0.5948, 1.0825, 0.8703, 0.6615,\n                      0.9166, 0.6265, 0.8547, 0.6730, 0.6973, 0.8037, 0.7377, 0.8709, 0.5427,\n                      0.9633, 1.1382, 0.8320, 0.6643, 0.6087, 0.5254, 0.8606, 0.9365, 0.6614,\n                      1.0353, 1.0733, 0.7138, 0.8840, 0.6587, 0.6403, 0.6409, 0.5961, 0.9092,\n                      0.6942, 0.6993, 0.8611, 0.8177, 0.8360, 0.5796, 0.8490, 0.5823, 0.6057,\n                      0.8175, 0.5699, 0.6198, 0.8840])),\n             ('layer3.1.bn2.bias',\n              tensor([-4.5116e-01, -9.8221e-02, -1.8101e-01, -1.2339e-01, -5.1066e-02,\n                      -1.0172e-01, -4.0244e-02, -4.4380e-01, -1.3355e-01, -1.7694e-01,\n                      -2.4363e-01, -3.0003e-01, -1.1470e-01, -4.3431e-01, -5.1482e-01,\n                      -1.2095e-01, -2.7045e-01,  3.5752e-04, -6.8224e-02, -4.5982e-01,\n                      -1.0553e-01, -3.9751e-02, -3.4761e-01, -1.7632e-01, -1.3436e-01,\n                       1.1597e-02, -2.5238e-01, -1.4519e-01, -2.2663e-01, -2.2228e-01,\n                      -4.8175e-01, -4.4619e-01, -1.8362e-01, -2.3837e-01, -2.8522e-01,\n                      -3.7365e-01, -3.5446e-01, -3.9792e-01, -4.0179e-01, -2.6102e-01,\n                      -1.5117e-01, -8.7804e-03, -1.7794e-01, -4.9586e-01, -1.1927e-01,\n                      -1.3920e-01,  1.2949e-01, -1.1323e-01, -1.8711e-01, -4.3610e-01,\n                       4.1556e-02, -1.2289e-01, -1.6049e-01, -3.3203e-01, -5.1156e-01,\n                      -3.1749e-01, -1.9653e-01, -4.1248e-01, -2.6989e-01, -1.5003e-01,\n                      -2.0242e-01, -3.7424e-01,  2.2441e-02, -4.0164e-01, -1.6214e-01,\n                      -2.6270e-01, -1.4182e-01, -2.6549e-01, -1.8628e-01, -2.7530e-01,\n                      -3.5636e-01, -4.1772e-01, -2.6140e-01, -7.3238e-02, -9.1079e-02,\n                      -1.7730e-01, -1.1198e-01, -3.8916e-01, -1.8220e-01, -3.1945e-01,\n                      -2.9780e-01, -3.9931e-01, -1.3306e-01, -5.1267e-01, -3.2290e-01,\n                      -1.8763e-01, -4.0811e-01, -1.5448e-01, -9.7133e-02, -3.6109e-01,\n                      -2.8700e-01,  1.5847e-01, -1.8789e-01, -1.5205e-01, -4.9466e-01,\n                      -2.8595e-01, -1.7525e-01,  1.4675e-02, -4.4520e-01, -3.1754e-01,\n                      -2.4164e-01, -3.2866e-01, -3.0876e-01, -1.9384e-01, -4.3510e-01,\n                      -2.6616e-01, -2.3338e-01, -2.7453e-01, -4.1674e-01, -1.5639e-01,\n                       7.5091e-02, -2.5109e-01, -2.8613e-01, -5.5995e-02, -2.1705e-01,\n                      -1.7401e-01, -1.3108e-01, -3.1204e-01, -3.6032e-01, -6.7922e-02,\n                      -1.0457e-01,  6.4463e-02, -4.0192e-01, -3.5567e-01, -1.8499e-01,\n                       4.1333e-02, -3.2506e-01, -1.1904e-01,  2.8450e-02, -1.5064e-01,\n                      -1.8737e-01, -2.1457e-01, -3.2675e-01, -4.8417e-01, -2.4275e-01,\n                       1.6050e-02, -2.2662e-01, -3.7249e-01, -1.0870e-01, -3.6888e-01,\n                      -3.8160e-01, -2.0981e-01, -3.4965e-01, -3.0703e-01, -1.4515e-01,\n                      -2.5677e-01, -4.4574e-01, -1.4929e-01, -4.0964e-01, -2.2492e-01,\n                      -1.7158e-01, -3.6322e-01, -1.2560e-01, -2.8098e-01, -1.4606e-01,\n                      -5.7089e-02, -1.6407e-01, -3.9653e-01, -1.6861e-01, -4.4690e-01,\n                      -1.7819e-02, -1.5427e-01, -3.3745e-01, -1.6343e-01, -2.5872e-01,\n                      -3.3345e-01, -3.9789e-04, -3.6436e-01, -1.6440e-01, -3.0501e-01,\n                      -1.8526e-01, -2.7735e-01, -1.9459e-01, -3.6138e-01, -1.4151e-01,\n                      -8.8667e-02, -3.3753e-01, -3.4853e-01, -7.0475e-02, -1.3181e-01,\n                      -5.0375e-01, -4.3101e-01, -9.8296e-02, -3.9477e-01, -2.3953e-01,\n                      -3.1060e-01, -1.8043e-01, -1.7663e-01, -3.3989e-02, -4.3300e-01,\n                      -3.1403e-02, -4.4145e-01, -4.0036e-01, -4.9332e-01, -3.5997e-01,\n                      -3.4450e-01,  4.4099e-03, -3.4571e-01, -2.0594e-01, -9.5377e-02,\n                      -4.9831e-01, -2.7193e-01, -2.1113e-01, -3.0132e-01, -3.2384e-01,\n                      -1.4444e-01, -2.3569e-01, -7.2972e-02, -3.9808e-01, -4.0795e-01,\n                      -4.4137e-01, -4.8453e-01, -1.0613e-01, -3.5361e-01, -2.5706e-01,\n                      -3.3482e-01, -2.7076e-01, -2.4105e-01, -3.2621e-01, -3.9213e-02,\n                      -7.0711e-02, -3.0087e-01, -3.2800e-01, -4.5224e-01, -1.9393e-01,\n                      -3.7656e-01, -1.7976e-01, -4.4197e-01, -3.6802e-01, -2.7234e-01,\n                      -1.6897e-01, -5.0774e-01, -4.3968e-01, -7.0998e-02, -4.8928e-01,\n                      -1.2564e-01, -4.6035e-01, -1.9100e-01, -2.4540e-01, -1.9882e-01,\n                      -4.8047e-02, -2.5736e-01, -4.2506e-01, -3.0721e-01, -2.4679e-01,\n                      -3.2474e-01, -2.7656e-01, -8.9008e-02, -2.4168e-01, -4.5436e-01,\n                      -3.6460e-01,  7.1937e-02, -6.5169e-02, -8.6456e-02, -1.1561e-01,\n                      -4.3840e-01])),\n             ('layer3.1.bn2.running_mean',\n              tensor([-2.1879, -3.4154, -2.2931, -3.2676, -4.5591, -3.5660, -3.4854, -0.1309,\n                      -2.6043, -3.0596, -3.5027,  5.2993, -5.4356, -0.7854, -0.2578, -6.2507,\n                       0.6148, -6.3291, -4.5772, -1.1052, -2.4506,  0.3588, -2.1211,  0.6452,\n                      -3.5296, -3.5216, -2.0301, -0.3614, -1.1124, -5.0542, -0.9301,  2.7962,\n                       1.4091,  2.8248,  2.5083,  0.2037, -2.6191,  4.2486,  1.1349, -4.3336,\n                       2.8058, -4.0841, -2.0282, -1.1307, -4.2142, -0.0197, -4.9061, -4.1941,\n                      -2.5321,  1.2256, -4.6470, -3.8565, -3.4595,  1.7776, -3.0351, -2.4113,\n                      -3.6886, -3.6277, -2.9410, -5.7368,  4.4105,  5.2963, -5.0741, -0.4452,\n                      -2.1449, -4.3579, -1.2482, -4.0894, -4.4941, -0.1053, -1.6989, -2.0607,\n                       0.9184, -5.5677, -4.4839, -0.0533, -5.4065,  3.1841, -1.5389,  0.8890,\n                      -0.0662,  1.1523, -6.6647, -0.8343,  4.0908,  3.3497, -0.3349, -3.2041,\n                      -6.4377, -1.1603,  1.8032, -4.9510, -2.1287, -1.6309, -1.5112, -1.2747,\n                      -5.8114, -3.6934,  2.0919,  5.6521, -1.3209, -1.5931,  1.5901, -4.4786,\n                      -1.1318, -4.4682, -2.0050, -1.0954,  0.5473, -2.2596, -5.9352, -2.2812,\n                      -2.9786, -0.3747, -4.1374,  0.6341, -1.8112,  1.5671, -0.1345, -4.3566,\n                      -0.7262, -4.7250, -4.7988,  0.4680, -0.2556, -3.5696,  0.8958, -4.9504,\n                      -3.1187, -3.9061, -2.1925, -3.0017,  1.8429,  3.5178, -4.5540, -7.1311,\n                      -3.9387, -2.7498, -3.4824, -2.2860, -3.6657, -2.3666,  5.2214, -2.7015,\n                      -4.5452, -2.2920, -1.6585, -3.3854,  4.2758, -5.0234, -4.4199, -1.5684,\n                      -3.2148, -2.8186, -3.1622, -7.0493, -5.3999, -3.6359, -3.7545,  2.7017,\n                      -4.1198, -0.5076, -0.5397, -3.1042, -0.1914, -3.5498, -3.9117,  2.0242,\n                      -4.0337, -4.7992, -2.3417, -0.9597, -2.7092, -3.2998, -5.2896, -3.9327,\n                      -1.0370, -2.3091, -2.7273, -4.5171,  0.8959,  2.8085, -4.9107, -3.1665,\n                      -4.4033,  2.2241, -5.7346, -4.4459, -3.5197,  5.0246, -6.4376,  2.4261,\n                      -0.4665, -0.5633,  2.7973,  5.4959, -7.7403, -2.7525, -4.3225,  0.1526,\n                       2.1105, -3.7360,  0.2387,  4.2897,  2.0216, -3.8100, -3.0094, -3.5717,\n                       2.5467, -2.2614,  5.3509, -2.2308, -4.5076, -3.8440, -2.9156,  1.2132,\n                      -5.1992,  0.9595, -3.5860, -1.9827, -3.7129, -1.0678,  0.9404, -0.7915,\n                      -3.6882, -3.3188, -2.3230,  3.5349,  0.9785,  2.0311, -2.7098,  0.1130,\n                      -0.6678, -5.4453,  0.0523, -4.4584,  0.2421, -2.2760,  1.6215, -2.3675,\n                      -2.0393,  4.7286, -3.3943,  3.7118,  0.2685, -2.7028, -3.9117, -3.5675,\n                       5.0565, -3.9848,  3.3326, -3.9669, -2.8907, -4.7943, -3.2638,  0.6801])),\n             ('layer3.1.bn2.running_var',\n              tensor([17.0037, 16.2853, 26.8936, 37.9678, 16.6755, 14.2498, 40.7828, 13.4091,\n                      11.1553, 39.1850, 34.5267, 25.0742, 25.4122, 13.3609, 14.0069, 26.9097,\n                      29.9130, 37.1774, 10.3695, 14.4953, 13.5450, 21.1392, 17.6570, 17.6045,\n                      25.7235, 19.0700, 13.0733, 53.6227, 16.6272, 18.8117, 11.9847, 25.5427,\n                      13.5159, 33.6483, 36.1167, 13.4032, 12.9377, 19.1114,  9.0827, 13.3441,\n                      41.5403, 48.0277, 30.7663, 12.1845, 46.8459, 15.0467, 51.7182, 39.2545,\n                      19.6488, 11.3381, 42.3745, 12.5279, 14.4584, 13.1571, 15.5394, 13.5871,\n                      17.3326, 30.6805, 12.4351, 23.2334, 65.5010, 20.7570, 43.5756, 12.2224,\n                      17.7335, 12.4292, 15.7045, 34.7126, 21.6116, 24.4399, 13.4385, 14.7778,\n                      15.8690, 47.0665, 17.3089, 15.8919, 21.5804, 14.1332, 34.9931, 34.4304,\n                       9.2434, 32.3873, 34.7672, 18.3665, 24.0293, 28.0810, 11.3359, 43.9484,\n                      12.8491,  8.9578, 10.5357, 47.9205, 32.9043, 25.1721, 13.5693, 41.2487,\n                      21.2377, 44.4655, 12.4280, 18.3614, 20.5000, 28.0951, 22.4324, 17.6374,\n                      12.5301, 16.9540, 13.0333, 16.4857, 12.3423, 53.6823, 93.3958, 41.0905,\n                      21.9578, 25.2868, 14.9638, 53.4813, 36.5495, 26.5933, 18.5767, 19.4451,\n                      33.3975, 27.3668, 15.3831, 12.7697, 38.2256, 48.4058, 17.5043, 22.0023,\n                      68.5825, 36.2259, 16.5710, 30.1721, 27.5257, 31.3880, 19.4019, 63.8900,\n                      11.7870, 13.0430, 65.6536, 16.0448, 16.3474, 35.9520, 15.6651, 43.8523,\n                      45.5144, 17.8503, 12.1965, 29.0614, 25.9202, 15.4187, 50.6225, 14.1119,\n                      45.2982, 14.1674, 13.0926, 76.5071, 28.9251, 17.9750, 49.0346, 10.3150,\n                      15.4101, 16.6317, 25.0693, 26.2184, 15.6929, 15.5033, 28.2417, 10.6517,\n                      45.8098, 23.0355, 65.1636, 20.1486, 23.7356, 19.2035, 24.6210, 33.4551,\n                      17.0307, 14.6748, 27.8284, 12.5514, 15.9752, 14.1325, 41.8067, 14.0914,\n                      13.6826, 26.3514, 16.0439, 14.9042, 65.5975, 19.6258, 11.6471, 16.5561,\n                      11.4643, 11.4252, 13.0222, 20.4414, 86.2853, 47.6289, 17.5446, 26.6366,\n                      21.1629, 14.8749, 22.9096, 22.3080, 14.9089, 49.1831, 15.2859, 15.3175,\n                      18.1262, 13.4668, 23.0553, 13.7169, 56.4970, 16.4858, 13.0606, 32.1200,\n                      15.4396, 41.8675, 14.2911, 22.9610, 46.0354, 12.0716, 15.4857, 10.7790,\n                      18.2589, 14.9642, 12.8372, 13.1287, 31.1204, 39.7117, 58.6584, 12.3918,\n                      10.8697, 36.1911, 15.6942, 15.2308, 10.9355, 14.2290, 15.3546, 37.3144,\n                      37.5267, 23.5260, 18.5708, 31.6553, 35.7173, 16.0532, 23.4270, 29.4917,\n                      20.9553, 16.9053, 26.3521, 22.9924, 42.0637, 72.7125, 42.3701, 17.3299])),\n             ('layer3.1.bn2.num_batches_tracked', tensor(14770)),\n             ('layer4.0.conv1.weight',\n              tensor([[[[-0.0614, -0.0745,  0.1063],\n                        [-0.1182, -0.2657,  0.0515],\n                        [-0.1199,  0.1044, -0.1744]],\n              \n                       [[-0.2314,  0.0083,  0.1500],\n                        [-0.0782,  0.0499,  0.0460],\n                        [ 0.0382,  0.0226, -0.0012]],\n              \n                       [[-0.0088,  0.1084,  0.1153],\n                        [ 0.1721,  0.0975,  0.1150],\n                        [ 0.2439,  0.2689,  0.2657]],\n              \n                       ...,\n              \n                       [[-0.2868, -0.0387,  0.2131],\n                        [-0.0635, -0.1142, -0.1672],\n                        [-0.0088, -0.0135, -0.0518]],\n              \n                       [[-0.2086, -0.1668, -0.1494],\n                        [ 0.2014, -0.0235,  0.0173],\n                        [ 0.1099,  0.0142, -0.0228]],\n              \n                       [[ 0.0986,  0.1574,  0.2102],\n                        [-0.0615,  0.0050,  0.0471],\n                        [-0.0589,  0.0044,  0.0600]]],\n              \n              \n                      [[[-0.0624,  0.1167, -0.1435],\n                        [-0.1460,  0.0137, -0.0112],\n                        [-0.0849, -0.1127, -0.0727]],\n              \n                       [[ 0.1611, -0.0659,  0.1563],\n                        [-0.0395, -0.0948,  0.1056],\n                        [-0.1594, -0.1572,  0.0203]],\n              \n                       [[-0.0699,  0.0757,  0.0656],\n                        [-0.2141, -0.0442,  0.0164],\n                        [-0.0467,  0.0886,  0.1135]],\n              \n                       ...,\n              \n                       [[ 0.0268,  0.0612, -0.0213],\n                        [ 0.0804,  0.0759,  0.1307],\n                        [-0.0885,  0.0234, -0.0241]],\n              \n                       [[-0.1139, -0.1732, -0.1309],\n                        [-0.1399, -0.0285, -0.1297],\n                        [-0.1491, -0.1196, -0.0869]],\n              \n                       [[-0.0357, -0.0955, -0.0272],\n                        [-0.1315, -0.2044,  0.1473],\n                        [ 0.2137, -0.0438,  0.1294]]],\n              \n              \n                      [[[-0.1092, -0.1936, -0.0652],\n                        [-0.1022, -0.2223, -0.1216],\n                        [ 0.1113, -0.0703, -0.0129]],\n              \n                       [[ 0.2142,  0.1042, -0.2164],\n                        [ 0.2202,  0.1047,  0.0537],\n                        [-0.0903,  0.0095,  0.0758]],\n              \n                       [[ 0.1382,  0.2057,  0.0450],\n                        [ 0.1023, -0.0047,  0.1159],\n                        [ 0.0060, -0.0485, -0.1272]],\n              \n                       ...,\n              \n                       [[-0.0348,  0.0049, -0.0381],\n                        [ 0.0104,  0.0102, -0.0240],\n                        [ 0.0610,  0.0765,  0.1821]],\n              \n                       [[-0.0776,  0.1082,  0.0903],\n                        [-0.0068,  0.2164,  0.0232],\n                        [-0.1640, -0.0453,  0.0676]],\n              \n                       [[ 0.1623, -0.0335,  0.0056],\n                        [-0.0085, -0.0268,  0.0723],\n                        [ 0.0557, -0.1212, -0.0921]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0699, -0.0390,  0.1083],\n                        [ 0.0637,  0.0208,  0.1119],\n                        [ 0.1763,  0.0466,  0.1655]],\n              \n                       [[ 0.0825, -0.2523, -0.1065],\n                        [-0.1685, -0.1508, -0.2603],\n                        [-0.1286, -0.2182, -0.0630]],\n              \n                       [[-0.0352,  0.0456,  0.0972],\n                        [ 0.1113,  0.1025,  0.0569],\n                        [ 0.1791,  0.1475, -0.0407]],\n              \n                       ...,\n              \n                       [[ 0.0764,  0.0321, -0.1791],\n                        [ 0.1006, -0.0416, -0.0936],\n                        [ 0.0391, -0.1434, -0.3639]],\n              \n                       [[-0.1265,  0.1311,  0.1199],\n                        [ 0.0217,  0.1029,  0.0218],\n                        [ 0.2846,  0.2019,  0.2002]],\n              \n                       [[ 0.0975, -0.1454, -0.1324],\n                        [-0.0288, -0.2109, -0.0971],\n                        [ 0.1059, -0.1844, -0.0737]]],\n              \n              \n                      [[[-0.1813, -0.0846,  0.1080],\n                        [ 0.0073, -0.1617, -0.0073],\n                        [-0.0646,  0.0330, -0.0486]],\n              \n                       [[ 0.0662,  0.0751,  0.1952],\n                        [ 0.0362, -0.0261,  0.0565],\n                        [ 0.1493,  0.0805,  0.1852]],\n              \n                       [[-0.1128, -0.0254, -0.0091],\n                        [-0.0985,  0.1162,  0.1513],\n                        [-0.0901,  0.0812, -0.0005]],\n              \n                       ...,\n              \n                       [[-0.1971,  0.1424, -0.1143],\n                        [ 0.0821,  0.1262, -0.0745],\n                        [-0.2044, -0.0871, -0.2621]],\n              \n                       [[-0.0686, -0.0544,  0.1509],\n                        [-0.1291, -0.0356,  0.1246],\n                        [-0.1688, -0.0570, -0.0297]],\n              \n                       [[ 0.3352, -0.0013,  0.1343],\n                        [ 0.1648, -0.0871,  0.0130],\n                        [-0.1302, -0.0561, -0.1216]]],\n              \n              \n                      [[[ 0.0793, -0.0242,  0.0698],\n                        [-0.1036,  0.0240, -0.1184],\n                        [-0.0099, -0.1659,  0.0668]],\n              \n                       [[ 0.0434, -0.0746, -0.0702],\n                        [ 0.0478,  0.1278, -0.0084],\n                        [ 0.0020,  0.1268, -0.0323]],\n              \n                       [[ 0.0147,  0.0652,  0.1502],\n                        [ 0.1684,  0.2001,  0.1614],\n                        [ 0.3208,  0.1048,  0.0868]],\n              \n                       ...,\n              \n                       [[ 0.0608, -0.0535,  0.0264],\n                        [ 0.1198,  0.1435,  0.0292],\n                        [ 0.1110,  0.0716,  0.0812]],\n              \n                       [[ 0.0284,  0.1865,  0.0815],\n                        [-0.0300,  0.1900,  0.0391],\n                        [ 0.2145,  0.1203,  0.1483]],\n              \n                       [[-0.0944, -0.3402, -0.3220],\n                        [ 0.0523, -0.1885, -0.1618],\n                        [-0.0806,  0.1338, -0.0797]]]])),\n             ('layer4.0.bn1.weight',\n              tensor([1.0830, 0.4976, 0.6705, 0.7508, 0.8898, 0.7427, 0.7879, 0.3586, 0.7535,\n                      0.7507, 0.6147, 0.6477, 0.8237, 0.7971, 0.6444, 0.8736, 0.3305, 0.7721,\n                      0.6972, 0.9128, 0.6894, 0.4704, 0.7545, 0.7342, 0.4698, 0.6686, 0.7147,\n                      0.5462, 0.6942, 0.6774, 0.8986, 0.8322, 0.8308, 0.8774, 0.3087, 0.7006,\n                      0.9705, 0.6807, 0.7664, 0.6731, 0.6768, 0.4282, 0.7588, 0.9007, 0.7810,\n                      0.4650, 0.7671, 0.7327, 0.4833, 0.7679, 0.8211, 0.9486, 0.7270, 0.8155,\n                      0.6959, 0.8059, 0.4196, 0.8533, 0.8492, 0.7160, 0.3308, 0.8966, 0.7942,\n                      0.4422, 0.7691, 0.7419, 0.7401, 0.6775, 0.7721, 0.8264, 0.9241, 0.9408,\n                      0.7239, 0.3618, 0.7779, 0.7013, 0.7753, 0.7048, 0.6493, 0.7749, 0.7851,\n                      1.0086, 0.7660, 0.7320, 0.6971, 0.8698, 0.7058, 0.7450, 0.4040, 0.4624,\n                      0.7763, 0.4212, 0.8030, 0.8011, 0.3272, 0.4839, 0.7005, 0.7510, 0.6668,\n                      0.8070, 0.7699, 0.6770, 0.7474, 0.7957, 0.7150, 0.7954, 0.3280, 0.7171,\n                      0.4624, 0.4699, 0.5667, 0.7361, 0.6725, 0.8949, 0.5687, 0.3536, 0.7486,\n                      0.5454, 0.5371, 0.8287, 0.7139, 0.6137, 0.7484, 0.6118, 0.7072, 0.4055,\n                      0.6133, 0.6748, 0.7108, 0.9991, 0.4599, 0.8429, 0.7032, 1.0190, 0.5478,\n                      0.7647, 0.8384, 0.7274, 0.5277, 0.4250, 0.2785, 0.6183, 0.8346, 0.3360,\n                      0.4266, 0.7576, 0.6601, 0.5029, 0.6581, 0.6544, 0.6627, 0.4437, 0.8137,\n                      0.9574, 0.5467, 0.4084, 0.8427, 0.5422, 1.0594, 0.7541, 0.4230, 0.7277,\n                      0.7577, 0.4599, 1.1545, 0.9182, 0.7724, 0.8651, 0.6477, 0.6238, 0.7038,\n                      1.0135, 0.6404, 0.3387, 0.3587, 0.3437, 0.7649, 0.8536, 0.3093, 0.8526,\n                      0.5843, 0.3223, 0.7527, 0.7719, 0.8212, 0.2909, 0.3238, 0.8349, 0.3583,\n                      0.3350, 0.4166, 0.6559, 0.6706, 0.4250, 0.9705, 0.7836, 0.8173, 0.5265,\n                      0.7376, 0.9390, 0.3003, 0.7290, 0.8710, 0.7314, 0.4750, 1.0386, 0.2206,\n                      0.6615, 0.3540, 0.7422, 0.9125, 0.5989, 0.8059, 0.8577, 0.4652, 0.6008,\n                      0.7569, 0.7500, 0.4277, 0.4917, 0.8339, 0.7255, 0.8506, 1.0144, 0.6679,\n                      0.7626, 0.6579, 0.8445, 0.4553, 0.9179, 0.3075, 0.5632, 0.7077, 0.6595,\n                      0.3110, 0.6748, 0.6437, 0.4510, 0.6665, 0.7568, 0.5345, 0.8016, 0.6860,\n                      0.3910, 0.9016, 0.4762, 0.7673, 0.6784, 0.7955, 0.4425, 0.4812, 0.6793,\n                      0.6935, 0.5447, 0.6809, 0.5414, 0.8476, 0.7039, 0.7667, 0.4900, 0.8175,\n                      1.0289, 0.7035, 0.7753, 0.5518, 0.7244, 0.9361, 0.7442, 0.8443, 0.7274,\n                      0.6965, 0.8173, 0.7218, 0.7411, 0.7428, 0.7527, 0.7452, 0.6871, 0.9064,\n                      0.4366, 0.6286, 0.6856, 0.3334, 0.4614, 0.7602, 0.7420, 0.7511, 0.4587,\n                      0.8127, 0.7801, 0.3509, 0.7999, 0.7458, 0.7174, 0.2712, 0.7433, 0.6370,\n                      0.6835, 0.7547, 0.8506, 0.6771, 0.7831, 0.8563, 0.3072, 0.6363, 0.7753,\n                      0.3556, 0.3181, 0.8564, 0.8953, 0.7190, 0.9497, 0.4012, 0.8910, 0.8366,\n                      0.7106, 0.9991, 0.5900, 0.4624, 0.6726, 0.8176, 0.3768, 0.3835, 0.8949,\n                      0.6810, 0.3017, 0.8898, 0.5027, 0.6981, 0.6322, 0.8033, 0.3215, 0.2977,\n                      0.7408, 0.8535, 0.7717, 0.5686, 0.3980, 0.7532, 0.7144, 0.2879, 0.8292,\n                      0.9230, 0.8708, 0.8711, 0.9602, 0.6199, 0.4581, 0.6988, 0.5615, 0.3949,\n                      0.4260, 0.4636, 0.7329, 0.7141, 0.8875, 0.4650, 0.6806, 0.5411, 0.4395,\n                      0.7460, 0.7477, 0.7333, 0.7105, 0.4052, 0.9866, 0.8405, 0.3281, 0.8169,\n                      0.4851, 0.6081, 0.4708, 0.6061, 0.7046, 0.3233, 0.6811, 0.2932, 0.5803,\n                      0.5924, 0.3337, 0.5776, 0.8164, 0.8278, 0.6616, 0.6776, 0.7157, 0.7224,\n                      0.7397, 0.5028, 0.7469, 0.6174, 0.7125, 0.6059, 0.6987, 0.6955, 0.6754,\n                      0.6205, 0.9868, 0.9082, 0.8055, 0.7054, 0.8538, 1.0875, 0.7408, 0.9058,\n                      1.1779, 0.9224, 0.7392, 0.7307, 0.6856, 0.7745, 0.7307, 0.7244, 0.8941,\n                      0.7892, 0.2871, 0.6718, 0.8026, 0.3067, 0.3583, 0.3984, 0.6860, 0.7649,\n                      0.5397, 0.6244, 0.5952, 0.7343, 0.6704, 0.8046, 0.3372, 0.4923, 0.4454,\n                      0.6976, 0.6749, 0.4313, 0.7620, 0.8432, 0.7392, 0.3952, 0.7043, 0.8829,\n                      0.3695, 0.7147, 0.3374, 0.7634, 0.8470, 0.8399, 0.8384, 0.3983, 0.7778,\n                      0.6544, 0.7544, 0.4769, 0.9177, 0.5291, 0.7293, 0.4461, 0.4273, 0.4943,\n                      0.3920, 0.4248, 0.8212, 0.8480, 0.7491, 0.6825, 0.7488, 0.8701, 0.8049,\n                      0.8185, 0.8275, 0.7210, 0.8870, 0.9675, 0.4490, 0.5289, 0.8478, 0.6835,\n                      0.7616, 0.7612, 0.9292, 0.5024, 0.6847, 0.6231, 0.4057, 0.4698, 0.8896,\n                      0.7701, 0.5532, 0.4121, 1.1826, 0.5989, 0.9853, 0.8393, 0.8351, 0.2908,\n                      0.7042, 0.7653, 0.3985, 0.3850, 0.8069, 1.0141, 0.8148, 0.2442, 0.7529,\n                      0.8582, 0.8718, 0.7271, 0.6987, 0.3639, 0.4629, 0.7032, 0.6438])),\n             ('layer4.0.bn1.bias',\n              tensor([-0.3779, -0.5381, -0.4205, -0.4809, -0.4475, -0.4039, -0.4382, -0.4104,\n                      -0.5546, -0.3896, -0.4980, -0.4360, -0.5017, -0.4521, -0.3057, -0.3781,\n                      -0.3223, -0.4455, -0.4517, -0.3932, -0.4759, -0.4244, -0.5617, -0.2993,\n                      -0.4387, -0.3015, -0.4072, -0.5670, -0.3582, -0.3288, -0.5503, -0.5565,\n                      -0.4709, -0.4607, -0.3938, -0.4771, -0.5023, -0.3338, -0.4434, -0.4489,\n                      -0.3690, -0.4017, -0.4712, -0.4905, -0.5906, -0.3876, -0.5215, -0.4380,\n                      -0.4875, -0.3872, -0.5488, -0.3430, -0.3660, -0.4870, -0.4363, -0.3351,\n                      -0.3904, -0.4017, -0.5002, -0.3774, -0.4754, -0.3664, -0.4885, -0.4381,\n                      -0.4425, -0.3477, -0.4928, -0.4790, -0.5390, -0.4755, -0.5656, -0.5133,\n                      -0.3678, -0.3612, -0.5077, -0.3381, -0.4079, -0.4679, -0.2342, -0.4046,\n                      -0.4839, -0.4157, -0.5974, -0.4317, -0.5139, -0.4970, -0.4464, -0.4111,\n                      -0.5386, -0.4632, -0.4828, -0.1857, -0.5342, -0.5036, -0.3496, -0.4803,\n                      -0.3480, -0.5934, -0.3919, -0.5661, -0.3559, -0.3544, -0.4539, -0.5693,\n                      -0.3038, -0.5161, -0.3822, -0.3921, -0.5040, -0.4100, -0.2256, -0.3535,\n                      -0.2086, -0.4459, -0.5300, -0.3545, -0.4684, -0.5458, -0.4915, -0.5221,\n                      -0.3721, -0.3230, -0.4699, -0.5316, -0.4114, -0.4841, -0.4762, -0.5438,\n                      -0.4224, -0.5130, -0.1640, -0.5065, -0.4871, -0.3738, -0.5522, -0.4650,\n                      -0.5619, -0.4174, -0.2218, -0.3557, -0.3599, -0.5462, -0.4821, -0.3729,\n                      -0.3863, -0.4068, -0.5146, -0.5500, -0.4955, -0.3666, -0.3330, -0.4927,\n                      -0.5269, -0.5092, -0.5492, -0.3836, -0.5590, -0.5424, -0.3784, -0.4088,\n                      -0.5197, -0.6207, -0.4979, -0.5555, -0.3541, -0.4278, -0.4151, -0.3584,\n                      -0.3831, -0.5159, -0.3737, -0.4455, -0.4557, -0.4158, -0.4375, -0.4379,\n                      -0.4507, -0.5645, -0.3865, -0.4101, -0.4752, -0.2995, -0.4257, -0.4935,\n                      -0.3406, -0.4742, -0.3150, -0.4602, -0.3203, -0.3548, -0.4349, -0.3838,\n                      -0.3761, -0.4537, -0.6067, -0.5104, -0.4368, -0.5299, -0.4012, -0.3888,\n                      -0.4423, -0.3233, -0.4217, -0.4014, -0.3758, -0.3831, -0.2976, -0.3691,\n                      -0.4061, -0.4784, -0.4903, -0.2683, -0.4524, -0.4115, -0.4093, -0.5572,\n                      -0.4790, -0.4779, -0.3837, -0.5055, -0.4928, -0.5101, -0.3563, -0.5076,\n                      -0.4042, -0.3263, -0.3590, -0.4320, -0.4829, -0.5006, -0.3680, -0.3934,\n                      -0.4870, -0.4967, -0.3899, -0.5614, -0.4151, -0.5102, -0.3883, -0.3783,\n                      -0.5303, -0.4609, -0.3628, -0.3966, -0.3567, -0.4459, -0.4960, -0.3910,\n                      -0.5142, -0.4283, -0.5546, -0.4671, -0.3267, -0.4055, -0.3501, -0.3600,\n                      -0.4445, -0.4946, -0.4375, -0.4977, -0.5071, -0.4263, -0.3286, -0.4952,\n                      -0.5413, -0.4220, -0.4359, -0.4631, -0.3469, -0.4818, -0.4732, -0.4309,\n                      -0.4043, -0.4927, -0.5122, -0.4419, -0.5132, -0.4841, -0.4000, -0.4269,\n                      -0.3965, -0.4884, -0.4852, -0.5510, -0.4215, -0.3845, -0.4558, -0.5039,\n                      -0.4804, -0.3356, -0.3026, -0.5685, -0.4579, -0.4310, -0.3155, -0.4719,\n                      -0.3666, -0.3740, -0.4694, -0.5304, -0.4858, -0.5486, -0.5911, -0.2907,\n                      -0.3254, -0.5084, -0.4327, -0.3612, -0.5051, -0.5408, -0.4843, -0.5196,\n                      -0.3705, -0.3267, -0.4536, -0.4581, -0.4241, -0.5153, -0.3196, -0.3788,\n                      -0.4628, -0.4106, -0.4355, -0.4033, -0.3328, -0.4604, -0.4118, -0.3536,\n                      -0.2601, -0.5004, -0.5424, -0.4173, -0.4512, -0.2983, -0.4662, -0.4655,\n                      -0.5169, -0.4595, -0.4208, -0.4899, -0.4449, -0.5027, -0.5726, -0.3072,\n                      -0.4972, -0.5455, -0.5418, -0.3783, -0.5208, -0.4834, -0.3467, -0.4853,\n                      -0.5315, -0.3808, -0.3698, -0.4498, -0.4463, -0.4086, -0.4648, -0.3787,\n                      -0.4658, -0.4945, -0.4019, -0.4122, -0.4432, -0.2391, -0.5336, -0.4430,\n                      -0.4201, -0.4993, -0.2309, -0.5292, -0.6337, -0.2389, -0.3239, -0.5252,\n                      -0.3837, -0.3675, -0.4569, -0.3055, -0.3536, -0.4562, -0.5238, -0.4664,\n                      -0.4509, -0.3085, -0.4494, -0.4158, -0.5245, -0.4914, -0.2981, -0.4539,\n                      -0.0955, -0.4974, -0.4743, -0.4228, -0.5704, -0.6350, -0.5519, -0.3770,\n                      -0.4356, -0.3560, -0.4211, -0.5177, -0.4666, -0.2981, -0.5863, -0.4966,\n                      -0.4826, -0.5257, -0.5930, -0.4143, -0.3533, -0.4569, -0.4796, -0.3610,\n                      -0.3598, -0.6074, -0.3721, -0.3323, -0.4323, -0.4288, -0.2843, -0.5029,\n                      -0.5024, -0.5690, -0.4920, -0.5344, -0.5143, -0.3940, -0.4201, -0.4811,\n                      -0.4488, -0.4001, -0.4737, -0.3504, -0.5522, -0.3852, -0.4512, -0.4697,\n                      -0.5339, -0.4697, -0.3551, -0.3894, -0.4908, -0.4562, -0.4860, -0.3700,\n                      -0.4021, -0.4146, -0.3356, -0.3715, -0.5314, -0.4960, -0.4277, -0.3489,\n                      -0.3841, -0.3901, -0.5086, -0.4326, -0.4416, -0.4922, -0.4428, -0.6002,\n                      -0.4778, -0.3060, -0.4280, -0.4515, -0.4918, -0.5685, -0.3177, -0.4439,\n                      -0.5838, -0.3492, -0.4481, -0.5484, -0.3648, -0.4068, -0.5347, -0.5032,\n                      -0.3934, -0.4123, -0.3868, -0.3662, -0.4032, -0.4319, -0.4335, -0.4944,\n                      -0.4538, -0.4859, -0.4983, -0.4466, -0.4335, -0.4703, -0.4293, -0.3903,\n                      -0.4349, -0.5259, -0.5022, -0.4686, -0.5285, -0.3704, -0.2758, -0.3779,\n                      -0.3892, -0.2352, -0.4521, -0.3942, -0.3787, -0.3152, -0.3640, -0.3508])),\n             ('layer4.0.bn1.running_mean',\n              tensor([ -2.8170,   0.3493, -14.2190,  -7.4583,  -7.6171, -11.7039, -13.7786,\n                      -17.1934,  -8.6862, -19.1030,  -9.1739, -10.0802,  -4.0166,  -5.4788,\n                      -13.2812, -10.9012, -10.6388,  -6.3385,  -2.3940, -10.7024,  -0.5988,\n                      -15.0757,   1.0217, -13.1721,  -1.4903, -11.8913, -20.4354,  -7.5908,\n                      -23.9470, -15.2462,  -8.5440,  -2.9120,  -6.6848, -11.5263, -15.7035,\n                      -14.5993,  -2.7342, -14.8682,  -9.9633, -15.6365, -11.9321,  -5.7556,\n                      -12.7282,  -5.2055,  -2.7376,  -4.8280,  -4.7505,  -8.5271,  -6.7735,\n                       -8.2883,  -9.0034,  -7.4373,  -9.2029,  -4.7329,   6.6198, -14.0190,\n                       -8.6956,  -8.6951,   0.3723, -21.5734,   3.2643, -13.7293, -14.2618,\n                      -11.5659, -15.3680,  -9.6562, -14.4490, -13.6528,  -8.4776,  -9.6235,\n                       -9.5818, -11.7462, -16.4081,  -3.3869,  -4.8817, -10.2025,  -9.4084,\n                       -4.7103, -16.9765, -15.7666,  -6.5474,  -6.0714, -10.5260, -17.2637,\n                       -6.1045,  -0.9145, -18.7875,  -6.6107,  -5.3090,  -0.6426,  -8.7467,\n                      -17.9496,  -7.2040,  -8.8517,   4.3430,  -5.8206,  -8.4245,  -7.7904,\n                      -13.0324,  -4.7104, -13.5317, -18.7644,   3.9669,  -3.2182, -13.9871,\n                      -10.4097,   4.3562, -10.5381,  -5.6758,  -7.6048, -12.8174, -13.3526,\n                      -10.5133, -10.9084,   4.3672,  -9.3046,   1.7912,  -3.3461,   9.8162,\n                       -7.1821, -16.2823,  -6.4537, -16.4799,  -1.2799,  -9.5104,  -2.1530,\n                       -0.0951,  -8.8004,  -5.3762,  -6.1846, -22.3620,  -6.9979,   0.6510,\n                       -9.4558,   3.1925, -12.3712,  -8.7859, -11.4456, -21.9911,  -7.3614,\n                        9.7496,  -7.5347,  -8.1815,  -6.3643,  -1.9107, -13.9015,  -0.4097,\n                        0.0280,  -2.7603, -11.8326, -16.7251, -12.5926, -15.9331, -15.5105,\n                        7.5542,  -2.8716,  -6.8158,  11.5714,  -7.0263,  -7.7774,  14.0597,\n                        0.1119,  -9.9119,   0.2970, -11.5096,  -7.7574,  -7.9301,  -8.5474,\n                      -15.7931, -11.0356, -14.7524, -12.3226,  -8.0885, -13.5500,  -8.4098,\n                        1.6390, -16.4462, -10.3870,  -0.3123,  -8.4272,   3.6626,  -8.3874,\n                      -20.5181,  -7.6605, -11.5516,  13.9640, -17.9072,  -4.4841,  -8.3311,\n                      -18.2808,   5.4102, -11.6663, -13.0921,   8.3043,  -9.3947, -14.7418,\n                       -5.2053,  -5.1883, -10.7834, -11.3440, -21.6315, -13.0874, -11.0759,\n                      -10.8135,  -1.0691, -10.0055,   4.4383, -12.6235,   2.7044,  -5.9048,\n                       -9.9580, -15.4046,  -7.9437,  -4.1448,   2.6495,  14.0998,  -1.2587,\n                       -9.5558,  -7.9267,  -8.9345,  -8.2678,  -7.2199, -14.9289,  -8.2978,\n                       -3.1660, -11.8900,  -6.3503,  -8.6542,  -9.7100,   0.4762,   5.6614,\n                       -5.0594,  -6.2834, -17.2787, -12.7073, -11.1091,  -3.4221,   5.7190,\n                      -16.8994,   0.5314,   1.5055,  -2.5202, -14.6965, -11.0041,  -8.6448,\n                       -3.2417,  -8.7850, -14.4861,  -5.0568,   3.4263,  23.2184, -14.2597,\n                       -8.2872,  -0.7014, -10.1741, -13.8638,  -7.9033, -12.3663, -12.8755,\n                       -3.0935,  -8.3624, -13.1113, -10.9882, -13.0026,  -0.6368, -11.2774,\n                       -7.1011, -14.7076, -15.1414, -17.4351, -13.4055,  -7.9936, -11.9697,\n                      -14.6244,  -9.0337, -13.0219,   0.8192,  -7.5577, -14.4650,  -4.7310,\n                      -16.4131,  -6.4075,  18.4831,   6.5890, -14.2958, -18.8433, -14.1506,\n                       -5.5782,  -0.1693, -19.5076, -16.5424,  -9.2337,  -8.8955, -10.7125,\n                       -3.3177, -11.5696, -20.7210,  -8.6667, -10.1283,  -0.7157,   3.6518,\n                      -11.7556,  -3.5498, -12.2981, -13.3085, -10.9052,  -4.8823, -27.9667,\n                      -11.5112,   1.6401,  -2.3610,  -6.3607,  -9.9573,  -2.5181,  -3.0694,\n                        6.6536,  -5.6587,  -5.3073, -10.2935,  -5.4996, -11.2816,  -9.1417,\n                        4.2757, -14.4098, -16.3550,  -7.5395, -10.4732, -13.2563, -15.6896,\n                       -7.9957,  -5.4539,  -1.7045,   6.5614, -10.9767,  -5.1328, -16.3548,\n                        1.3526,  -4.1690,  -5.1994, -10.6393,   8.6968,  -5.2661,  -5.9399,\n                      -15.5482, -12.3783, -13.7950,  -4.9238, -13.8154,  -9.3466, -13.7365,\n                      -13.9229,   5.5787,   1.7359, -12.0825,  -7.2886,  -6.6448,   3.0977,\n                      -15.0184,  -5.8837,  -6.7610,  -5.1636, -11.9641, -16.9855, -13.1550,\n                        7.8215, -10.1065,  -7.3276,   5.6316,  -5.0687, -12.5552, -17.7195,\n                       15.2866,  10.7249, -16.9207, -11.7654,  -2.1343,  -5.2499,  -9.2449,\n                       -9.4458, -14.6142,  -3.4967, -14.3899,  -5.8044, -20.3767,  -7.3597,\n                      -15.2007,  -5.8997,  -3.0917,   2.2889, -11.1116, -19.8808, -13.6442,\n                      -18.9706,  -8.2449,  -3.2424, -15.1115,  -6.8309,  -6.7386, -11.9037,\n                       -5.5790,  -8.4903, -14.0128,  -7.0546, -12.8191,   0.5982,  -5.9988,\n                        0.5923, -10.2310,  -9.0541,  -0.8898,   8.8651,  -4.0389,   0.2417,\n                       -7.4537,  -4.3908,   1.4138, -15.1717, -11.6030,   8.0093, -16.8912,\n                       -4.6659,  -3.2298, -25.0749,  -9.2299,   6.6248,  14.0958,  -9.2899,\n                       -9.5463, -10.8190,  -6.9678,  -5.1449,   2.5488, -15.5809,  -6.7828,\n                        5.5796,  -5.6534,  -9.9253, -11.8458,   5.7548, -13.6956, -15.5675,\n                       12.1400,  -5.2810,  -5.0308,  -9.0651,  -9.0622,  -4.8882, -13.8807,\n                      -14.7784, -11.0039,  -7.1854, -12.0134,   0.7929, -12.0472,   2.4928,\n                      -10.8546, -13.3133, -16.0372,   2.1321, -13.0968,  -4.3220,  -6.7015,\n                      -13.6011, -11.3036, -15.1630, -10.6067, -16.1627, -15.7775,  -6.9351,\n                      -16.9822, -13.1668,  -5.0901,  -7.9663,  -6.9642,  -6.1610, -18.1246,\n                      -16.9692, -13.1425,  -7.2014,  -9.0065,  -4.7632,  -6.8202, -11.6036,\n                      -13.9158, -10.2854,  -3.2765, -12.0081,  -9.4918, -12.7270,  -5.5666,\n                       -0.7831,  -4.7526, -10.8077, -14.3548,   1.8162, -10.9971,  -9.8427,\n                        3.7881,   0.6572,  -6.0706, -11.7787,  -7.0503,  -3.6576, -12.5029,\n                       -2.0795, -11.0113,  -7.9089, -11.3605,   3.9524, -15.4527, -16.2019,\n                      -15.0664])),\n             ('layer4.0.bn1.running_var',\n              tensor([176.9987,  72.2225, 218.2410,  90.8297,  86.1482, 139.2177, 100.9471,\n                      141.1977,  74.1661, 109.6154,  85.7094, 239.5565, 110.0102,  79.4403,\n                      327.0452,  86.0766, 109.4963,  98.2724,  88.3296, 270.1660, 109.1402,\n                      101.4726,  79.3458, 320.8705,  88.9908, 271.6825, 182.0488,  79.9732,\n                      253.8745, 234.3289, 110.5096,  84.3597,  90.7749,  93.7199,  91.8940,\n                      100.4128,  91.4771, 170.2373, 153.0251, 199.4154, 209.7701, 111.7554,\n                      112.7104,  82.7313,  68.6296,  75.6956,  96.1784, 101.9981,  93.2157,\n                       95.3515,  71.9337,  95.7660, 155.9924,  86.8646, 122.9247, 183.6559,\n                      134.4319, 177.0359,  77.1709, 221.7073,  89.7829, 151.7769,  92.9344,\n                       99.7240, 128.6878, 180.2123, 171.1580,  98.3656, 109.2295, 100.7612,\n                      116.1660,  90.6716, 204.6767,  84.2838, 102.2227, 200.1614, 146.2423,\n                      114.4549, 151.9806, 127.7345,  77.3219,  80.2119,  98.8337, 304.7369,\n                       92.5866,  81.0635, 120.8635, 160.0974,  77.7552,  91.4722, 109.1832,\n                      447.0985,  79.1083, 105.9753,  90.3866,  82.7066, 182.5995,  72.2600,\n                      186.0683,  86.8189, 142.0645, 212.8756,  93.1035,  78.1252, 188.1327,\n                       80.9354,  97.2880, 252.5982,  86.8751,  95.3722, 289.6496, 142.9408,\n                      134.6013,  84.0657,  84.0164, 130.4602,  90.5921,  78.1833,  96.2811,\n                       99.5048, 139.0309, 192.5037, 107.4449,  83.7336, 149.3389,  88.3631,\n                      105.2847,  77.9067,  86.7310, 122.4246, 389.5359,  86.6280,  96.6867,\n                      104.3611,  88.1823,  95.2583,  90.2465, 102.2196, 268.4799,  92.2261,\n                      101.5254, 108.0437, 100.0761,  81.1016,  81.2362, 107.5089, 183.9369,\n                       80.6300,  83.7425, 133.0755, 207.6823,  89.7500,  90.5708, 122.3680,\n                       88.7967,  92.9073,  94.2777,  68.7807, 136.8469, 111.0152, 101.0618,\n                       72.1694, 148.6960,  84.6507,  96.4562, 106.4792,  94.5101, 108.6985,\n                      368.7336, 102.3351, 264.3031, 173.6361, 117.7829, 107.6278,  81.9122,\n                       91.4383,  97.6869,  99.7901,  91.4606,  96.2549,  91.8334, 109.0541,\n                      269.5192,  90.0167, 248.3636, 122.7463, 112.7323, 128.6867, 108.0624,\n                      198.1572,  78.1190,  97.3905, 318.5702, 105.4308,  90.2998, 109.6502,\n                       79.5667,  70.2402, 157.6520, 126.3894, 106.0764, 123.8028,  86.1231,\n                      133.3584,  99.8645, 117.1845, 104.2989, 132.4206, 103.6257, 156.8975,\n                       94.6367, 157.2486, 103.5839, 105.7682, 139.4287,  93.4344, 204.1695,\n                      101.7389, 110.0728,  75.5542,  85.9012, 163.9786,  97.1014, 121.9873,\n                      223.0837, 124.5346, 134.8365, 114.4307, 103.4141,  92.7640, 115.6958,\n                      107.6167,  83.7135, 122.6262, 115.2517,  94.7660,  89.7360,  88.1181,\n                      325.8983, 168.8566, 105.2768,  68.4175, 155.3056,  81.7435,  85.0762,\n                       83.3588, 100.6860, 310.9301,  79.3717, 112.1435,  95.5943, 133.4983,\n                      231.7145,  76.8253, 192.8173,  85.7744, 158.5731,  89.7364,  92.6873,\n                       91.7958,  72.5632, 102.6463, 190.8816, 101.0506,  87.0329, 245.0405,\n                      102.4157, 111.1361,  96.9794,  96.7906, 105.2411, 103.4089, 161.1522,\n                       89.7802,  93.0422, 106.2368,  90.3035, 117.3798, 130.5555, 105.2320,\n                      259.8525,  70.8121, 105.3931,  99.1613, 153.5096, 182.0273,  91.6999,\n                      135.8994, 151.4226, 225.8360, 162.3829, 138.1995,  86.0734, 136.3205,\n                      136.8594,  92.2304, 264.9523, 238.6781,  86.2698,  94.7951, 144.0684,\n                       94.2461,  91.6660, 113.1984, 196.8258,  87.3756, 124.7299, 151.4355,\n                       70.1056,  81.9343, 103.4256,  81.2317, 109.7567,  82.8535,  92.1499,\n                      108.9697, 132.3555,  79.5148, 103.1589, 178.7210,  88.6352,  96.9906,\n                       95.1068, 155.7658, 272.8590,  89.1254,  96.6134, 102.5594, 140.5633,\n                       98.3352,  79.5321,  95.2087,  87.1422,  92.4316,  79.2106, 211.8193,\n                       86.1239,  94.6081, 101.0620,  79.6822,  90.5590,  84.5624,  83.0423,\n                      170.0004, 148.5481, 101.8410,  86.4753, 104.7136, 131.6035,  97.0379,\n                      147.7157, 115.5689,  77.2923,  96.8782, 125.6273,  84.7318, 104.5871,\n                      260.2551,  75.0945, 118.5660,  77.8417,  90.1466, 163.4932, 176.3573,\n                      114.9743, 115.3631,  88.6642,  97.2676, 126.8719, 100.1156, 247.8029,\n                      110.5202,  70.3730, 184.8453,  96.1828,  76.0549,  90.8302,  98.9876,\n                       76.8247,  97.3776,  88.3370, 193.7529,  74.1402, 192.6938, 107.5630,\n                      271.7114, 153.8063,  85.0769,  86.5716,  80.0693, 256.8594, 126.1144,\n                      143.6398, 105.3909,  82.3067, 163.8515,  81.0072,  99.3891, 116.1313,\n                      135.7104, 168.5758, 197.1351, 111.4741,  83.8610,  70.8896, 150.2867,\n                       70.7429, 187.3169, 102.9096,  83.5324,  93.8930, 108.4373, 268.1720,\n                      161.8838, 102.6775,  88.1671, 176.2200,  81.5096, 119.9411, 108.7166,\n                       86.8336,  97.0118, 346.1610, 120.5798,  72.8836,  88.8317,  82.6482,\n                       79.1960, 123.4386,  79.0949,  85.8922,  87.5811, 144.1593, 167.7470,\n                       92.5460, 117.5913, 133.9254, 114.1417,  72.6321, 129.0112, 115.0916,\n                      112.3632, 113.5376, 125.3625,  96.4309,  72.2629,  70.3979,  92.3891,\n                      112.7384,  85.9195, 192.0962, 100.1225,  77.7247, 163.1606, 104.8177,\n                      256.6675, 153.5582, 110.9449,  72.3469, 109.3238,  98.7863, 101.3663,\n                      145.5670,  85.5917, 137.6402,  81.5450, 143.5761, 193.3911,  82.9219,\n                       96.0102, 245.1778,  87.7692,  81.0284,  98.3384,  76.5316,  75.1441,\n                      195.4463, 149.2241, 102.2254,  83.9914, 126.3445, 176.3471, 180.1204,\n                      137.4955, 109.4149, 133.5324, 116.6867,  86.2986,  86.9087, 106.9946,\n                       93.6703,  97.5019,  91.0957, 115.8836, 108.7226,  78.8352,  78.4878,\n                       81.2619,  85.7180,  79.0261,  86.5185, 114.2927, 118.8099,  86.0817,\n                       94.8178, 108.2729, 107.3157, 206.4520, 100.8293, 137.1081, 123.8857,\n                      178.6049])),\n             ('layer4.0.bn1.num_batches_tracked', tensor(14770)),\n             ('layer4.0.conv2.weight',\n              tensor([[[[ 0.0553,  0.0037, -0.1553],\n                        [ 0.0355, -0.0375, -0.2355],\n                        [-0.1695, -0.1227, -0.1459]],\n              \n                       [[-0.1459, -0.1568, -0.1588],\n                        [-0.1189,  0.0368, -0.1187],\n                        [-0.1357,  0.0220, -0.0593]],\n              \n                       [[ 0.0426, -0.0456, -0.0633],\n                        [ 0.0386, -0.0303, -0.0087],\n                        [-0.0511, -0.3207, -0.1487]],\n              \n                       ...,\n              \n                       [[-0.0455,  0.0266,  0.0583],\n                        [-0.0650, -0.0193,  0.0444],\n                        [-0.2434, -0.1204, -0.1387]],\n              \n                       [[ 0.0939,  0.1964,  0.2204],\n                        [-0.0085, -0.0584,  0.1193],\n                        [ 0.0275, -0.0955,  0.0615]],\n              \n                       [[ 0.1108, -0.0695, -0.0556],\n                        [ 0.0606, -0.0427,  0.1122],\n                        [ 0.0562, -0.1677, -0.1630]]],\n              \n              \n                      [[[-0.0765, -0.1794, -0.0296],\n                        [-0.0259, -0.0354,  0.1546],\n                        [-0.1710, -0.0480,  0.0283]],\n              \n                       [[-0.0472,  0.0282, -0.0704],\n                        [-0.1467, -0.1605,  0.1918],\n                        [ 0.0028,  0.0483, -0.0874]],\n              \n                       [[-0.1020, -0.1661, -0.1069],\n                        [-0.0897, -0.0696, -0.1004],\n                        [ 0.1032, -0.1719, -0.2036]],\n              \n                       ...,\n              \n                       [[ 0.1738,  0.0741,  0.0537],\n                        [ 0.1545, -0.1678, -0.0984],\n                        [ 0.1057, -0.0362, -0.1282]],\n              \n                       [[-0.0122, -0.1497, -0.1452],\n                        [-0.1856, -0.2010, -0.2053],\n                        [-0.1507, -0.2430, -0.1229]],\n              \n                       [[ 0.2021,  0.1120,  0.3766],\n                        [ 0.0136, -0.0620,  0.0430],\n                        [ 0.1428, -0.1165, -0.1215]]],\n              \n              \n                      [[[-0.0068, -0.0113, -0.0718],\n                        [ 0.1396, -0.1265,  0.0481],\n                        [-0.1456, -0.1320,  0.2137]],\n              \n                       [[ 0.2653,  0.2448, -0.1004],\n                        [-0.0467, -0.0960, -0.1600],\n                        [ 0.0078,  0.2669,  0.0434]],\n              \n                       [[-0.2906, -0.2455, -0.2788],\n                        [-0.1553,  0.0609, -0.0461],\n                        [-0.2825, -0.0312,  0.0973]],\n              \n                       ...,\n              \n                       [[-0.0166,  0.1491,  0.0380],\n                        [-0.3353, -0.2368, -0.1502],\n                        [-0.1742, -0.1504, -0.1527]],\n              \n                       [[-0.1248,  0.0624,  0.0579],\n                        [ 0.0209,  0.1230,  0.0287],\n                        [-0.2246, -0.2104,  0.0325]],\n              \n                       [[-0.1951, -0.1032, -0.0608],\n                        [-0.0254,  0.0836, -0.0253],\n                        [ 0.1229,  0.0717,  0.0203]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1515, -0.0288, -0.1425],\n                        [ 0.0930,  0.0035,  0.0102],\n                        [-0.0582,  0.1005,  0.0794]],\n              \n                       [[-0.1450, -0.1103,  0.0560],\n                        [-0.0643, -0.0992, -0.0240],\n                        [ 0.0962, -0.0517, -0.1268]],\n              \n                       [[-0.2004, -0.0426, -0.1464],\n                        [ 0.0317, -0.0495, -0.0290],\n                        [ 0.0420,  0.0718, -0.2298]],\n              \n                       ...,\n              \n                       [[ 0.0010, -0.0289,  0.1509],\n                        [ 0.0230, -0.0352, -0.0047],\n                        [ 0.0374,  0.0073, -0.1148]],\n              \n                       [[ 0.0711,  0.0822, -0.1904],\n                        [-0.0384,  0.1627,  0.0983],\n                        [-0.2107, -0.0185, -0.0814]],\n              \n                       [[-0.1979, -0.1619, -0.1297],\n                        [-0.0888,  0.0582,  0.0793],\n                        [-0.2297, -0.0163,  0.0718]]],\n              \n              \n                      [[[-0.0596,  0.0362, -0.0791],\n                        [-0.0875,  0.0647, -0.0530],\n                        [-0.0762,  0.1501, -0.0891]],\n              \n                       [[ 0.1029,  0.0665,  0.1797],\n                        [ 0.0938,  0.0896,  0.2737],\n                        [ 0.1880, -0.0757, -0.0220]],\n              \n                       [[-0.1484, -0.0250, -0.0195],\n                        [-0.1756,  0.0509, -0.1552],\n                        [-0.1975, -0.1473, -0.2336]],\n              \n                       ...,\n              \n                       [[ 0.0820,  0.0065, -0.0369],\n                        [-0.0703,  0.1517,  0.2946],\n                        [-0.0285,  0.0719,  0.0204]],\n              \n                       [[-0.0577, -0.0652,  0.0467],\n                        [-0.0987, -0.1167, -0.0795],\n                        [-0.2421, -0.0640,  0.0310]],\n              \n                       [[-0.0578,  0.1234,  0.1054],\n                        [-0.0642,  0.0291,  0.1214],\n                        [ 0.2404,  0.0898,  0.2457]]],\n              \n              \n                      [[[-0.3236, -0.1997,  0.0416],\n                        [-0.1926, -0.0146,  0.0142],\n                        [ 0.0016, -0.0448, -0.1593]],\n              \n                       [[ 0.0744, -0.0732, -0.0446],\n                        [-0.0878, -0.0951, -0.1996],\n                        [ 0.0756, -0.0841, -0.1942]],\n              \n                       [[-0.1358, -0.0766, -0.0333],\n                        [ 0.0835, -0.2202, -0.0400],\n                        [-0.0268, -0.1657, -0.0726]],\n              \n                       ...,\n              \n                       [[ 0.0264,  0.0139, -0.0391],\n                        [ 0.0014, -0.1281, -0.1373],\n                        [ 0.0949,  0.0890,  0.1380]],\n              \n                       [[-0.1395,  0.0728, -0.1467],\n                        [-0.0049,  0.0972, -0.1019],\n                        [-0.1372, -0.0069, -0.0431]],\n              \n                       [[ 0.0898, -0.0524, -0.0648],\n                        [-0.1070, -0.1961, -0.1456],\n                        [ 0.0724, -0.0676, -0.2159]]]])),\n             ('layer4.0.bn2.weight',\n              tensor([0.7510, 0.6305, 0.7147, 0.8788, 0.6075, 0.6688, 0.5609, 0.6928, 0.8738,\n                      0.7716, 0.8699, 0.6796, 0.8284, 0.7010, 0.7096, 0.6160, 0.6618, 0.6405,\n                      0.7559, 0.6307, 0.7261, 0.9219, 0.6676, 0.6551, 0.6600, 0.9410, 0.8731,\n                      0.7839, 0.6368, 0.6283, 0.9019, 0.7589, 0.6749, 0.8833, 0.6869, 0.6582,\n                      0.6527, 0.9694, 0.5840, 0.5846, 0.6190, 0.6430, 0.7280, 0.5720, 0.5335,\n                      0.6104, 0.5770, 0.7495, 0.6395, 0.8891, 0.7441, 0.9220, 0.5910, 0.6200,\n                      0.6600, 0.5787, 1.0013, 0.6576, 0.7144, 0.6386, 0.6537, 0.6744, 0.6444,\n                      0.6149, 0.6324, 0.8873, 0.8714, 0.6381, 0.8685, 0.9054, 0.6126, 0.7178,\n                      0.6149, 0.8003, 0.7214, 0.5854, 0.9519, 0.6577, 0.6816, 0.7220, 0.7498,\n                      0.6567, 0.5903, 0.7221, 0.7098, 0.5601, 0.6723, 0.6828, 0.8667, 0.6171,\n                      0.6716, 0.8749, 0.7389, 0.9326, 0.5953, 0.6135, 0.8541, 0.8943, 0.6029,\n                      0.9361, 0.5760, 0.9131, 0.6761, 0.7070, 0.6114, 0.8833, 0.8424, 0.8923,\n                      0.7472, 0.9108, 0.8549, 0.9580, 0.7479, 0.8809, 0.6920, 0.6164, 0.7032,\n                      0.6103, 0.8346, 0.6449, 0.7561, 0.6539, 0.6908, 0.7640, 0.7597, 0.7183,\n                      0.6341, 0.8646, 0.6339, 0.6194, 0.6850, 0.7433, 0.6304, 0.7095, 0.7804,\n                      0.6846, 0.8170, 0.8859, 0.8202, 0.6718, 0.9011, 0.6158, 0.8134, 0.6347,\n                      0.9249, 0.6614, 0.5457, 0.6556, 0.5682, 0.6291, 0.6749, 0.8766, 0.6374,\n                      0.9210, 0.6771, 0.6965, 0.6108, 0.6789, 0.6158, 0.8436, 0.8578, 0.6374,\n                      0.7940, 0.5520, 0.6044, 0.7251, 0.8494, 0.7836, 0.6926, 0.6790, 0.6674,\n                      0.6445, 0.6848, 0.6911, 0.6559, 0.5711, 0.6978, 0.6763, 0.7099, 0.7618,\n                      0.6856, 0.5891, 0.5481, 0.8717, 0.6233, 0.8512, 0.9411, 0.6308, 0.6354,\n                      0.9320, 0.6589, 0.6881, 0.7742, 0.7128, 0.5647, 0.6776, 0.7508, 0.8028,\n                      0.9032, 0.8485, 0.8155, 0.7665, 0.5950, 0.6432, 0.6642, 0.8923, 0.9527,\n                      0.6345, 0.6573, 0.8391, 0.7064, 0.6605, 0.6282, 0.6676, 0.8391, 0.6173,\n                      0.6353, 0.6741, 0.6242, 0.9749, 0.6316, 0.5348, 0.6477, 0.7574, 0.7148,\n                      0.6506, 0.7950, 0.7006, 0.6310, 0.6490, 0.6262, 0.7644, 0.6385, 0.5489,\n                      0.7128, 0.6998, 0.8781, 0.6575, 0.6495, 0.6407, 0.7495, 0.6053, 0.8368,\n                      0.6121, 0.6459, 0.5619, 0.7716, 0.6465, 0.6432, 0.6147, 0.9343, 0.7081,\n                      0.8497, 0.9856, 0.6637, 0.7026, 0.8615, 0.6392, 0.6315, 0.7550, 0.6403,\n                      0.7386, 0.5819, 0.6705, 0.9147, 1.0351, 0.6394, 0.8099, 0.6041, 0.7339,\n                      0.9101, 0.7956, 0.6348, 0.6415, 0.9097, 0.7132, 0.6439, 0.6832, 0.6270,\n                      0.7022, 0.6765, 0.7383, 0.6855, 0.6449, 0.8533, 0.6357, 0.9164, 0.6372,\n                      0.6360, 0.6329, 0.8272, 0.6682, 0.6842, 0.8298, 0.7100, 0.6144, 0.7401,\n                      0.6976, 0.6245, 0.6865, 0.7149, 0.9153, 0.6346, 0.6500, 0.6787, 0.6177,\n                      0.7126, 0.5853, 0.5893, 0.6488, 0.8238, 0.8631, 0.9806, 0.6766, 0.8506,\n                      0.6662, 0.7639, 0.6994, 0.7791, 0.7072, 0.8860, 0.7445, 0.6631, 0.7551,\n                      0.5994, 0.6183, 0.7037, 0.6999, 0.6847, 0.6249, 0.6632, 0.7837, 0.5961,\n                      0.6773, 0.8578, 0.7825, 0.8412, 0.6095, 0.7174, 0.7060, 0.7049, 0.7154,\n                      0.6694, 0.6300, 0.6815, 0.9637, 0.6531, 0.9596, 0.5405, 0.8544, 0.6716,\n                      0.6888, 0.7027, 0.7046, 0.7402, 0.8276, 0.6778, 0.6305, 0.5787, 0.6571,\n                      0.5604, 0.6206, 0.6593, 0.6105, 0.8540, 0.6350, 0.7899, 0.9373, 0.6788,\n                      0.6105, 0.6228, 0.6062, 0.6182, 0.9090, 0.9212, 0.6983, 0.8296, 0.8696,\n                      0.6764, 0.5700, 0.6161, 0.5801, 0.7544, 0.8751, 0.8992, 0.6651, 0.8029,\n                      0.6250, 0.6170, 0.6744, 0.7286, 0.6933, 0.7935, 0.8415, 0.6980, 0.6613,\n                      0.9397, 0.9695, 0.6727, 0.6788, 0.6728, 0.6595, 0.8813, 0.8811, 0.8615,\n                      0.9035, 0.8377, 0.6914, 0.7612, 0.6487, 0.5932, 0.6442, 0.7460, 0.6223,\n                      0.7377, 0.6211, 0.8936, 0.6335, 0.7258, 0.6556, 0.5490, 0.7979, 1.0143,\n                      0.8840, 0.6090, 0.6897, 0.6193, 0.6401, 0.6314, 0.7457, 0.6706, 0.9323,\n                      0.6304, 0.7291, 0.5837, 0.7744, 0.5626, 0.7180, 0.8276, 0.6893, 0.6727,\n                      0.6745, 0.6615, 0.6427, 0.6804, 0.7328, 0.6957, 0.7039, 0.6830, 0.6672,\n                      0.6621, 0.6392, 0.6094, 0.7117, 0.6807, 0.8244, 0.8224, 0.6440, 0.7278,\n                      0.6393, 0.6938, 0.6584, 0.7010, 0.8163, 0.8615, 0.6933, 0.8270, 0.6315,\n                      0.6725, 0.6972, 0.6327, 0.7525, 0.8797, 0.6105, 0.6956, 0.9749, 0.6584,\n                      0.5993, 0.7217, 0.6235, 0.8557, 0.9697, 0.8631, 0.8886, 0.9016, 0.5863,\n                      0.8225, 0.5854, 0.9679, 0.8383, 0.6557, 0.6933, 0.6941, 0.7891, 0.6959,\n                      0.6512, 0.6302, 0.6715, 0.7919, 0.6675, 0.6197, 0.5345, 0.8564, 0.5881,\n                      0.9320, 0.6342, 0.5603, 0.6771, 0.6830, 0.5875, 0.6695, 0.9271])),\n             ('layer4.0.bn2.bias',\n              tensor([-0.4564, -0.4218, -0.4613, -0.4731, -0.3194, -0.4972, -0.3595, -0.4265,\n                      -0.3650, -0.4644, -0.4148, -0.3740, -0.3789, -0.4411, -0.4453, -0.4352,\n                      -0.3793, -0.3687, -0.4601, -0.4049, -0.4617, -0.4022, -0.3812, -0.3176,\n                      -0.4453, -0.4773, -0.4760, -0.4751, -0.3162, -0.4127, -0.4768, -0.4305,\n                      -0.4050, -0.4802, -0.3474, -0.3963, -0.3968, -0.5090, -0.3673, -0.4208,\n                      -0.4315, -0.3251, -0.4339, -0.3469, -0.3548, -0.3713, -0.3267, -0.3926,\n                      -0.3954, -0.5975, -0.4014, -0.4102, -0.3112, -0.2928, -0.3915, -0.4295,\n                      -0.5044, -0.3699, -0.4618, -0.4210, -0.4012, -0.4673, -0.4295, -0.4196,\n                      -0.4693, -0.4627, -0.4568, -0.3909, -0.5007, -0.5133, -0.4307, -0.3837,\n                      -0.3797, -0.3525, -0.2941, -0.4312, -0.4478, -0.3251, -0.3577, -0.4428,\n                      -0.4335, -0.4485, -0.3194, -0.4208, -0.3742, -0.4086, -0.3328, -0.4131,\n                      -0.3994, -0.3596, -0.3530, -0.4506, -0.3913, -0.4855, -0.4288, -0.3452,\n                      -0.3032, -0.4940, -0.3180, -0.4640, -0.3800, -0.4286, -0.4412, -0.4128,\n                      -0.4072, -0.4766, -0.3287, -0.4367, -0.4173, -0.4412, -0.3848, -0.3511,\n                      -0.4920, -0.3790, -0.4109, -0.3383, -0.4633, -0.4169, -0.3342, -0.4408,\n                      -0.3761, -0.3770, -0.3731, -0.4491, -0.4406, -0.4215, -0.3238, -0.4524,\n                      -0.2840, -0.4183, -0.3957, -0.3568, -0.4715, -0.4176, -0.5090, -0.3357,\n                      -0.4112, -0.2518, -0.3851, -0.3448, -0.3224, -0.3867, -0.4258, -0.3697,\n                      -0.4375, -0.3975, -0.3822, -0.3869, -0.4014, -0.3761, -0.3677, -0.4618,\n                      -0.4104, -0.5066, -0.4625, -0.4159, -0.3473, -0.4175, -0.2946, -0.3598,\n                      -0.4046, -0.3912, -0.4442, -0.2818, -0.4664, -0.4243, -0.3982, -0.4467,\n                      -0.4303, -0.4497, -0.4264, -0.3353, -0.4332, -0.4561, -0.4040, -0.3765,\n                      -0.4014, -0.4952, -0.3547, -0.4349, -0.4638, -0.3906, -0.3897, -0.3814,\n                      -0.3766, -0.4488, -0.4264, -0.3920, -0.3656, -0.5696, -0.4338, -0.3725,\n                      -0.4723, -0.3621, -0.3894, -0.3732, -0.3346, -0.3438, -0.4508, -0.4837,\n                      -0.3306, -0.4605, -0.4072, -0.3560, -0.3881, -0.3916, -0.4660, -0.4017,\n                      -0.3588, -0.4394, -0.3892, -0.3348, -0.4270, -0.3824, -0.4309, -0.3777,\n                      -0.4165, -0.4157, -0.4254, -0.4884, -0.3744, -0.3619, -0.3519, -0.3518,\n                      -0.3032, -0.3025, -0.4338, -0.4533, -0.3776, -0.3324, -0.4108, -0.4578,\n                      -0.4127, -0.2744, -0.4713, -0.3062, -0.3900, -0.3481, -0.4195, -0.3491,\n                      -0.3957, -0.2879, -0.3667, -0.3486, -0.3815, -0.3926, -0.5004, -0.3822,\n                      -0.3691, -0.4258, -0.4249, -0.3922, -0.4097, -0.5420, -0.4278, -0.3258,\n                      -0.4345, -0.3774, -0.3516, -0.4029, -0.3844, -0.4077, -0.3200, -0.4117,\n                      -0.4506, -0.6064, -0.3555, -0.4419, -0.3324, -0.4547, -0.3600, -0.4447,\n                      -0.3716, -0.4359, -0.4355, -0.4528, -0.3482, -0.4291, -0.4086, -0.3789,\n                      -0.4467, -0.4592, -0.3620, -0.3554, -0.3448, -0.4212, -0.4409, -0.3492,\n                      -0.3619, -0.3822, -0.3926, -0.3872, -0.4138, -0.3842, -0.3373, -0.3535,\n                      -0.5111, -0.3817, -0.4211, -0.3526, -0.3711, -0.4298, -0.3340, -0.3351,\n                      -0.3768, -0.3941, -0.4524, -0.3070, -0.3463, -0.4426, -0.3546, -0.4071,\n                      -0.5875, -0.4357, -0.4416, -0.4916, -0.4759, -0.4414, -0.3104, -0.3554,\n                      -0.4053, -0.4777, -0.3458, -0.4554, -0.4627, -0.3983, -0.4261, -0.3744,\n                      -0.4284, -0.3470, -0.4758, -0.4217, -0.3706, -0.4286, -0.3507, -0.4750,\n                      -0.4030, -0.3653, -0.3344, -0.4968, -0.4130, -0.4090, -0.3793, -0.4468,\n                      -0.3463, -0.4821, -0.3160, -0.4755, -0.2941, -0.3188, -0.4084, -0.4419,\n                      -0.4140, -0.4296, -0.3839, -0.3750, -0.4590, -0.3369, -0.3998, -0.3551,\n                      -0.3162, -0.4170, -0.3877, -0.4011, -0.3453, -0.3607, -0.2951, -0.5107,\n                      -0.3463, -0.3944, -0.3888, -0.3754, -0.4318, -0.3025, -0.3970, -0.4961,\n                      -0.4891, -0.3780, -0.3854, -0.4062, -0.4369, -0.3337, -0.4105, -0.3846,\n                      -0.4636, -0.3382, -0.2628, -0.4107, -0.3806, -0.4267, -0.5830, -0.4244,\n                      -0.3539, -0.4767, -0.3873, -0.3733, -0.5822, -0.4932, -0.4352, -0.3820,\n                      -0.4180, -0.3978, -0.4892, -0.2988, -0.3492, -0.3720, -0.4582, -0.4122,\n                      -0.4163, -0.4047, -0.3259, -0.3333, -0.4081, -0.4071, -0.4549, -0.4297,\n                      -0.4028, -0.3829, -0.4028, -0.3948, -0.3399, -0.4830, -0.4582, -0.4470,\n                      -0.4113, -0.5077, -0.3545, -0.3867, -0.3940, -0.4203, -0.2813, -0.4889,\n                      -0.3456, -0.3825, -0.3036, -0.4619, -0.3791, -0.3751, -0.3673, -0.4292,\n                      -0.4637, -0.3464, -0.4387, -0.4042, -0.3263, -0.4168, -0.4039, -0.3584,\n                      -0.4264, -0.4230, -0.4333, -0.3707, -0.3568, -0.4031, -0.4271, -0.5004,\n                      -0.3820, -0.3803, -0.3368, -0.4606, -0.4467, -0.3128, -0.3934, -0.3832,\n                      -0.4336, -0.4454, -0.4371, -0.3962, -0.4163, -0.4717, -0.3661, -0.3784,\n                      -0.3663, -0.3381, -0.3491, -0.5424, -0.4125, -0.3587, -0.3813, -0.4393,\n                      -0.4199, -0.4859, -0.3883, -0.3920, -0.4072, -0.3691, -0.4691, -0.4041,\n                      -0.5012, -0.3484, -0.3972, -0.3968, -0.4434, -0.4310, -0.4462, -0.3612,\n                      -0.3908, -0.3391, -0.2095, -0.3854, -0.4484, -0.3621, -0.4003, -0.4037,\n                      -0.4187, -0.4362, -0.3648, -0.3747, -0.4169, -0.4057, -0.3174, -0.4014])),\n             ('layer4.0.bn2.running_mean',\n              tensor([-2.8230, -3.1724, -4.7036, -0.8056, -4.1899, -5.1407, -5.1808, -5.5525,\n                      -7.4163,  1.0856, -2.7495, -4.3670, -4.3480, -3.9970, -5.6404,  0.4430,\n                      -1.5277, -4.0289, -4.8112, -2.5078, -4.6745, -6.0433, -3.0104, -4.6595,\n                      -1.4006, -5.2019, -3.6337, -2.0621, -2.2101, -4.5269, -4.6922, -6.9708,\n                      -4.9169, -3.0309, -3.7746, -1.2610,  1.1410, -3.1018, -6.3657, -3.5146,\n                      -5.1504, -1.8594, -4.3829, -4.8687, -4.4040,  1.0962, -4.5281, -3.2376,\n                      -1.4192, -4.0403, -4.1657, -2.6361, -0.3220, -5.9326,  1.1793, -4.4751,\n                      -3.0220, -6.0819, -3.9446, -4.5080, -2.6621, -7.5311, -4.2991, -3.8398,\n                      -5.3741,  0.4629, -2.2550, -1.5959, -5.1262, -1.3506, -3.6939, -5.0738,\n                      -6.7024, -4.8224, -4.2126,  0.0595, -4.8962, -5.3988, -1.9722, -5.5241,\n                      -1.8940, -4.8402, -2.8008, -3.7189, -5.3571, -3.2194, -2.1614, -0.8036,\n                      -1.0177, -6.8738, -5.0235, -3.2313, -1.7848, -1.5141, -2.7504, -1.8404,\n                      -6.2457, -5.9934, -1.5654, -2.2245, -0.0137, -3.0285, -4.7825, -3.2379,\n                      -0.3492, -1.5812,  1.4184, -2.9447,  1.1341, -3.1168, -3.4468, -4.6677,\n                      -2.6309, -3.0874, -3.5176, -2.1109, -1.0893, -5.3722, -2.4226, -0.4423,\n                      -6.3923, -1.7217, -5.6199, -5.3242, -2.7995, -5.2210, -0.6261, -4.5223,\n                      -0.3755, -5.9658, -3.4865,  0.6747, -6.2554, -4.5216, -3.1976,  3.1806,\n                      -3.7896, -6.6921, -2.9233, -0.9003, -4.6931, -5.6516, -1.6080, -4.2494,\n                      -1.5036, -1.2485, -2.0175, -4.1678, -0.9579, -4.5238, -2.9640, -4.8287,\n                      -2.2146, -3.1454,  3.7921, -6.7419, -4.4379, -3.0368, -0.5507,  1.6334,\n                      -1.0578, -2.1252, -7.6395, -5.1291, -8.6770, -0.5699, -3.3219, -3.6212,\n                       5.3669, -3.8152, -4.9358, -7.5080, -2.0195, -4.3898, -1.3584,  3.5347,\n                       0.7956, -2.4593, -2.6420, -4.3560, -3.6175, -3.4876, -5.5996, -2.2985,\n                      -4.6440, -3.9819, -4.2931, -6.0500, -3.7595,  0.0780, -4.8302,  0.5697,\n                      -3.2107, -2.6418, -1.4054, -4.4106,  1.1661, -4.7681, -5.4581, -1.5578,\n                      -4.9385, -2.0472,  1.0321, -4.3426, -3.8916, -3.1138, -1.6333, -2.5847,\n                       0.1447, -3.2616, -2.5105, -0.2451, -4.1573, -2.1863, -0.8786, -1.8849,\n                      -2.8338, -1.6337, -3.9322, -5.8302, -3.8745, -4.6643, -1.4695, -4.1850,\n                      -2.0942,  0.1768, -2.9621, -4.1084, -2.5587, -6.6195, -2.5330, -1.9875,\n                      -4.0457,  2.0723, -4.7456, -1.3651, -4.9696, -4.4167, -5.0756, -3.7997,\n                      -1.3238, -1.4464, -4.5733, -2.8929, -3.8161,  1.7563, -4.7661, -3.6003,\n                      -2.3424, -3.7466, -3.1392, -3.4286, -2.6638, -5.1658, -3.1030,  2.7506,\n                      -3.7251,  0.4440, -4.8926, -2.6141, -1.8223, -2.0317, -0.7488, -7.2489,\n                      -2.7608, -6.5030, -7.2681, -3.3135, -0.5390,  0.6011, -6.2597, -3.3021,\n                      -6.3885, -4.0542, -3.9207, -2.8456, -2.0642, -1.9962, -1.6351, -7.1175,\n                      -3.1380, -2.3121, -0.0481, -5.1315, -3.9523, -3.5919, -2.9144, -2.5969,\n                      -3.7725, -4.3469, -3.7546, -4.1524, -5.5897, -1.9036, -2.4453, -6.2102,\n                      -4.4585,  1.8065, -1.7297, -8.3292, -3.1576, -1.6106, -0.6220, -1.5460,\n                      -2.1790, -7.2286, -2.9778, -1.3972, -4.3752, -5.8256, -6.6056, -2.5167,\n                      -2.0728, -1.8371, -2.5439, -2.3676, -2.8545, -7.0075, -3.5152, -4.6000,\n                      -5.9996, -4.1138, -6.6513, -3.6417, -3.8569, -4.6713, -3.8258, -0.8125,\n                      -3.3298,  2.6239, -1.6899, -3.4934, -3.9968, -3.4683, -2.5293,  0.9116,\n                      -4.3394,  0.3270, -7.3443, -1.8856, -3.1949, -1.5535, -1.2688, -5.1146,\n                      -7.1933, -2.9096, -0.3469, -3.3803, -4.0233, -2.2127, -2.4409,  0.2814,\n                      -1.4045, -3.0760, -6.8296, -1.1933, -0.9836, -4.8057, -2.0082, -4.3831,\n                       3.0331, -3.8723, -1.6112,  0.4981, -4.5457, -5.4514, -6.4676, -4.6406,\n                      -5.7622, -1.4693, -2.2889,  2.2378, -3.3839, -3.2674, -2.0453, -3.9914,\n                      -3.6983, -4.1588, -3.9703, -1.1955, -3.6460, -4.4109, -7.7486, -4.0971,\n                      -2.1543, -4.3138, -5.0895, -3.0490, -1.9567, -3.0109,  0.7497, -3.3637,\n                      -1.2486, -4.6086, -3.2111,  0.3594, -2.6884,  0.0904, -4.5930, -3.7440,\n                      -2.7158, -5.1425, -2.7932, -4.4995, -2.2489, -1.7468, -4.4014,  0.6860,\n                      -4.4859, -2.2362, -4.2263, -0.1446, -3.1878, -4.0096, -1.5542, -4.2349,\n                      -1.3370, -1.7538, -1.3977, -3.6771, -3.0982, -6.4102, -4.2558, -4.1757,\n                      -0.4897, -5.4376, -3.3712, -2.1804, -3.6185, -4.5479,  1.3310, -5.3761,\n                      -3.2832, -0.2134, -1.1507, -4.9680, -4.4110, -1.1245, -7.2630, -3.1721,\n                       0.8079, -0.4845, -4.4038, -2.3114, -3.5504,  0.5659, -4.1081, -4.0611,\n                      -4.4083, -1.3945, -0.4378, -1.2505, -4.0566, -1.6043, -6.4255, -2.2227,\n                      -4.0810, -2.9845, -3.2394, -6.9407, -5.2328, -1.5583, -1.8772, -2.4599,\n                      -2.0258, -4.8350, -4.0661, -6.0253, -0.8059, -3.1426, -4.0224, -1.0479,\n                      -3.7309, -1.8307, -3.0321, -4.5883, -2.4054, -5.6625, -4.3059, -4.4241,\n                      -1.4188, -5.4018, -7.2591, -4.7545, -3.4141,  1.3284,  0.8345, -1.3906,\n                      -2.3184, -4.8435, -4.4377, -2.6843, -4.0212, -2.7368, -3.8742, -6.2907,\n                      -3.6883, -1.5162, -5.2351, -0.7999, -4.6129, -1.4508, -5.7343, -1.9722,\n                      -5.9498, -1.7301, -2.1513, -2.1722, -1.8531, -3.9331, -0.1045, -5.8016])),\n             ('layer4.0.bn2.running_var',\n              tensor([10.3340, 20.2086, 13.6463, 20.4467, 21.4599, 13.4519, 16.9161, 14.0015,\n                      24.9532, 13.3299, 23.2039, 13.5276, 18.7827, 15.6440, 13.8456, 20.5368,\n                      14.8505, 15.0639, 15.3123, 14.7511, 19.0464, 22.7570, 18.3285, 18.0391,\n                      20.7634, 26.8647, 19.7561, 12.7772, 20.2525, 18.8657, 22.2021, 17.2897,\n                      15.2541, 16.8167, 13.3398, 17.8557, 15.9938, 23.5018, 15.4907, 18.3353,\n                      17.2674, 22.6656, 17.2386, 19.5006, 18.3024, 18.5855, 21.1419, 19.3808,\n                      15.9760, 10.6602, 15.7314, 22.0014, 18.6908, 34.3431, 19.8251, 16.3580,\n                      15.1147, 27.7248, 18.1875, 21.3405, 22.4223, 22.5401, 14.2332, 22.9895,\n                      14.7787, 24.3075, 19.5730, 17.1964, 17.9297, 19.6271, 15.0415, 15.6624,\n                      15.3204, 23.6891, 14.7587, 20.0908, 22.2952, 21.0488, 18.5918, 17.0186,\n                      14.2446, 11.7790, 25.0467, 19.7102, 23.8273, 23.1802, 15.6930, 14.6899,\n                      19.5652, 30.9514, 15.7246, 24.5661, 15.3531, 18.9997, 15.6939, 17.4722,\n                      21.4517, 18.4840, 19.8855, 18.7674, 15.8730, 22.1739, 13.0792, 23.0380,\n                      16.9051, 17.3054, 30.9692, 23.2335, 17.4645, 22.7945, 23.4049, 24.0032,\n                      11.6276, 22.4903, 13.9088, 18.8172, 17.4197, 20.8295, 26.8998, 20.9465,\n                      18.9713, 18.3967, 13.1929, 10.8545, 14.6558, 12.5571, 18.1406, 20.2235,\n                      22.2054, 19.4756, 16.6879, 16.7561, 22.1769, 12.9567, 10.7776, 24.3056,\n                      13.1165, 24.7784, 18.0555, 19.7876, 28.6527, 20.3748, 23.3476, 15.1673,\n                      21.9301, 17.1727, 18.2093, 14.0275, 18.3848, 14.6493, 15.0071, 20.2363,\n                      18.5388, 18.9956, 18.2517, 24.1677, 16.9656, 18.8437, 20.8381, 21.6553,\n                      22.8531, 23.3456, 19.6983, 22.5208, 25.1094, 17.5835, 23.9092, 12.3919,\n                      25.0432, 12.9708, 15.2885, 18.9629, 13.1738, 16.4999, 17.6940, 20.6106,\n                      17.7674, 14.0820, 14.4035, 16.0400, 14.7617, 17.9633, 18.2611, 27.5326,\n                      16.1818, 14.9606, 17.7350, 13.8122, 12.3822, 19.4636, 23.5623, 18.2497,\n                      14.1057, 19.3530, 22.7076, 12.2205, 20.6324, 28.0866, 20.7967, 17.7287,\n                      24.8762, 14.2191, 21.4909, 19.4585, 15.7922, 20.3799, 15.3829, 29.5295,\n                      19.4605, 21.7122, 14.4087, 17.0077, 25.1720, 16.3325, 20.0065, 21.7276,\n                      16.8307, 15.7009, 13.7319, 17.8229, 18.9860, 30.9247, 23.0139, 18.7585,\n                      21.3744, 19.8519, 14.2867, 12.8827, 17.2226, 20.9407, 22.6740, 19.7761,\n                      16.9511, 25.1909, 15.9075, 20.5333, 25.1671, 13.3485, 16.1445, 17.1449,\n                      13.5509, 23.5689, 20.5873, 18.6885, 13.8308, 18.2064, 14.3813, 16.0813,\n                      14.4340, 13.0898, 16.8656, 14.3483, 20.9283, 17.1731, 15.7400, 23.7160,\n                      21.5864, 21.6498, 23.4705, 17.7954, 19.7756, 16.6488, 20.2497, 17.6704,\n                      20.4717, 16.5865, 16.3315, 25.8226, 18.6539, 14.7190, 23.8434, 24.3042,\n                      17.1622, 11.8970, 21.4138, 12.3096, 17.5382, 17.0784, 16.5367, 21.3102,\n                      16.9918, 17.1640, 19.3215, 21.3305, 23.2404, 13.1687, 27.6039, 18.7164,\n                      15.8365, 18.2386, 22.7283, 13.9824, 12.9541, 27.1523, 21.1255, 15.1664,\n                      12.7546, 14.4598, 11.5569, 16.8369, 24.0972, 19.7384, 16.3158, 25.4410,\n                      17.7378, 25.7216, 15.9315, 22.6275, 20.4755, 18.5730, 32.1683, 21.1395,\n                      17.9031, 16.0180, 17.2429, 16.2241, 12.7391, 18.7690, 27.4997, 25.2079,\n                      24.5367, 14.0674, 22.7911, 15.0499, 20.4662, 25.0071, 13.2348, 23.7118,\n                      12.2353, 17.4124, 14.1944, 14.2798, 25.1659, 16.6583, 22.6828, 12.1330,\n                      22.0103, 17.5795, 18.6864, 11.5875, 16.1256, 16.3806, 19.3620, 15.4380,\n                      17.5948, 17.9297, 19.1412, 20.6224, 33.3560, 22.2144, 16.9001, 16.8233,\n                      16.8524, 12.1931, 19.8753, 24.4465, 13.6738, 20.0210, 11.9112, 22.3439,\n                      22.4228, 14.1794, 14.9797, 24.7096, 26.2837, 16.8512, 30.3665, 19.4187,\n                      19.3759, 19.0315, 22.6889, 14.5347, 23.1910, 23.6758, 14.5847, 13.0648,\n                      22.2553, 29.6501, 16.3706, 18.0765, 11.8863, 24.1392, 27.0158, 24.3069,\n                      20.3983, 21.5120, 25.9257, 13.0494, 17.8302, 15.4694, 14.2045, 22.9396,\n                      20.1742, 10.9548, 15.6222, 18.8501, 12.5496, 18.9844, 12.5521, 18.7207,\n                      14.3720, 17.1042, 19.8525, 32.6347, 22.4971, 22.8211, 23.3787, 15.6443,\n                      12.9859, 19.8348, 17.8626, 19.1805, 18.3333, 14.2145, 12.6837, 27.8672,\n                      23.1546, 13.7714, 12.7099, 18.5065, 28.3534, 16.1026, 19.3431, 23.3312,\n                      22.2968, 13.7862, 15.5079, 18.9684, 18.0483, 12.7724, 22.8123, 24.3122,\n                      16.0732, 16.7414, 18.7624, 15.8413, 26.7771, 13.0588, 21.5512, 11.8627,\n                      16.2694, 15.9768, 20.2523, 22.1445, 18.6462, 13.6761, 17.0408, 12.7435,\n                      20.9314, 16.6884, 18.6929, 17.8994, 16.5926, 23.8958, 24.9169, 11.8118,\n                      18.0120, 18.6458, 16.1216, 26.2980, 16.5004, 22.6395, 18.1803, 17.0024,\n                      22.5319, 22.4400, 21.8702, 15.8029, 15.8031, 18.6063, 14.6564, 23.1742,\n                      21.0237, 22.3907, 13.9356, 14.7785, 18.4482, 23.2165, 15.6870, 11.6329,\n                      23.0647, 19.1860, 24.8730, 29.2000, 21.6379, 17.6595, 16.7172, 17.8088,\n                      18.6502, 21.6392, 19.1897, 11.7962, 18.8083, 16.1023, 11.7485, 19.3149,\n                      18.4651, 15.5657, 32.1572, 11.5337, 21.6902, 23.0133, 21.5509, 19.0003,\n                      22.4358, 13.1678, 18.8182, 18.1140, 13.6968, 22.2017, 18.4793, 19.9076])),\n             ('layer4.0.bn2.num_batches_tracked', tensor(14770)),\n             ('layer4.0.shortcut.0.weight',\n              tensor([[[[-0.2584]],\n              \n                       [[-0.0311]],\n              \n                       [[ 0.0413]],\n              \n                       ...,\n              \n                       [[ 0.0301]],\n              \n                       [[ 0.0547]],\n              \n                       [[-0.0770]]],\n              \n              \n                      [[[ 0.1576]],\n              \n                       [[-0.0553]],\n              \n                       [[-0.0654]],\n              \n                       ...,\n              \n                       [[-0.0006]],\n              \n                       [[-0.1000]],\n              \n                       [[ 0.1267]]],\n              \n              \n                      [[[-0.0816]],\n              \n                       [[ 0.1092]],\n              \n                       [[-0.0214]],\n              \n                       ...,\n              \n                       [[ 0.0320]],\n              \n                       [[-0.1253]],\n              \n                       [[-0.0624]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0381]],\n              \n                       [[-0.0436]],\n              \n                       [[-0.0419]],\n              \n                       ...,\n              \n                       [[-0.0049]],\n              \n                       [[-0.0379]],\n              \n                       [[ 0.0029]]],\n              \n              \n                      [[[-0.0087]],\n              \n                       [[ 0.0454]],\n              \n                       [[-0.1096]],\n              \n                       ...,\n              \n                       [[ 0.0534]],\n              \n                       [[-0.0894]],\n              \n                       [[ 0.0855]]],\n              \n              \n                      [[[ 0.0207]],\n              \n                       [[ 0.1003]],\n              \n                       [[ 0.1877]],\n              \n                       ...,\n              \n                       [[ 0.0645]],\n              \n                       [[-0.1500]],\n              \n                       [[-0.0247]]]])),\n             ('layer4.0.shortcut.1.weight',\n              tensor([0.6212, 0.6583, 0.7409, 0.6158, 0.6755, 0.7310, 0.6920, 0.6563, 0.6018,\n                      0.7843, 0.6608, 0.5284, 0.6376, 0.6651, 0.6365, 0.6143, 0.6383, 0.6180,\n                      0.6534, 0.8330, 0.6420, 0.5639, 0.8439, 0.8600, 0.6316, 0.6047, 0.6110,\n                      0.6287, 0.8295, 0.7148, 0.5051, 0.6789, 0.6523, 0.6209, 0.6224, 0.7929,\n                      0.8163, 0.5287, 0.7048, 0.6729, 0.6888, 0.8317, 0.7908, 0.7090, 0.7275,\n                      0.8771, 0.6882, 0.7543, 0.8157, 0.7151, 0.7517, 0.5633, 0.8778, 0.6949,\n                      0.8336, 0.7252, 0.5579, 0.5608, 0.7509, 0.6875, 0.6584, 0.6558, 0.6115,\n                      0.6711, 0.6312, 0.5930, 0.6485, 0.7014, 0.5454, 0.6439, 0.7087, 0.6053,\n                      0.6655, 0.7268, 0.5290, 0.9002, 0.5471, 0.6526, 0.6856, 0.7542, 0.7876,\n                      0.6388, 0.6962, 0.6858, 0.7251, 0.6576, 0.6769, 0.7861, 0.5642, 0.5828,\n                      0.6895, 0.5950, 0.7175, 0.6024, 0.7027, 0.7074, 0.6428, 0.5390, 0.9163,\n                      0.4923, 0.8262, 0.5984, 0.6376, 0.5672, 0.6946, 0.6098, 0.5883, 0.6186,\n                      0.7306, 0.6204, 0.5957, 0.5971, 0.7287, 0.6201, 0.5768, 0.7276, 0.7323,\n                      0.6238, 0.6208, 0.7170, 0.6549, 0.8265, 0.6987, 0.6476, 0.7617, 0.6358,\n                      0.7486, 0.5663, 0.7880, 0.5814, 0.7104, 0.7803, 0.6723, 0.5410, 0.7230,\n                      0.7232, 0.7169, 0.5221, 0.6508, 0.7480, 0.5630, 0.6357, 0.6210, 0.6504,\n                      0.6008, 0.6801, 0.7947, 0.5881, 0.8637, 0.7052, 0.7082, 0.5951, 0.7370,\n                      0.6330, 0.8310, 0.6818, 0.6962, 0.6391, 0.8197, 0.6503, 0.6689, 0.8329,\n                      0.6133, 0.8370, 0.7071, 0.7939, 0.6143, 0.6690, 0.6300, 0.8434, 0.6001,\n                      0.6742, 0.6424, 0.7402, 0.8481, 0.8380, 0.6288, 0.7606, 0.6344, 0.7818,\n                      0.6353, 0.7527, 0.6821, 0.7013, 0.6705, 0.6152, 0.4464, 0.6752, 0.6230,\n                      0.5872, 0.6819, 0.7608, 0.6178, 0.6597, 0.8878, 0.6440, 0.7251, 0.6145,\n                      0.4875, 0.6777, 0.5987, 0.7659, 0.8179, 0.6416, 0.6787, 0.5438, 0.6648,\n                      0.6783, 0.8169, 0.6461, 0.6781, 0.7528, 0.6581, 0.6945, 0.6703, 0.6955,\n                      0.6679, 0.7451, 0.6496, 0.5703, 0.6943, 0.6200, 0.8190, 0.6928, 0.7282,\n                      0.7917, 0.5903, 0.7625, 0.8225, 0.6166, 0.6429, 0.6694, 0.5766, 0.8460,\n                      0.7565, 0.8520, 0.6431, 0.6971, 0.6669, 0.8444, 0.8608, 0.8567, 0.6779,\n                      0.7957, 0.6105, 0.9779, 0.7096, 0.6134, 0.8056, 0.6349, 0.5981, 0.5048,\n                      0.5556, 0.5956, 0.6511, 0.6965, 0.5549, 0.7305, 0.6723, 0.7522, 0.6988,\n                      0.7971, 0.8349, 0.6781, 0.6179, 0.5763, 0.6531, 0.6439, 0.8181, 0.7683,\n                      0.5118, 0.6382, 0.6515, 0.6965, 0.6244, 0.8739, 0.8404, 0.8353, 0.7911,\n                      0.6139, 0.6469, 0.6113, 0.6870, 0.7490, 0.5893, 0.6768, 0.6378, 0.8513,\n                      0.6376, 0.7001, 0.5954, 0.6903, 0.6352, 0.6181, 0.6568, 0.6693, 0.6843,\n                      0.7315, 0.7355, 0.7071, 0.7002, 0.5949, 0.8411, 0.6113, 0.8161, 0.7217,\n                      0.7874, 0.8586, 0.7133, 0.6660, 0.6337, 0.6139, 0.5816, 0.6128, 0.5528,\n                      0.6249, 0.6177, 0.6919, 0.5863, 0.6332, 0.6397, 0.7397, 0.7066, 0.7282,\n                      0.6114, 0.6637, 0.6371, 0.7468, 0.6614, 0.8915, 0.6586, 0.7539, 0.7101,\n                      0.6008, 0.6526, 0.6730, 0.6344, 0.8488, 0.6009, 0.6678, 0.7036, 0.7572,\n                      0.7298, 0.6776, 0.6306, 0.5324, 0.8628, 0.5353, 0.7074, 0.5883, 0.8408,\n                      0.8217, 0.7920, 0.8423, 0.6622, 0.6550, 0.8372, 0.8196, 0.5929, 0.7206,\n                      0.7929, 0.9321, 0.8676, 0.8365, 0.5966, 0.6223, 0.6982, 0.5245, 0.6783,\n                      0.7681, 0.6583, 0.7588, 0.7058, 0.5667, 0.6781, 0.6417, 0.6998, 0.6243,\n                      0.7078, 0.7576, 0.5872, 0.6921, 0.6217, 0.6524, 0.6671, 0.7941, 0.5852,\n                      0.6644, 0.9197, 0.8239, 0.6156, 0.7306, 0.6401, 0.7690, 0.6903, 0.8192,\n                      0.6503, 0.4754, 0.6355, 0.6952, 0.5877, 0.6168, 0.5872, 0.5875, 0.5770,\n                      0.6241, 0.5992, 0.8251, 0.6477, 0.7985, 0.7250, 0.6851, 0.5746, 0.6919,\n                      0.7687, 0.6035, 0.6339, 0.8494, 0.7931, 0.6975, 0.6373, 0.6609, 0.4660,\n                      0.6371, 0.9219, 0.6664, 0.6955, 0.8472, 0.7152, 0.6878, 0.7802, 0.5197,\n                      0.7259, 0.7843, 0.8035, 0.6697, 0.5932, 0.6449, 0.4613, 0.5558, 0.7925,\n                      0.6704, 0.7431, 0.7870, 0.6997, 0.6573, 0.8205, 0.5622, 0.6183, 0.8183,\n                      0.8459, 0.8530, 0.7066, 0.6732, 0.6634, 0.7252, 0.6317, 0.6459, 0.7428,\n                      0.6423, 0.6485, 0.7732, 0.8033, 0.6992, 0.6052, 0.6596, 0.6412, 0.6571,\n                      0.8379, 0.6963, 0.6364, 0.6291, 0.5971, 0.6962, 0.6573, 0.5516, 0.8384,\n                      0.7238, 0.6259, 0.6397, 0.5695, 0.5503, 0.5039, 0.6367, 0.5580, 0.7849,\n                      0.6167, 0.6891, 0.5144, 0.6292, 0.6091, 0.8232, 0.6993, 0.7224, 0.5577,\n                      0.6491, 0.6328, 0.7852, 0.6297, 0.6832, 0.6621, 0.6813, 0.5505, 0.6887,\n                      0.6235, 0.6314, 0.7111, 0.8283, 0.8535, 0.7097, 0.9098, 0.5619])),\n             ('layer4.0.shortcut.1.bias',\n              tensor([-0.4564, -0.4218, -0.4613, -0.4731, -0.3194, -0.4972, -0.3595, -0.4265,\n                      -0.3650, -0.4644, -0.4148, -0.3740, -0.3789, -0.4411, -0.4453, -0.4352,\n                      -0.3793, -0.3687, -0.4601, -0.4049, -0.4617, -0.4022, -0.3812, -0.3176,\n                      -0.4453, -0.4773, -0.4760, -0.4751, -0.3162, -0.4127, -0.4768, -0.4305,\n                      -0.4050, -0.4802, -0.3474, -0.3963, -0.3968, -0.5090, -0.3673, -0.4208,\n                      -0.4315, -0.3251, -0.4339, -0.3469, -0.3548, -0.3713, -0.3267, -0.3926,\n                      -0.3954, -0.5975, -0.4014, -0.4102, -0.3112, -0.2928, -0.3915, -0.4295,\n                      -0.5044, -0.3699, -0.4618, -0.4210, -0.4012, -0.4673, -0.4295, -0.4196,\n                      -0.4693, -0.4627, -0.4568, -0.3909, -0.5007, -0.5133, -0.4307, -0.3837,\n                      -0.3797, -0.3525, -0.2941, -0.4312, -0.4478, -0.3251, -0.3577, -0.4428,\n                      -0.4335, -0.4485, -0.3194, -0.4208, -0.3742, -0.4086, -0.3328, -0.4131,\n                      -0.3994, -0.3596, -0.3530, -0.4506, -0.3913, -0.4855, -0.4288, -0.3452,\n                      -0.3032, -0.4940, -0.3180, -0.4640, -0.3800, -0.4286, -0.4412, -0.4128,\n                      -0.4072, -0.4766, -0.3287, -0.4367, -0.4173, -0.4412, -0.3848, -0.3511,\n                      -0.4920, -0.3790, -0.4109, -0.3383, -0.4633, -0.4169, -0.3342, -0.4408,\n                      -0.3761, -0.3770, -0.3731, -0.4491, -0.4406, -0.4215, -0.3238, -0.4524,\n                      -0.2840, -0.4183, -0.3957, -0.3568, -0.4715, -0.4176, -0.5090, -0.3357,\n                      -0.4112, -0.2518, -0.3851, -0.3448, -0.3224, -0.3867, -0.4258, -0.3697,\n                      -0.4375, -0.3975, -0.3822, -0.3869, -0.4014, -0.3761, -0.3677, -0.4618,\n                      -0.4104, -0.5066, -0.4625, -0.4159, -0.3473, -0.4175, -0.2946, -0.3598,\n                      -0.4046, -0.3912, -0.4442, -0.2818, -0.4664, -0.4243, -0.3982, -0.4467,\n                      -0.4303, -0.4497, -0.4264, -0.3353, -0.4332, -0.4561, -0.4040, -0.3765,\n                      -0.4014, -0.4952, -0.3547, -0.4349, -0.4638, -0.3906, -0.3897, -0.3814,\n                      -0.3766, -0.4488, -0.4264, -0.3920, -0.3656, -0.5696, -0.4338, -0.3725,\n                      -0.4723, -0.3621, -0.3894, -0.3732, -0.3346, -0.3438, -0.4508, -0.4837,\n                      -0.3306, -0.4605, -0.4072, -0.3560, -0.3881, -0.3916, -0.4660, -0.4017,\n                      -0.3588, -0.4394, -0.3892, -0.3348, -0.4270, -0.3824, -0.4309, -0.3777,\n                      -0.4165, -0.4157, -0.4254, -0.4884, -0.3744, -0.3619, -0.3519, -0.3518,\n                      -0.3032, -0.3025, -0.4338, -0.4533, -0.3776, -0.3324, -0.4108, -0.4578,\n                      -0.4127, -0.2744, -0.4713, -0.3062, -0.3900, -0.3481, -0.4195, -0.3491,\n                      -0.3957, -0.2879, -0.3667, -0.3486, -0.3815, -0.3926, -0.5004, -0.3822,\n                      -0.3691, -0.4258, -0.4249, -0.3922, -0.4097, -0.5420, -0.4278, -0.3258,\n                      -0.4345, -0.3774, -0.3516, -0.4029, -0.3844, -0.4077, -0.3200, -0.4117,\n                      -0.4506, -0.6064, -0.3555, -0.4419, -0.3324, -0.4547, -0.3600, -0.4447,\n                      -0.3716, -0.4359, -0.4355, -0.4528, -0.3482, -0.4291, -0.4086, -0.3789,\n                      -0.4467, -0.4592, -0.3620, -0.3554, -0.3448, -0.4212, -0.4409, -0.3492,\n                      -0.3619, -0.3822, -0.3926, -0.3872, -0.4138, -0.3842, -0.3373, -0.3535,\n                      -0.5111, -0.3817, -0.4211, -0.3526, -0.3711, -0.4298, -0.3340, -0.3351,\n                      -0.3768, -0.3941, -0.4524, -0.3070, -0.3463, -0.4426, -0.3546, -0.4071,\n                      -0.5875, -0.4357, -0.4416, -0.4916, -0.4759, -0.4414, -0.3104, -0.3554,\n                      -0.4053, -0.4777, -0.3458, -0.4554, -0.4627, -0.3983, -0.4261, -0.3744,\n                      -0.4284, -0.3470, -0.4758, -0.4217, -0.3706, -0.4286, -0.3507, -0.4750,\n                      -0.4030, -0.3653, -0.3344, -0.4968, -0.4130, -0.4090, -0.3793, -0.4468,\n                      -0.3463, -0.4821, -0.3160, -0.4755, -0.2941, -0.3188, -0.4084, -0.4419,\n                      -0.4140, -0.4296, -0.3839, -0.3750, -0.4590, -0.3369, -0.3998, -0.3551,\n                      -0.3162, -0.4170, -0.3877, -0.4011, -0.3453, -0.3607, -0.2951, -0.5107,\n                      -0.3463, -0.3944, -0.3888, -0.3754, -0.4318, -0.3025, -0.3970, -0.4961,\n                      -0.4891, -0.3780, -0.3854, -0.4062, -0.4369, -0.3337, -0.4105, -0.3846,\n                      -0.4636, -0.3382, -0.2628, -0.4107, -0.3806, -0.4267, -0.5830, -0.4244,\n                      -0.3539, -0.4767, -0.3873, -0.3733, -0.5822, -0.4932, -0.4352, -0.3820,\n                      -0.4180, -0.3978, -0.4892, -0.2988, -0.3492, -0.3720, -0.4582, -0.4122,\n                      -0.4163, -0.4047, -0.3259, -0.3333, -0.4081, -0.4071, -0.4549, -0.4297,\n                      -0.4028, -0.3829, -0.4028, -0.3948, -0.3399, -0.4830, -0.4582, -0.4470,\n                      -0.4113, -0.5077, -0.3545, -0.3867, -0.3940, -0.4203, -0.2813, -0.4889,\n                      -0.3456, -0.3825, -0.3036, -0.4619, -0.3791, -0.3751, -0.3673, -0.4292,\n                      -0.4637, -0.3464, -0.4387, -0.4042, -0.3263, -0.4168, -0.4039, -0.3584,\n                      -0.4264, -0.4230, -0.4333, -0.3707, -0.3568, -0.4031, -0.4271, -0.5004,\n                      -0.3820, -0.3803, -0.3368, -0.4606, -0.4467, -0.3128, -0.3934, -0.3832,\n                      -0.4336, -0.4454, -0.4371, -0.3962, -0.4163, -0.4717, -0.3661, -0.3784,\n                      -0.3663, -0.3381, -0.3491, -0.5424, -0.4125, -0.3587, -0.3813, -0.4393,\n                      -0.4199, -0.4859, -0.3883, -0.3920, -0.4072, -0.3691, -0.4691, -0.4041,\n                      -0.5012, -0.3484, -0.3972, -0.3968, -0.4434, -0.4310, -0.4462, -0.3612,\n                      -0.3908, -0.3391, -0.2095, -0.3854, -0.4484, -0.3621, -0.4003, -0.4037,\n                      -0.4187, -0.4362, -0.3648, -0.3747, -0.4169, -0.4057, -0.3174, -0.4014])),\n             ('layer4.0.shortcut.1.running_mean',\n              tensor([-2.5533e+00, -2.7711e+00, -1.2741e+00, -1.7566e-01, -2.8253e+00,\n                      -8.5176e-02, -1.7121e+00, -3.4623e+00, -7.7300e-01, -2.0516e+00,\n                       7.4821e-01, -3.3370e+00, -5.9212e-01, -1.1994e+00, -4.5009e+00,\n                      -3.7338e+00, -3.4730e+00, -2.2498e+00, -4.8193e+00, -3.7017e+00,\n                       3.9576e-02,  1.4574e+00, -2.9426e+00, -4.5845e+00, -3.2280e+00,\n                       2.5263e-01,  7.4847e-01, -3.2437e+00, -4.2250e+00, -1.8406e+00,\n                       1.5161e+00, -4.6325e+00, -1.8516e+00,  1.1605e+00, -2.6591e+00,\n                      -3.0648e+00, -2.3491e+00,  7.0998e-01, -2.0129e+00, -4.4963e+00,\n                      -3.9959e+00, -2.6758e+00, -2.5434e+00, -3.1116e+00, -2.2063e+00,\n                      -2.3204e+00, -1.0130e+00, -4.0839e+00, -3.4332e+00, -7.7955e-01,\n                      -1.9022e+00, -1.2036e-03, -3.1815e+00, -1.3466e+00, -2.6127e+00,\n                      -1.7419e+00, -4.3349e-02, -1.9293e+00, -4.4701e+00, -4.8812e+00,\n                      -2.3557e+00, -2.9627e+00, -2.4374e+00, -2.6076e+00, -3.3724e+00,\n                       3.5411e+00,  1.0902e+00, -3.5963e+00,  1.4728e+00,  2.5117e-01,\n                      -3.4544e+00, -4.1919e+00, -1.5912e+00, -4.3581e-01, -4.6569e+00,\n                      -2.9319e+00, -3.1853e-01, -1.4049e+00, -1.3940e+00, -3.1243e+00,\n                      -3.3090e+00, -5.2683e+00, -1.8208e+00, -3.2017e-01, -1.3622e+00,\n                      -2.3359e+00, -2.3823e+00, -2.4591e+00,  2.3365e+00, -2.2670e+00,\n                      -3.6722e+00,  1.0170e+00, -1.7166e+00,  1.7577e-01, -2.7659e+00,\n                      -2.9519e+00,  8.1948e-01,  3.4419e-01, -3.5856e+00,  1.9673e+00,\n                      -3.3761e+00,  1.6894e-01, -1.8178e+00,  2.1490e+00, -9.4081e-01,\n                       1.6335e+00,  1.3891e+00,  9.6332e-01, -1.1342e+00,  4.2002e-01,\n                       5.5255e-01,  2.4509e+00, -2.7583e+00, -6.3844e-01, -1.8116e+00,\n                      -1.8347e+00, -3.8122e+00, -2.5369e+00,  1.7238e+00, -3.4291e+00,\n                      -9.8704e-01, -7.9972e-01, -1.5117e+00, -6.4886e-01, -3.6599e+00,\n                      -3.9070e+00, -3.7627e+00,  2.6424e-01, -3.0964e+00, -2.0968e+00,\n                      -3.3052e+00, -1.7415e+00, -3.2737e+00, -2.6831e+00, -8.8152e-01,\n                      -3.0182e+00, -2.3441e+00,  3.6651e+00,  1.6653e+00, -2.9915e+00,\n                       1.2417e+00, -4.7079e+00,  7.2546e-01, -1.9281e+00, -1.0706e-01,\n                      -3.5272e+00, -2.7812e+00, -3.4544e+00, -2.9045e+00, -3.1213e+00,\n                      -3.9019e+00,  8.8548e-01, -2.4599e+00,  4.3193e-01, -1.8972e+00,\n                      -1.1765e-01, -2.7212e+00, -1.5138e+00, -3.8930e+00,  4.0700e+00,\n                       1.5623e+00, -2.8090e+00, -1.1949e+00, -2.2619e+00, -3.1084e+00,\n                      -1.1707e+00, -3.7004e-02, -2.9218e+00,  1.9131e+00, -2.6326e+00,\n                      -4.1361e+00, -2.3626e+00, -4.5030e+00, -1.3489e+00, -3.4219e+00,\n                      -3.1295e+00, -1.8017e+00, -4.8346e+00, -3.3372e+00, -4.3773e+00,\n                      -4.0118e+00, -2.6248e-01, -8.3007e-01,  2.2747e+00, -1.7490e+00,\n                      -3.5259e+00,  1.8525e+00, -3.7292e+00, -3.6232e+00,  3.6859e+00,\n                      -3.6454e+00, -2.6517e+00, -4.0023e+00, -1.0968e+00, -3.6204e+00,\n                      -3.6117e+00, -1.1428e+00,  8.3229e-01,  8.9542e-01, -2.1959e+00,\n                       5.0961e-03, -3.2931e+00, -3.6505e+00, -4.2537e+00, -3.6074e+00,\n                       1.7528e+00,  9.7607e-01, -2.0282e+00, -3.4751e+00,  2.1386e+00,\n                      -4.4408e+00, -1.1462e+00, -7.7401e-01, -3.5295e+00,  1.4845e+00,\n                      -1.4890e+00, -3.5070e+00, -5.9079e+00, -3.3809e+00,  3.0967e-01,\n                      -2.6568e+00, -1.2497e+00, -3.8325e+00, -3.1356e+00, -1.9307e+00,\n                      -3.8996e+00, -2.8796e+00, -3.1968e+00, -4.1474e+00, -2.9392e+00,\n                      -2.3637e+00,  2.3939e+00, -3.0591e+00, -2.7745e+00, -3.3263e+00,\n                      -7.5191e-01,  1.6825e-01, -1.9659e+00, -2.8985e+00, -3.5714e+00,\n                      -4.3345e+00, -3.4132e+00,  9.9772e-01, -5.0759e+00, -4.2706e+00,\n                      -4.1420e+00, -3.8877e+00, -3.6792e+00, -3.8996e+00, -3.5473e+00,\n                       1.3079e+00, -2.3702e+00,  2.4258e+00,  9.3085e-01, -1.7766e+00,\n                      -3.1080e+00,  6.8252e-01, -3.5617e+00, -2.7272e+00, -4.1003e+00,\n                      -2.0471e+00, -3.8257e+00, -3.2727e+00, -3.6895e+00,  7.7193e-01,\n                      -1.8838e+00, -3.0364e+00,  3.5606e-01, -2.3485e+00, -1.0674e+00,\n                       2.2046e+00,  1.3236e+00, -2.8801e+00, -1.9823e+00,  2.4048e+00,\n                      -2.9078e+00, -2.4668e+00, -4.2229e+00, -2.5607e+00, -3.9470e+00,\n                      -1.9489e+00, -2.7727e+00, -1.0370e+00, -1.9655e+00,  1.1776e+00,\n                      -2.9436e+00,  5.2585e-01, -3.2274e+00, -3.3443e+00, -3.6046e+00,\n                       9.2995e-01, -3.6898e+00, -4.4563e+00,  9.4954e-01, -4.9838e-01,\n                      -3.4090e+00, -4.4627e+00, -2.0633e+00, -2.2369e+00, -1.7649e+00,\n                      -6.5357e-01,  1.6414e+00, -1.7964e+00, -8.2143e-01, -2.1903e+00,\n                      -3.4530e+00, -3.6971e+00, -3.0392e+00, -3.5799e+00, -1.8701e+00,\n                      -2.8225e-01,  1.6411e+00,  2.0111e-01, -1.8628e+00,  5.1205e-01,\n                      -1.5655e+00, -3.6800e+00, -3.1663e+00, -8.8201e-01,  4.0162e-02,\n                       1.4537e+00, -3.4701e+00, -2.2729e+00, -4.1690e+00, -2.2765e+00,\n                      -2.1770e+00, -3.3950e+00, -2.4846e+00, -2.3693e+00, -6.0811e-01,\n                      -3.7993e+00, -3.1875e+00, -3.0856e+00, -2.5369e+00,  2.2929e+00,\n                      -1.0414e+00, -2.8866e-01, -3.3916e+00, -4.6877e+00, -3.4586e+00,\n                      -1.6492e+00, -2.1818e+00, -1.4395e+00, -2.6101e+00, -2.5489e+00,\n                       1.9384e+00, -1.8325e+00,  2.0590e+00, -2.4587e+00,  2.1275e+00,\n                      -5.4047e+00, -1.3779e+00, -3.0804e+00, -4.5364e+00, -1.2319e+00,\n                       2.9847e+00, -2.2077e+00, -3.5003e+00, -3.8781e+00, -2.4482e+00,\n                      -2.5436e+00, -3.0494e+00, -3.7075e+00, -1.8879e+00,  8.2211e-01,\n                      -3.6670e+00,  1.9746e+00, -1.2942e+00, -7.7227e-01, -3.2501e+00,\n                      -1.5426e+00, -4.2902e+00, -2.7428e+00,  3.3886e+00, -4.5521e+00,\n                      -3.6527e+00, -3.0451e-01,  1.8113e+00, -2.0054e+00, -2.7064e+00,\n                      -4.0763e+00, -2.2645e+00, -2.5246e-01,  1.1759e+00,  4.6471e-01,\n                      -4.9414e+00, -2.7991e+00, -1.6380e+00, -3.5072e+00, -3.3772e+00,\n                       6.6616e-01, -2.2875e+00,  2.1621e+00, -3.9780e+00, -3.8824e+00,\n                      -2.7139e+00,  9.5495e-01,  3.0664e+00, -3.1839e+00, -3.6249e+00,\n                      -2.7806e+00, -1.6586e+00,  6.8348e-03,  1.5850e+00,  8.2696e-01,\n                       7.2599e-01, -4.8836e-01, -2.5835e+00, -2.3484e+00, -3.8866e+00,\n                      -4.0943e+00, -3.7466e+00,  9.0349e-01, -2.3852e+00, -4.8071e+00,\n                      -2.3111e+00,  1.0160e+00, -4.7147e+00, -3.9484e+00, -2.5941e+00,\n                      -3.3142e+00, -4.3802e+00,  3.0232e+00,  1.4647e+00, -2.2360e+00,\n                      -3.1851e+00, -2.7374e+00, -3.5651e+00, -3.1843e+00, -2.4152e+00,\n                      -2.0731e+00,  4.7499e-01, -3.4246e+00, -2.8140e+00, -2.0839e+00,\n                      -3.6350e+00, -3.8378e+00,  1.2808e+00, -1.2952e+00, -2.4872e+00,\n                      -2.8569e+00, -4.4937e+00, -1.4579e+00, -3.5494e+00, -2.8195e+00,\n                      -5.3062e+00, -2.9409e+00, -4.1363e+00, -3.7329e+00, -4.7070e+00,\n                      -4.0670e+00, -2.8395e+00, -1.1202e+00, -2.6037e-01, -1.6611e+00,\n                      -2.2976e+00,  1.8342e+00, -2.6049e+00, -2.7991e+00, -1.0590e+00,\n                      -1.3323e+00, -2.3988e+00, -3.6990e+00, -2.3845e+00, -8.6308e-02,\n                      -9.6688e-01,  9.0134e-02, -3.1142e+00, -2.7497e+00, -2.2233e+00,\n                      -2.5237e+00, -1.0242e+00,  2.4073e+00, -3.3483e+00, -1.7426e+00,\n                      -4.1182e-01, -2.6789e+00, -4.1031e+00, -2.9939e+00, -3.6879e+00,\n                       8.1583e-01,  5.0226e-01,  7.7907e-01,  2.1733e+00,  1.4245e+00,\n                      -2.7647e+00,  2.7674e-01,  2.0823e-01,  1.1778e+00,  2.1764e+00,\n                      -2.9336e+00, -2.5454e+00, -4.5450e+00, -1.1522e+00, -3.2241e+00,\n                      -3.3988e+00, -3.9248e+00, -1.7742e+00,  3.3468e+00, -1.5912e+00,\n                      -1.2105e-01, -1.8985e+00,  1.3581e+00, -2.6194e+00, -4.4440e-01,\n                      -3.2320e+00, -2.8602e+00, -3.5438e+00, -3.2601e+00, -6.2164e-01,\n                      -3.1833e+00,  1.8878e-01])),\n             ('layer4.0.shortcut.1.running_var',\n              tensor([15.1249,  4.3630,  5.9435,  9.5981, 13.3781,  3.5425,  8.0772, 11.8952,\n                       7.8321,  9.2291,  6.5430, 16.2358,  7.3675,  4.1391, 10.1735,  3.8329,\n                      13.7410, 11.6563, 13.0314, 13.2378,  3.8907, 11.0939, 12.1811, 12.8617,\n                      12.0986,  9.2654,  8.9718, 12.8075, 12.5924,  4.0717, 10.2486, 14.5850,\n                       9.4397,  9.3561, 11.7161, 15.5961, 11.9845,  9.7624, 12.1133, 13.9037,\n                      10.7397, 11.4293, 12.7860,  4.8824, 11.0293, 10.3029, 10.9503, 18.2793,\n                      12.7684,  3.3569, 11.1474, 11.7238,  9.7094,  4.8219, 10.0919,  7.5884,\n                       5.7253,  3.7572, 12.5825,  6.4091,  6.1192,  9.4559, 14.8495,  9.5573,\n                       7.6885,  9.7793,  8.1604, 16.8798,  9.6085,  9.0819, 14.0258, 16.7504,\n                       8.8099,  6.2769, 20.5560,  9.9861,  9.2901,  3.7683,  4.2150, 13.1061,\n                      17.8956, 13.9591,  4.0673,  4.8729,  4.6910,  4.1919,  6.4017, 17.4286,\n                      11.1667,  7.4312, 15.0530,  7.0919, 15.5803,  6.7245,  5.2641, 11.7075,\n                      10.3695,  7.4604, 10.3163, 16.2664, 13.5698,  8.6232, 10.3928,  9.6365,\n                       3.7256, 10.2924, 15.3705,  8.4285, 11.4829,  9.2989,  8.6832, 11.2448,\n                      16.1301,  6.6372, 10.4306,  5.9987, 15.1642,  8.2139,  8.3874, 11.3528,\n                       6.2066, 10.6584, 17.2859, 16.3171, 13.3110, 15.3054, 10.8694,  7.1854,\n                      11.3611, 15.4047,  5.6918,  8.2502,  5.2239, 13.9355, 12.0732, 22.2648,\n                      16.7611, 15.6368, 12.3494, 12.8624, 11.9619, 12.9660,  9.5422, 11.9031,\n                       9.2265, 19.4087, 13.2219, 12.1168, 10.3658, 10.9560, 12.2935, 10.1003,\n                      16.8138,  7.6277,  9.3585,  4.0092, 11.0566, 10.7615, 13.7688, 14.8469,\n                      10.7138, 11.3135,  4.7201, 13.8725,  4.3939, 12.2658,  9.5259,  9.6584,\n                       3.6458, 10.9402, 17.2790, 12.3831, 15.2523,  4.5546, 10.3016, 10.3966,\n                      11.8309, 15.4413, 11.9359, 13.3961, 12.3562,  4.0416,  5.8077, 10.2383,\n                      12.0970, 15.9736, 13.1735, 12.2852, 16.2252,  8.5009,  7.2113,  9.9219,\n                      10.1237,  9.9533, 10.6809, 17.7855, 10.6607,  6.2131,  8.8946,  4.0720,\n                       8.2926, 11.0963, 15.7802, 17.9557,  9.5477, 11.2693,  7.5786,  6.0035,\n                      12.2110,  8.2577,  9.7502,  6.5557,  4.6821, 13.1034, 10.5515,  4.0028,\n                      13.3833, 17.1711, 14.8354, 10.4297,  2.9599,  4.2557, 18.7819, 20.2762,\n                      13.4070, 16.5418, 25.2881,  8.6481, 16.2744, 11.0298,  4.8504,  4.8766,\n                      18.5845, 13.5606, 11.6979, 12.5259,  9.5210, 16.1656, 11.5024, 12.8978,\n                      11.2958, 10.7171,  9.1849, 19.0838, 17.5706, 11.6926, 11.6103, 11.7713,\n                      14.3478, 11.3444, 11.2803, 17.3007, 12.6873,  6.6296, 12.9604,  9.1547,\n                       8.7800,  4.5427,  4.5783, 15.6107,  8.8063, 12.4028,  8.5034, 12.5694,\n                      10.0784,  8.9705, 15.5790,  8.2972, 13.3929, 14.0419, 14.6846,  7.0022,\n                      12.0759, 12.0683,  8.0434, 17.9963, 13.6141,  8.0659, 13.1131, 17.5247,\n                       4.2537,  5.2263,  5.6956, 10.1850,  6.8761,  9.2073,  6.7911, 11.3048,\n                      11.6288, 10.1121,  6.6149, 13.5613,  9.0306,  6.8893,  7.1480,  7.4955,\n                      13.6274, 11.6783,  6.6509,  5.7618,  4.6746, 10.7916, 11.7605,  7.3831,\n                      13.3407,  5.3282, 13.8829, 11.8967, 12.7343,  4.3198, 10.1136,  9.3715,\n                      10.5130, 10.7290,  7.3668,  3.8126, 19.8936, 12.9000,  8.8368,  3.4948,\n                       7.9004, 16.7949, 10.3513, 13.3012,  3.4932,  5.9126, 11.8335,  4.4293,\n                       8.6169, 10.7880, 14.4746, 12.9564,  9.1993, 12.1054,  8.9855, 10.9086,\n                       9.3474, 12.1049, 16.0806, 23.5990,  9.9264, 17.1543, 10.5526, 12.2111,\n                      16.0451, 11.4719, 10.2228, 10.4522,  7.7827, 12.8297, 14.1344,  8.9396,\n                      11.1858, 13.0351,  4.5382,  6.2163, 11.6242, 12.0670, 10.7177,  9.6453,\n                      13.9839, 12.6644, 12.2632, 11.1281,  8.3395, 13.7579, 12.6067,  9.7452,\n                       9.9969, 12.2758,  4.4878, 16.9206,  4.0903, 10.0317, 14.8206, 16.1017,\n                       7.4168, 13.6922,  8.9029, 10.1077, 21.6368,  8.2024,  7.2629,  8.4784,\n                       6.4136, 17.2297, 14.6532,  8.9033,  9.0008, 12.2745,  2.8981,  3.5811,\n                      10.5839, 11.8748, 17.2261,  8.5623,  6.0912, 12.4863, 14.3637,  8.2930,\n                      14.2069, 10.0003, 12.2921, 11.8229, 11.1243, 11.6619,  8.7515, 10.1108,\n                       9.9721, 14.3397,  8.4254, 13.5910,  4.3479, 10.5343, 11.3248,  4.3271,\n                      10.3125, 13.6075, 13.9409,  9.7507,  6.5590, 13.1615, 15.2885,  7.8879,\n                      10.9638,  4.7321,  9.8835, 10.9126, 12.6471, 14.0272, 12.5727, 10.3130,\n                       7.7503,  9.5760, 16.9527, 13.6716,  5.4414,  6.6563,  5.7120, 14.6771,\n                       8.5296, 12.2222,  4.3840, 10.6478, 15.7253,  8.7036, 12.5571, 17.8341,\n                      16.5428, 13.0470, 11.9598, 12.0124,  4.5846,  4.6887,  3.2082, 14.1340,\n                      11.0874, 12.5437, 16.7792,  3.5248,  3.4990,  9.8688,  8.7403, 16.0968,\n                       8.7412,  4.1108,  7.6003, 13.1015, 15.2748,  3.0982, 13.7237, 10.1036,\n                       8.2789,  6.7137, 10.5623,  8.1977, 12.6319, 13.9053, 11.9281, 15.8812,\n                      10.8316, 11.8630,  8.7933,  7.3038,  7.5897,  9.4721,  9.0946,  4.0655,\n                      14.4785,  6.7516, 11.4869, 13.8727, 11.1124, 11.4871, 16.3301, 13.0064,\n                      12.8501, 14.1632,  9.3938,  9.3821,  4.3454,  5.6582, 12.8499,  5.0973,\n                      11.7953, 11.3860,  6.5264, 12.1987, 13.3602,  4.7586, 11.8434,  8.6693])),\n             ('layer4.0.shortcut.1.num_batches_tracked', tensor(14770)),\n             ('layer4.1.conv1.weight',\n              tensor([[[[-2.4575e-02,  7.4262e-02,  8.4681e-02],\n                        [-1.1069e-02,  1.3483e-01,  1.0382e-01],\n                        [ 8.3321e-03, -1.8621e-02,  1.4670e-01]],\n              \n                       [[ 2.2811e-01,  3.3067e-01,  2.6158e-01],\n                        [ 6.6075e-02,  2.0183e-02,  3.4325e-02],\n                        [-3.5336e-02, -1.1648e-01, -6.0618e-02]],\n              \n                       [[-1.4233e-01, -2.2719e-01, -1.2114e-01],\n                        [-1.0650e-01, -2.1666e-01, -1.0013e-01],\n                        [ 1.2167e-01,  1.6498e-01,  1.2525e-01]],\n              \n                       ...,\n              \n                       [[-1.7109e-01, -1.1171e-01, -1.6002e-01],\n                        [-1.1095e-01,  1.0720e-02,  5.7276e-02],\n                        [-8.8389e-03,  3.7497e-02,  1.6415e-02]],\n              \n                       [[ 2.1096e-02, -6.6381e-02, -1.4671e-01],\n                        [ 1.2609e-01, -7.9934e-02, -1.9637e-02],\n                        [ 1.6278e-01,  5.7935e-02, -7.0024e-02]],\n              \n                       [[-1.8229e-01, -2.5689e-01, -2.5634e-01],\n                        [-1.0972e-01, -2.7592e-02, -2.5297e-02],\n                        [ 1.1229e-02,  3.0251e-02, -1.1881e-01]]],\n              \n              \n                      [[[-2.5925e-01, -3.4560e-01, -2.5292e-01],\n                        [ 8.7593e-02,  1.1546e-01,  2.1661e-01],\n                        [ 1.1923e-01,  1.4015e-01,  1.9542e-01]],\n              \n                       [[-1.8022e-01, -5.9384e-02, -1.7682e-01],\n                        [-4.0331e-02, -1.4692e-02,  2.8849e-02],\n                        [ 2.7975e-02, -1.1240e-01, -1.0844e-01]],\n              \n                       [[ 8.8775e-02, -4.9771e-02, -4.3241e-02],\n                        [ 2.0962e-01,  6.9990e-02,  1.6924e-01],\n                        [ 1.5941e-02,  9.9007e-02,  1.0955e-01]],\n              \n                       ...,\n              \n                       [[ 3.6604e-02, -2.0030e-02,  4.9489e-02],\n                        [ 6.8760e-02,  8.1308e-02,  1.6002e-01],\n                        [ 4.5665e-02,  1.6003e-01,  6.0100e-02]],\n              \n                       [[-2.3102e-01, -9.2064e-02, -1.0181e-01],\n                        [-8.2338e-02, -6.7340e-02, -3.6326e-01],\n                        [ 5.6379e-02,  1.6663e-01,  1.3510e-01]],\n              \n                       [[-1.5615e-01,  8.2590e-02, -3.5754e-02],\n                        [-2.2369e-01, -3.7497e-02, -7.6033e-02],\n                        [-2.7925e-01, -2.8045e-01, -3.2734e-01]]],\n              \n              \n                      [[[ 7.6453e-02,  1.2207e-01,  1.9891e-01],\n                        [ 8.6468e-02,  8.8376e-02,  7.2249e-02],\n                        [ 1.0510e-02,  4.3578e-02,  9.2748e-02]],\n              \n                       [[-2.0216e-01, -1.1094e-01, -1.9655e-01],\n                        [-1.0871e-01, -5.1111e-02, -7.1851e-02],\n                        [-2.5557e-01, -1.0304e-01, -2.1149e-01]],\n              \n                       [[ 4.8790e-02,  1.3328e-02,  1.1573e-01],\n                        [-7.0832e-03, -1.5142e-01, -1.0487e-01],\n                        [-1.5364e-02, -5.2083e-02, -4.6620e-02]],\n              \n                       ...,\n              \n                       [[ 2.4219e-02, -2.7727e-02, -2.4249e-02],\n                        [-5.2132e-03, -1.4676e-02, -1.2116e-02],\n                        [-1.2058e-02, -1.5434e-01, -8.7497e-02]],\n              \n                       [[-3.8524e-02,  4.3414e-02,  5.1759e-02],\n                        [ 2.9196e-02,  4.7413e-02, -2.8620e-02],\n                        [ 4.0354e-02,  8.5702e-02, -6.8424e-02]],\n              \n                       [[-5.9043e-02, -1.4224e-01, -1.6059e-02],\n                        [ 1.4266e-01,  1.0197e-02,  4.1881e-02],\n                        [ 5.4840e-02,  8.6098e-03,  2.7373e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 1.1725e-02, -2.0104e-01, -1.9890e-01],\n                        [ 1.2189e-01, -4.2990e-02,  4.4069e-02],\n                        [ 1.2162e-01, -2.9485e-02, -2.7401e-01]],\n              \n                       [[-2.0323e-02, -4.7319e-02, -8.9027e-02],\n                        [-1.7093e-01,  7.2732e-03,  8.3814e-02],\n                        [-2.5821e-02,  1.7224e-01,  7.7383e-02]],\n              \n                       [[ 9.1234e-02, -3.8131e-02, -4.0297e-02],\n                        [ 2.0156e-01,  4.4444e-02, -1.7387e-02],\n                        [-1.2983e-02, -3.7384e-02,  5.1094e-02]],\n              \n                       ...,\n              \n                       [[ 2.8947e-02, -1.3864e-01,  8.2294e-02],\n                        [-7.7662e-03,  4.5403e-02,  3.9325e-02],\n                        [-2.6425e-01, -5.3546e-02, -6.5024e-02]],\n              \n                       [[ 2.2620e-01,  8.1628e-02, -4.2840e-02],\n                        [ 1.1019e-01, -8.2268e-02, -1.6521e-01],\n                        [ 3.1312e-02, -1.4900e-01, -6.8589e-02]],\n              \n                       [[-2.1834e-01,  1.9049e-02,  9.4926e-02],\n                        [-3.0453e-01, -1.1234e-01,  1.5095e-02],\n                        [-6.5826e-02, -5.3969e-02,  8.6989e-02]]],\n              \n              \n                      [[[ 1.4354e-01,  9.2410e-03,  2.9152e-04],\n                        [ 1.9693e-01,  6.6979e-02,  3.8277e-02],\n                        [-9.3732e-03, -5.8049e-02,  4.3855e-02]],\n              \n                       [[-7.1180e-02, -7.7812e-02, -1.1623e-01],\n                        [-6.1406e-02,  3.6192e-04, -2.8839e-02],\n                        [-1.0804e-02,  8.3165e-02, -9.6328e-03]],\n              \n                       [[-1.1715e-01, -9.8959e-02, -6.9721e-02],\n                        [-5.6792e-02, -1.8020e-01, -1.3432e-01],\n                        [-7.3122e-02, -1.0812e-02, -2.0273e-02]],\n              \n                       ...,\n              \n                       [[-3.3479e-02, -3.1669e-02, -1.0374e-01],\n                        [-2.5539e-02,  2.5708e-02, -2.1022e-02],\n                        [ 1.5304e-01,  1.2970e-01, -4.0874e-02]],\n              \n                       [[-2.8740e-01, -2.0633e-02, -5.3960e-02],\n                        [-3.2228e-01, -1.1847e-01, -3.9562e-02],\n                        [-5.7311e-02,  7.6319e-02,  1.6245e-01]],\n              \n                       [[-4.1908e-02,  1.4111e-01,  1.0997e-01],\n                        [-3.1988e-02,  1.8253e-01,  6.5342e-02],\n                        [ 6.2978e-02,  8.5975e-02,  3.4569e-02]]],\n              \n              \n                      [[[ 1.7469e-01,  2.6340e-01,  3.5081e-02],\n                        [ 6.2666e-02, -1.0587e-01, -1.9374e-01],\n                        [-4.2697e-02, -1.4256e-01, -9.0259e-02]],\n              \n                       [[-4.0190e-03, -5.4520e-02, -9.7890e-02],\n                        [-5.6244e-02, -6.1809e-02,  1.9212e-02],\n                        [ 1.2836e-01,  8.1380e-02,  7.8316e-02]],\n              \n                       [[ 1.5420e-01,  1.0840e-01,  2.4947e-01],\n                        [ 1.9587e-01,  1.2778e-01,  8.1217e-02],\n                        [-2.7344e-02,  1.7789e-02,  1.3190e-01]],\n              \n                       ...,\n              \n                       [[ 2.3762e-02,  5.2759e-03, -2.0341e-02],\n                        [ 1.6843e-02,  4.0751e-02, -1.1272e-01],\n                        [ 3.6883e-02, -1.2929e-01, -1.5895e-01]],\n              \n                       [[-1.6662e-01, -2.0482e-01, -2.1994e-01],\n                        [-3.2553e-02, -1.4733e-01, -6.8205e-02],\n                        [ 1.5789e-01,  9.3416e-02,  1.0225e-03]],\n              \n                       [[-8.8435e-02, -1.7084e-02,  4.5601e-02],\n                        [ 1.2211e-02,  3.2117e-03,  3.4223e-02],\n                        [-5.8950e-02, -8.6452e-02, -9.7756e-03]]]])),\n             ('layer4.1.bn1.weight',\n              tensor([0.7459, 0.7004, 0.6283, 0.6788, 0.6986, 0.7565, 0.6568, 0.7227, 0.6626,\n                      0.7412, 0.6536, 0.7804, 0.7555, 0.7430, 0.7838, 0.6861, 0.7470, 0.6756,\n                      0.5938, 0.6198, 0.6207, 0.5270, 0.8839, 0.6961, 0.5928, 0.7244, 0.6573,\n                      0.8157, 0.8036, 0.7151, 0.6513, 0.8673, 0.7956, 0.7890, 0.7253, 0.7829,\n                      0.6947, 0.7595, 0.7730, 0.7390, 0.6221, 0.6719, 0.5921, 0.6715, 0.5047,\n                      0.6360, 0.8319, 0.6888, 0.6046, 0.6940, 0.6913, 0.6936, 0.6950, 0.7984,\n                      0.6930, 0.7655, 0.6929, 0.7103, 0.6690, 0.6401, 0.7016, 0.7789, 0.5781,\n                      0.7774, 0.7970, 0.6680, 0.7324, 0.8370, 0.7405, 0.6090, 0.8319, 0.5923,\n                      0.9742, 0.6593, 0.7200, 0.6604, 0.7353, 0.6068, 0.7863, 0.7816, 0.6472,\n                      0.6096, 0.7896, 0.5398, 0.7923, 0.6576, 0.6396, 0.7687, 0.9117, 0.8135,\n                      0.6034, 0.8000, 0.6589, 0.7280, 0.7269, 0.8052, 0.7447, 0.6845, 0.6624,\n                      0.6964, 0.7416, 0.7722, 0.8657, 0.6611, 1.0126, 0.8341, 0.6824, 0.5831,\n                      0.8575, 0.6198, 0.6657, 0.7636, 0.6048, 0.9008, 0.9136, 0.7318, 0.7931,\n                      0.6479, 0.6277, 0.6638, 0.7275, 0.6157, 0.8807, 0.7004, 0.7236, 0.8526,\n                      0.7199, 0.6461, 0.8213, 0.6260, 0.7248, 0.8303, 0.7219, 0.6094, 0.7342,\n                      0.6879, 0.6165, 0.8070, 0.6177, 0.6394, 0.6621, 0.7639, 0.7824, 0.6497,\n                      0.6214, 0.6326, 0.7441, 0.6494, 0.8467, 0.7605, 0.6516, 0.7435, 0.7157,\n                      0.7363, 0.6691, 0.7200, 0.8520, 0.7876, 0.6070, 0.8063, 0.8797, 0.8215,\n                      0.6552, 0.6756, 0.9803, 0.7340, 0.7877, 0.6343, 0.6957, 0.7542, 0.7564,\n                      0.6708, 0.6696, 0.6791, 0.8366, 0.7318, 0.8347, 0.7945, 0.8937, 0.7639,\n                      0.6085, 0.7893, 0.8854, 0.7637, 0.7670, 0.7437, 0.6587, 0.5546, 0.6256,\n                      0.6834, 0.7561, 0.5882, 0.8268, 0.6651, 0.6495, 0.7104, 0.6566, 0.6621,\n                      0.7994, 0.6212, 0.7304, 0.7440, 0.6779, 0.6942, 0.6203, 0.6634, 0.9308,\n                      0.6453, 0.6282, 0.7219, 0.6779, 0.7079, 0.8563, 0.8680, 0.8794, 0.7074,\n                      0.7477, 0.7156, 0.5851, 0.8024, 0.7448, 0.7384, 0.5663, 0.7620, 0.5581,\n                      0.7337, 0.8251, 0.7475, 0.8473, 0.6444, 0.7324, 0.7106, 0.8902, 0.6059,\n                      0.7089, 0.7477, 0.6944, 0.6672, 0.9163, 0.6515, 0.7671, 0.7139, 0.7372,\n                      0.8016, 0.6347, 0.6669, 0.7282, 0.6808, 0.6656, 0.6609, 0.6922, 0.6753,\n                      0.7593, 0.7666, 0.6508, 0.6489, 0.6625, 0.8447, 0.8503, 0.7293, 0.6754,\n                      0.6657, 0.9269, 0.6546, 0.6993, 0.5845, 0.7910, 0.7235, 0.6954, 0.6694,\n                      0.8004, 0.6375, 0.6252, 0.7449, 0.7715, 0.6351, 0.6562, 0.7467, 0.6734,\n                      0.6618, 0.6652, 0.6396, 0.8335, 0.6477, 0.7121, 0.7738, 0.6851, 0.6472,\n                      0.6972, 0.7489, 0.8807, 0.6662, 0.9850, 0.5960, 0.8084, 0.6856, 0.6871,\n                      0.9155, 0.7343, 0.7987, 0.6625, 0.8239, 0.6531, 0.8367, 0.8259, 0.7231,\n                      0.7422, 0.6583, 0.6703, 0.5834, 0.7048, 0.6988, 0.7041, 0.5475, 0.5562,\n                      0.6657, 0.8400, 0.7237, 0.7603, 0.6258, 0.6785, 0.6319, 0.7724, 0.6332,\n                      0.7013, 0.7989, 0.8157, 0.6157, 0.8226, 0.7426, 0.8201, 0.6950, 0.7202,\n                      0.7298, 0.7654, 0.8149, 0.7708, 0.6759, 0.6736, 0.6289, 0.6302, 0.7742,\n                      0.7538, 0.7635, 0.6273, 0.7613, 0.6880, 0.7578, 0.7446, 0.6965, 0.6971,\n                      0.9772, 0.7300, 0.7801, 0.6634, 0.7651, 0.6587, 0.6099, 0.7434, 0.7156,\n                      0.7950, 0.6778, 0.7027, 0.7194, 0.6849, 0.8308, 0.6833, 0.7472, 0.7441,\n                      0.5903, 0.8173, 0.8138, 0.6408, 0.7148, 0.7442, 0.6366, 0.7009, 0.7160,\n                      0.8253, 0.6901, 0.9338, 0.8183, 0.7678, 0.8667, 0.6598, 0.9240, 0.7364,\n                      0.6378, 0.7410, 0.7340, 0.7023, 0.6398, 0.6560, 0.6280, 0.7849, 0.6527,\n                      0.6998, 0.6384, 0.6787, 0.7612, 0.8357, 0.6791, 0.7362, 0.7286, 0.7750,\n                      0.7460, 0.6742, 0.8951, 0.6009, 0.7635, 0.6589, 0.7095, 0.6167, 0.7169,\n                      0.9282, 0.6364, 0.7186, 0.5271, 0.7576, 0.7760, 0.8143, 0.8118, 0.7797,\n                      0.6883, 0.8337, 0.6484, 0.6924, 0.7615, 0.7173, 0.6585, 0.6433, 0.6956,\n                      0.7033, 0.8132, 0.5700, 0.8259, 0.7772, 0.6041, 0.6068, 0.7391, 0.8038,\n                      0.7949, 0.7274, 0.6085, 0.7003, 0.7724, 0.8053, 0.5968, 0.7614, 0.6469,\n                      0.7045, 0.8199, 0.8353, 0.9092, 0.8623, 0.6682, 0.8216, 0.7272, 0.8030,\n                      0.6628, 0.6313, 0.7429, 0.7241, 0.8791, 0.7215, 0.7889, 0.7906, 0.6920,\n                      0.5904, 0.6718, 0.6427, 0.7762, 0.6203, 0.7206, 0.6679, 0.5894, 0.5669,\n                      0.8554, 0.7134, 0.7385, 0.5300, 0.8500, 0.6599, 0.6906, 0.5169, 0.9588,\n                      0.6989, 0.6520, 0.7574, 0.9378, 0.7958, 0.6947, 0.5818, 0.7616, 0.5989,\n                      0.6639, 0.6534, 0.7579, 0.9046, 0.5180, 0.7359, 0.5997, 0.7203, 0.7547,\n                      0.8649, 0.8620, 0.7399, 0.9384, 0.6571, 0.7623, 0.6359, 0.7346])),\n             ('layer4.1.bn1.bias',\n              tensor([-0.6149, -0.5857, -0.5724, -0.6488, -0.5535, -0.5698, -0.5010, -0.5717,\n                      -0.5274, -0.6952, -0.4340, -0.6236, -0.7010, -0.5776, -0.5511, -0.4057,\n                      -0.4680, -0.6887, -0.5595, -0.4599, -0.5515, -0.5326, -0.5958, -0.5559,\n                      -0.5331, -0.5455, -0.5669, -0.2318, -0.6679, -0.4734, -0.5018, -0.6841,\n                      -0.2008, -0.4959, -0.4627, -0.6497, -0.6515, -0.5472, -0.3803, -0.5950,\n                      -0.5598, -0.5195, -0.4020, -0.6215, -0.5041, -0.2325, -0.6631, -0.4250,\n                      -0.4850, -0.4819, -0.7174, -0.5077, -0.5463, -0.0399, -0.5566, -0.6694,\n                      -0.5567, -0.6503, -0.5201, -0.5901, -0.4519, -0.7281, -0.4608, -0.5182,\n                      -0.6164, -0.5958, -0.5949, -0.7973, -0.6023, -0.4209, -0.5327, -0.4794,\n                      -0.7847, -0.4410, -0.5800, -0.4146, -0.7521, -0.5333, -0.5112, -0.5243,\n                      -0.5613, -0.3355, -0.6902, -0.3933, -0.5931, -0.4106, -0.4196, -0.6517,\n                      -0.6018, -0.6255, -0.4033, -0.5130, -0.5770, -0.5326, -0.3257, -0.7010,\n                      -0.7325, -0.5469, -0.5098, -0.5135, -0.5975, -0.5587, -0.5949, -0.5145,\n                      -0.7800, -0.4484, -0.2785, -0.4491, -0.7156, -0.6384, -0.6902, -0.5507,\n                      -0.5744, -0.6862, -0.8793, -0.5046, -0.5276, -0.4850, -0.2790, -0.5667,\n                      -0.5164, -0.4498, -0.4452, -0.7716, -0.6689, -0.5650, -0.5965, -0.3453,\n                      -0.7446, -0.5362, -0.6922, -0.3875, -0.5691, -0.5919, -0.4221, -0.4781,\n                      -0.5205, -0.5989, -0.5785, -0.3983, -0.5739, -0.5142, -0.4795, -0.6193,\n                      -0.4496, -0.5849, -0.5233, -0.5566, -0.5380, -0.5297, -0.4740, -0.5266,\n                      -0.6491, -0.5824, -0.4194, -0.5139, -0.7121, -0.4015, -0.4565, -0.7491,\n                      -0.6734, -0.6484, -0.6622, -0.5617, -0.5406, -0.5970, -0.6338, -0.4377,\n                      -0.5576, -0.5381, -0.5176, -0.4574, -0.5445, -0.4136, -0.7508, -0.4623,\n                      -0.7094, -0.6870, -0.6344, -0.5682, -0.4033, -0.6974, -0.3431, -0.4810,\n                      -0.5785, -0.4614, -0.6595, -0.3656, -0.4537, -0.5863, -0.6652, -0.6142,\n                      -0.5982, -0.6612, -0.5348, -0.5121, -0.5230, -0.5523, -0.5829, -0.5134,\n                      -0.4761, -0.7449, -0.4953, -0.4685, -0.5427, -0.6447, -0.7066, -0.4872,\n                      -0.4871, -0.6137, -0.7182, -0.5569, -0.6144, -0.5066, -0.3208, -0.6866,\n                      -0.5507, -0.4401, -0.5146, -0.7204, -0.5891, -0.6483, -0.5309, -0.4297,\n                      -0.4952, -0.4114, -0.6099, -0.5168, -0.7173, -0.7012, -0.5706, -0.7301,\n                      -0.5413, -0.5649, -0.7713, -0.5854, -0.5281, -0.6444, -0.7692, -0.5113,\n                      -0.5812, -0.5575, -0.6082, -0.5374, -0.4546, -0.4395, -0.5016, -0.6359,\n                      -0.5213, -0.6996, -0.5861, -0.6291, -0.5198, -0.6230, -0.2742, -0.4615,\n                      -0.5235, -0.5973, -0.6538, -0.6103, -0.6125, -0.5623, -0.6070, -0.6199,\n                      -0.3628, -0.3414, -0.6187, -0.5982, -0.5116, -0.4578, -0.6085, -0.4606,\n                      -0.5225, -0.6090, -0.5065, -0.4518, -0.6189, -0.5319, -0.5697, -0.3689,\n                      -0.4327, -0.5526, -0.6382, -0.3664, -0.5762, -0.5872, -0.5541, -0.5753,\n                      -0.6146, -0.1456, -0.4545, -0.4452, -0.7651, -0.4341, -0.5920, -0.3537,\n                      -0.5985, -0.4642, -0.5281, -0.4435, -0.2778, -0.4374, -0.4986, -0.5849,\n                      -0.4582, -0.5611, -0.6337, -0.5188, -0.2599, -0.3558, -0.3995, -0.5712,\n                      -0.6252, -0.4510, -0.5082, -0.4815, -0.6010, -0.3889, -0.6328, -0.4559,\n                      -0.5121, -0.4722, -0.6494, -0.6288, -0.5439, -0.7106, -0.6003, -0.5629,\n                      -0.5188, -0.5718, -0.3161, -0.5824, -0.5286, -0.6623, -0.6018, -0.5471,\n                      -0.4871, -0.5763, -0.6019, -0.4456, -0.5387, -0.5429, -0.5152, -0.4373,\n                      -0.5797, -0.5702, -0.5027, -0.5175, -0.4092, -0.5164, -0.6378, -0.6190,\n                      -0.7741, -0.6718, -0.4956, -0.5742, -0.4907, -0.5623, -0.4032, -0.5800,\n                      -0.5469, -0.6631, -0.6424, -0.6235, -0.5847, -0.5147, -0.6276, -0.6776,\n                      -0.6414, -0.5417, -0.7100, -0.6884, -0.4267, -0.6934, -0.4906, -0.5699,\n                      -0.5087, -0.6616, -0.6116, -0.5908, -0.6917, -0.5157, -0.2820, -0.6733,\n                      -0.5344, -0.5928, -0.6398, -0.6153, -0.5307, -0.8352, -0.6012, -0.6650,\n                      -0.4267, -0.6230, -0.4547, -0.5856, -0.5170, -0.5851, -0.5804, -0.4735,\n                      -0.4923, -0.5984, -0.6781, -0.4657, -0.4598, -0.6395, -0.3179, -0.6404,\n                      -0.4888, -0.6615, -0.4788, -0.4289, -0.4289, -0.6182, -0.6456, -0.5993,\n                      -0.4972, -0.4190, -0.6801, -0.7240, -0.6267, -0.6887, -0.6502, -0.6741,\n                      -0.6746, -0.5768, -0.6683, -0.4655, -0.8161, -0.6024, -0.4571, -0.6359,\n                      -0.6446, -0.6114, -0.5439, -0.6316, -0.6675, -0.5810, -0.6622, -0.6061,\n                      -0.6678, -0.6666, -0.4987, -0.5982, -0.6987, -0.6657, -0.5251, -0.5911,\n                      -0.4073, -0.5260, -0.5337, -0.6946, -0.6297, -0.5844, -0.6535, -0.3960,\n                      -0.6893, -0.4243, -0.2280, -0.4174, -0.6110, -0.5416, -0.5490, -0.8209,\n                      -0.6098, -0.6613, -0.5834, -0.5915, -0.4738, -0.4350, -0.4321, -0.5769,\n                      -0.5335, -0.4602, -0.6091, -0.4063, -0.5305, -0.6164, -0.7260, -0.5093,\n                      -0.6047, -0.6882, -0.4272, -0.5529, -0.5469, -0.6844, -0.6426, -0.6041,\n                      -0.7238, -0.5674, -0.7345, -0.4275, -0.4601, -0.8663, -0.4629, -0.4281,\n                      -0.5626, -0.4297, -0.1798, -0.4562, -0.5836, -0.4276, -0.5475, -0.5265,\n                      -0.6234, -0.6118, -0.4869, -0.5389, -0.3711, -0.5787, -0.5691, -0.6009])),\n             ('layer4.1.bn1.running_mean',\n              tensor([-4.2691, -4.8796, -3.3988, -2.7245, -3.4174, -3.1188, -3.8433, -2.3015,\n                      -0.6933, -2.1173, -4.2978, -4.3623, -3.5606, -2.2323, -2.0781, -2.6786,\n                      -6.0014,  1.0967, -4.7050, -5.2780, -3.9029, -2.1960, -2.7691, -2.4997,\n                      -2.7446, -3.4448, -3.0423, -4.5526, -1.8871, -5.2686, -3.6313, -1.5512,\n                      -3.7142, -2.9846, -4.8273, -3.3372, -2.7920, -4.4827, -6.1213, -4.9127,\n                      -4.0322, -3.2687, -4.8138, -3.5263,  1.3968, -4.3626, -4.9628, -3.5951,\n                      -5.1789, -3.9597, -1.5778, -3.7621, -2.2978, -5.5018, -3.6934, -3.4962,\n                      -3.4902, -4.7892, -2.5877, -0.4218, -3.8583, -1.7277, -4.9115, -4.7751,\n                      -4.0963, -1.8508, -2.0016, -1.2244, -3.6144, -3.6643, -3.8647, -4.6283,\n                      -2.4235, -5.7944, -4.4576, -3.1528, -0.7428, -2.3903, -4.4980, -3.3192,\n                      -3.7866, -6.0231, -1.9799, -3.3576, -4.2360, -2.3004, -3.5398, -3.8958,\n                      -3.0137, -2.9993, -3.2669, -3.6675, -4.3993, -3.7539, -4.7818, -2.7568,\n                      -2.7179, -1.7869, -3.4227, -3.7435, -4.4327, -4.7417, -2.0173, -2.1893,\n                      -2.4511, -5.2625, -5.5037, -3.7762, -3.0441, -1.3673,  0.5658, -3.1995,\n                      -2.5883, -3.4025, -3.2573, -2.7461, -3.8437, -5.4819, -5.4149, -1.8113,\n                      -4.0612, -4.1830, -3.1346,  2.2184, -1.8445, -3.5532, -4.3206, -4.3070,\n                      -3.1277, -2.4903, -1.7258, -4.1796, -4.3779, -4.0855, -2.1571, -3.5795,\n                      -2.1112, -1.7966, -2.3045, -5.1481, -3.0199, -1.8547, -5.5143, -3.4372,\n                      -3.5937, -1.3013, -3.8522, -2.6228, -3.5817, -2.7649, -2.5476, -3.8646,\n                      -3.0094, -5.6379, -4.4973, -3.5441, -2.5142, -5.0448, -4.3914, -4.3324,\n                      -2.7547, -3.8912, -2.7226, -4.2117, -3.7912, -3.4883, -3.1346, -2.8718,\n                      -1.5387, -2.6100, -3.8681, -3.8700, -4.1260, -3.7116, -2.4951, -4.2564,\n                      -0.6218, -1.8036, -4.5007, -3.4894, -5.0055, -2.7517, -3.9602, -3.3452,\n                      -4.0555, -4.2176, -3.2266, -5.5132, -3.7324, -3.5618, -2.4299, -0.2839,\n                      -3.0390, -0.4827, -1.5430, -2.9027, -2.3605, -0.8924, -4.1295, -2.5955,\n                      -2.2922, -2.9387, -3.7937, -3.7378, -3.3970, -3.0991, -2.6336, -2.9455,\n                      -3.2694, -3.0538,  1.5456, -5.8527, -3.9418, -3.5006, -4.4212, -1.2699,\n                      -1.7691, -4.6477, -4.1522, -2.2943, -1.1412, -1.4828, -3.2640, -3.1603,\n                      -1.5625, -4.0855, -4.0315, -2.4906, -3.8821,  0.3685, -3.4577, -2.4957,\n                      -2.7239, -2.9781,  0.3072, -3.1262, -4.2378, -2.2082, -3.8253, -2.9669,\n                      -3.6924, -1.4959, -2.3062, -5.2944, -1.7175, -6.3426, -2.4039, -2.2406,\n                      -4.0100,  1.7713, -4.7833, -4.8280, -3.9124, -4.4525, -4.7849, -5.3329,\n                      -4.3593, -4.7029, -1.3506, -3.6521, -0.4697, -3.4964, -2.3218, -3.0723,\n                      -2.9957, -4.0934, -2.6412,  0.8006, -5.3873, -3.9571, -1.8597, -4.2283,\n                      -3.4456, -1.2882, -1.6638, -1.9214, -3.7525, -3.6229, -2.8293, -5.0143,\n                      -5.6118, -1.0600, -3.4615, -3.9694, -2.9853, -1.9183, -3.1022, -3.8249,\n                      -3.1503, -4.8166, -3.9940, -3.5397, -2.9925, -5.2113, -2.2240, -4.6149,\n                      -3.5411, -3.0805, -3.4928, -4.0917, -5.0498, -5.5223, -3.6548, -4.3472,\n                      -5.7005, -5.3058, -2.9602, -2.4891, -5.2185, -3.2711, -3.2553, -1.4394,\n                      -2.3692, -3.1387, -3.8041, -2.8553, -4.8625, -5.0335, -1.3756, -4.3678,\n                      -2.8776, -3.7866, -2.9351, -2.0802, -2.6473, -3.3840, -3.8415, -0.4980,\n                      -3.5995, -4.2966, -3.2088, -5.2345, -0.3088, -1.5281, -4.1329, -4.8377,\n                      -3.3922, -5.2555, -3.2344, -3.7296, -2.3673, -3.2963, -3.8746, -0.8427,\n                      -1.6900, -1.8125, -5.0657, -3.4701, -4.3299, -3.0216, -3.8383, -2.9170,\n                       0.4408, -2.5475, -3.5652, -3.4501, -3.7885, -1.0490, -3.7592, -4.9348,\n                      -4.0409, -2.2757, -4.3035, -2.0734, -2.4834, -3.1212, -0.2546, -3.3781,\n                      -3.8620, -3.5620, -3.4256, -3.9002, -4.5015, -1.8608, -4.2046, -3.4401,\n                      -3.8551, -1.0804, -3.2144, -3.1394, -3.7064, -2.6583, -3.9013, -3.1686,\n                      -1.9973, -3.6464, -3.8006, -2.7434, -2.9103,  0.0418, -2.5673, -0.5726,\n                      -1.6040, -1.6670, -3.9577, -3.6935, -2.7491, -2.3121, -3.8486, -4.1272,\n                      -4.1469, -3.4353, -1.6824, -3.3683, -3.9863, -3.0778, -3.5696, -2.9954,\n                      -4.4346, -3.7409, -3.9190, -2.6422, -1.4525, -2.8414, -2.2526, -4.1493,\n                      -3.4195, -1.2641, -3.3729, -2.1660, -3.6675, -3.2837, -3.7814, -3.5549,\n                      -2.5233, -4.2416,  1.6662, -3.9561,  0.7463, -2.4719, -5.3544, -2.4938,\n                      -2.5743, -2.0841, -0.8156, -3.2323, -2.8455, -3.1493, -0.3582, -4.9845,\n                      -2.8631, -2.6563, -4.8930, -2.4008, -2.3043, -2.9291, -3.4275, -0.3834,\n                      -2.7400, -2.1567, -5.1375, -4.8708, -2.1669, -3.7894, -2.7363, -5.5867,\n                      -2.8887, -3.4423, -5.3443, -2.9653, -4.0718, -2.9354, -2.0935, -2.3285,\n                      -4.0537, -2.8019, -2.2451, -5.0301, -4.2160, -5.8986, -3.7728, -3.9340,\n                      -3.7546, -3.6142,  0.5336, -4.3402, -1.8024, -2.1997, -0.8516, -5.1829,\n                       0.7844, -2.5274, -4.9830, -3.9874, -1.8771, -2.8402, -0.7912, -2.1093,\n                      -1.6759, -3.4843, -1.8825, -4.5413, -3.3513, -1.9586, -3.4625, -4.7316,\n                      -2.5187, -4.9097, -5.8731, -2.7597, -2.9901, -4.9874, -4.6371, -4.9907,\n                      -2.4292, -1.4767, -5.0648, -3.3355, -5.5501, -2.0849, -3.8606, -3.4001])),\n             ('layer4.1.bn1.running_var',\n              tensor([10.7182, 11.7585, 12.6332,  9.6418,  8.8029, 11.7717, 16.9340, 10.5621,\n                       8.8913,  8.4725, 16.6335, 11.0009, 10.8009,  9.2159,  9.0966, 13.4616,\n                      16.1530,  8.7578, 12.1026, 14.7900,  9.8196, 10.7636,  8.8093, 10.2027,\n                       8.4758, 11.7927, 12.8423, 22.2373,  9.8740, 13.1799, 13.1219,  9.0107,\n                      16.7875, 10.6945, 15.1057,  9.3016,  9.9827, 11.4256, 15.2000,  9.7209,\n                       8.4988, 15.0487, 13.0259, 10.4597, 16.2468, 21.3591, 12.8099, 11.5083,\n                      11.6988, 11.5160,  7.8455,  9.3971, 10.4396, 31.5266,  9.0352,  8.7035,\n                      12.1825, 10.0271, 11.0955,  7.8356, 13.0947,  9.2033, 16.4000, 14.8925,\n                      10.7242,  9.6481,  9.1013,  7.5959, 11.7107, 11.8905, 13.1317, 11.5961,\n                      11.7261, 13.3007, 10.4809, 12.0808,  7.2213, 10.6251, 11.4791, 12.6208,\n                       9.9451, 17.3994,  8.4211,  9.9556, 10.7531, 13.9637, 11.5263, 10.3954,\n                      11.2501,  9.0128, 11.4271, 12.4379, 10.2904,  9.6841, 15.7156,  9.4643,\n                      10.1531, 11.1403,  9.7341,  9.7214,  9.6967, 10.4131,  9.0695, 12.7514,\n                       9.6517, 12.3278, 25.9882, 10.0066,  9.4499, 10.6999,  7.3683, 10.3180,\n                      10.4957, 10.9855,  8.2810, 11.1764, 10.6374, 12.8219, 20.7018, 11.1137,\n                      10.9982, 12.5825, 10.3363, 11.1217,  9.7106,  9.9988, 12.1570, 15.9498,\n                      11.2601,  8.7831, 12.9159, 13.8286, 11.6887, 10.5247, 12.8407, 11.7163,\n                       9.9591, 13.4140,  8.4326, 16.6623, 10.9479, 11.0347, 16.5238, 11.7821,\n                      11.2762, 10.3362, 10.3054, 10.8420, 12.2013,  8.7014, 11.1229, 11.3402,\n                       9.5042, 12.2083, 10.9672,  9.2447,  9.4357, 11.8684, 15.5912,  9.4865,\n                       8.7855, 14.1748,  9.4887,  9.3518, 10.8886, 10.6623,  8.3307, 11.6387,\n                      10.1852, 11.0477, 11.2711, 13.2245, 13.7045,  9.6553,  7.8706, 14.5187,\n                       8.5357,  8.5016, 13.6546, 10.5479, 13.2474,  8.5305, 16.8760, 11.4065,\n                      10.9179, 11.0318, 13.0840, 14.1260, 12.8774, 10.2333,  9.2906,  9.7703,\n                      11.5414,  8.9874, 11.1704, 12.1610,  8.2700,  8.3211, 11.6408, 10.3952,\n                      12.0734,  8.6187, 12.0261, 12.0094,  9.1627,  8.4763, 11.2671, 11.3975,\n                      10.2288,  8.8262,  7.5022, 13.3648, 14.8604, 12.4827, 15.0590,  8.2980,\n                      13.0908, 11.7414,  9.6184,  8.7328,  9.8111, 11.9554, 10.6311, 14.6456,\n                       9.1966, 15.8707, 10.6369, 10.8144, 12.8777,  8.4025,  9.8763,  8.7717,\n                      12.3679, 10.7710,  7.4430,  9.5162, 14.4801, 10.9438, 11.3334,  9.3168,\n                      13.6525, 13.7963, 10.5608, 16.5960, 15.4710, 18.8804, 15.2578,  8.6693,\n                      14.3187,  9.1232, 12.7955,  9.5318, 11.7379, 11.3172, 19.9057, 16.4479,\n                      11.8268, 11.5229, 10.7380, 10.0982,  9.0392, 11.3213,  9.4780,  9.9860,\n                      15.6893, 14.7313, 10.3183,  8.6725, 12.3414, 11.3551, 11.1272, 12.8839,\n                      11.6773,  9.1094, 10.3333, 14.0070, 10.4435, 14.2296, 10.8318, 13.1388,\n                      16.4772, 10.7775, 12.4767, 14.3194,  9.2840,  9.0565, 10.8983, 11.9247,\n                      12.5640, 22.7571, 17.3512, 13.4680, 11.6302, 11.7443,  8.8643, 15.1986,\n                      10.5397, 12.1584,  9.1941, 14.4396, 17.1945, 13.6052, 10.4468,  8.5643,\n                      12.6636, 12.1704,  9.6038,  8.3552, 22.0782, 12.9167, 13.7661,  8.1196,\n                       9.1586, 11.0128, 10.3132,  8.2986, 17.8200, 11.8477,  8.4258, 12.8473,\n                      12.0729, 12.0518,  9.4470,  9.4697, 12.1815, 10.9619, 10.9973, 10.5237,\n                      12.3748, 12.2216, 14.7472, 17.1254,  9.3715, 11.7071, 12.5682, 11.5649,\n                      14.3010, 10.9231, 12.3310, 15.2529,  8.9882, 11.8856, 10.4642,  9.9731,\n                       9.1180,  8.4828, 12.3605, 14.2169, 12.0065, 10.7894,  9.9395, 12.6884,\n                       8.2521, 11.5245, 14.4378, 11.6025, 11.3632, 10.9315, 13.1852, 11.8211,\n                      12.5697,  8.4621, 10.5919,  8.8444, 10.0683,  9.4765,  8.2650,  9.5487,\n                       9.1742, 11.6705, 10.9330, 10.4884, 12.5547,  9.8291, 11.3588, 10.9168,\n                      10.9135,  8.6129, 12.2320, 11.0980, 10.1469, 10.9082, 15.0625,  9.9326,\n                      10.0246, 10.9494,  9.3729,  8.2502, 10.1057,  7.9680, 10.9989,  8.2114,\n                      12.0108, 10.0293, 11.2855, 11.8035, 10.5892, 12.0528, 11.1521, 12.5226,\n                      14.2950, 10.5198,  9.0391, 15.9640, 11.3221,  8.5659, 15.1579,  8.9124,\n                      12.6243,  8.8603, 12.3836, 10.0497, 10.1612,  9.8284,  9.9271, 10.8384,\n                      10.5337, 10.9175, 10.3267,  7.1274, 10.2146, 10.9912, 11.6754, 10.3684,\n                       8.8229, 10.7457,  9.1243, 10.2870,  8.1212,  9.0486, 12.6741,  9.4780,\n                       7.4316, 10.0509,  9.6600,  9.2483,  9.4786, 12.0988,  8.8556, 10.2884,\n                       8.4124,  9.3646, 11.4639,  9.5440,  7.8975,  9.7741, 14.0307,  9.4313,\n                      13.8101,  9.9312, 16.3856, 12.6696, 10.0640, 11.2654, 12.9831, 14.3204,\n                      11.9680, 10.3978, 23.6570, 13.2772, 11.4513, 13.1916,  9.6805,  8.4793,\n                      11.7107, 12.9467, 10.3242, 10.1883, 11.1693, 15.4614, 16.6751, 11.8481,\n                      12.6623, 12.0851,  8.4339, 10.7196, 11.9687,  9.0260,  9.1724, 14.2140,\n                       9.9489,  8.1416, 13.1597, 10.3822, 10.1593, 11.4702, 10.1006, 10.3074,\n                       8.6423, 15.2882,  8.8412, 13.4112, 13.7338,  9.2551, 11.3701, 12.8119,\n                      10.1100, 13.1992, 21.5171, 10.8722,  8.4062, 10.4405, 12.4162, 11.7909,\n                       9.8532,  9.2438, 16.6583, 12.3548, 14.5918,  8.6002, 11.0098, 10.5219])),\n             ('layer4.1.bn1.num_batches_tracked', tensor(14770)),\n             ('layer4.1.conv2.weight',\n              tensor([[[[ 0.1620,  0.1648,  0.1030],\n                        [ 0.1190,  0.1646,  0.1232],\n                        [ 0.0598,  0.1155,  0.1027]],\n              \n                       [[-0.0632, -0.0923, -0.0953],\n                        [-0.0538, -0.1225, -0.1439],\n                        [-0.0013, -0.0648, -0.1046]],\n              \n                       [[-0.0346, -0.0022,  0.0297],\n                        [-0.0692, -0.0384, -0.0191],\n                        [-0.0215,  0.0046,  0.0162]],\n              \n                       ...,\n              \n                       [[-0.1558, -0.1448, -0.0908],\n                        [-0.2076, -0.2142, -0.2096],\n                        [-0.2626, -0.1790, -0.1550]],\n              \n                       [[-0.0229, -0.0295, -0.0240],\n                        [-0.0006,  0.0093,  0.0038],\n                        [ 0.0191,  0.0243, -0.0393]],\n              \n                       [[ 0.0289, -0.0286, -0.0445],\n                        [-0.0534, -0.0863, -0.1140],\n                        [-0.0506, -0.1453, -0.1073]]],\n              \n              \n                      [[[ 0.0056,  0.0340, -0.0042],\n                        [-0.0470, -0.0388, -0.0235],\n                        [-0.0592, -0.0608, -0.1093]],\n              \n                       [[-0.0026, -0.0738, -0.1151],\n                        [-0.0491, -0.1245, -0.1414],\n                        [-0.0752, -0.0896, -0.1430]],\n              \n                       [[ 0.1885,  0.1522,  0.1461],\n                        [ 0.0939,  0.0883,  0.0958],\n                        [ 0.0716,  0.0805,  0.0636]],\n              \n                       ...,\n              \n                       [[ 0.0307, -0.0202,  0.0014],\n                        [-0.0640, -0.0627, -0.0470],\n                        [-0.0995, -0.0770, -0.0835]],\n              \n                       [[-0.1096, -0.1264, -0.1798],\n                        [-0.1155, -0.1432, -0.2335],\n                        [-0.1746, -0.1104, -0.2266]],\n              \n                       [[ 0.0197,  0.0395,  0.0620],\n                        [-0.0636, -0.0116, -0.0233],\n                        [-0.1409, -0.1047, -0.0556]]],\n              \n              \n                      [[[ 0.1583,  0.2066,  0.2070],\n                        [ 0.1253,  0.1816,  0.1858],\n                        [-0.0030,  0.0569,  0.0583]],\n              \n                       [[ 0.1296,  0.0893,  0.1785],\n                        [ 0.1155,  0.0422,  0.0802],\n                        [ 0.2172,  0.1602,  0.2453]],\n              \n                       [[-0.2863, -0.2457, -0.2377],\n                        [-0.1755, -0.1324, -0.1138],\n                        [-0.1597, -0.0878, -0.0402]],\n              \n                       ...,\n              \n                       [[ 0.0073, -0.0021, -0.0109],\n                        [-0.0501, -0.0866, -0.0154],\n                        [-0.0882, -0.0394,  0.0176]],\n              \n                       [[-0.0488, -0.0120, -0.0841],\n                        [-0.0318, -0.0723, -0.1032],\n                        [-0.1077, -0.1163, -0.1383]],\n              \n                       [[ 0.0707,  0.0800, -0.0053],\n                        [ 0.0609,  0.0632,  0.0194],\n                        [-0.0115, -0.0123, -0.0037]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.0062,  0.0216,  0.0496],\n                        [-0.1398, -0.0869,  0.0048],\n                        [-0.2117, -0.1974, -0.1731]],\n              \n                       [[-0.0039, -0.0073,  0.0182],\n                        [-0.0909, -0.0651, -0.0109],\n                        [-0.1541, -0.0725, -0.1425]],\n              \n                       [[-0.0616, -0.1112, -0.1736],\n                        [-0.0143, -0.0836, -0.1600],\n                        [ 0.0185, -0.0277, -0.1086]],\n              \n                       ...,\n              \n                       [[-0.1549, -0.0865, -0.0972],\n                        [-0.1620, -0.1286, -0.0401],\n                        [-0.2460, -0.1612, -0.0996]],\n              \n                       [[-0.1002, -0.0684, -0.0783],\n                        [-0.0688, -0.0824, -0.1472],\n                        [-0.0953, -0.0822, -0.1686]],\n              \n                       [[ 0.1315,  0.1253,  0.1551],\n                        [ 0.1015,  0.1467,  0.1643],\n                        [ 0.0461,  0.0365,  0.1228]]],\n              \n              \n                      [[[ 0.0999,  0.0657,  0.0796],\n                        [ 0.0458,  0.0394,  0.0191],\n                        [ 0.0596,  0.0585,  0.0518]],\n              \n                       [[ 0.0962, -0.0069, -0.0065],\n                        [ 0.0574, -0.0266, -0.0397],\n                        [ 0.0097, -0.0079,  0.0649]],\n              \n                       [[ 0.0610,  0.0762,  0.1309],\n                        [ 0.0896,  0.0866,  0.1237],\n                        [ 0.1671,  0.1307,  0.1690]],\n              \n                       ...,\n              \n                       [[ 0.0169, -0.1024, -0.1103],\n                        [ 0.0086, -0.1166, -0.0677],\n                        [-0.0144, -0.0819, -0.0163]],\n              \n                       [[-0.0477, -0.0408, -0.0914],\n                        [-0.0059, -0.0198, -0.0481],\n                        [ 0.0024,  0.0062, -0.0407]],\n              \n                       [[ 0.0013, -0.0131,  0.0386],\n                        [-0.0865, -0.0604, -0.0285],\n                        [-0.0904, -0.0953, -0.1038]]],\n              \n              \n                      [[[-0.0070,  0.0118,  0.0795],\n                        [-0.0046,  0.0104,  0.0352],\n                        [ 0.0080,  0.0334,  0.0335]],\n              \n                       [[-0.2301, -0.2038, -0.1859],\n                        [-0.3513, -0.3575, -0.3402],\n                        [-0.2700, -0.3639, -0.3216]],\n              \n                       [[-0.1882, -0.0677, -0.0226],\n                        [-0.0831, -0.0106, -0.0220],\n                        [-0.0577,  0.0068,  0.0637]],\n              \n                       ...,\n              \n                       [[ 0.0647,  0.0017,  0.0123],\n                        [-0.0733, -0.1469, -0.0972],\n                        [-0.0842, -0.1706, -0.2103]],\n              \n                       [[ 0.0677,  0.0784,  0.0965],\n                        [ 0.1048,  0.1302,  0.1428],\n                        [ 0.0716,  0.1735,  0.1727]],\n              \n                       [[ 0.1189,  0.1011,  0.2017],\n                        [ 0.1873,  0.1587,  0.2202],\n                        [ 0.2072,  0.1787,  0.2164]]]])),\n             ('layer4.1.bn2.weight',\n              tensor([1.0884, 1.1667, 1.0521, 1.0898, 1.0645, 1.1203, 0.9253, 1.1526, 0.9788,\n                      1.0740, 0.9772, 1.0315, 1.0213, 1.0620, 1.2386, 0.9950, 1.0903, 0.9953,\n                      1.0571, 1.1067, 1.1803, 1.1417, 1.0761, 1.0777, 1.1347, 1.1909, 1.1653,\n                      1.0311, 0.9576, 1.0739, 1.1680, 1.0665, 1.0918, 1.0171, 1.0821, 1.0541,\n                      1.0643, 1.1218, 1.0941, 0.9990, 0.9938, 1.0683, 1.0814, 1.0939, 1.0226,\n                      1.0751, 1.0571, 0.9924, 1.1772, 1.2410, 1.1442, 1.0901, 1.1057, 1.0820,\n                      0.9913, 1.0066, 1.1841, 0.9405, 1.0624, 1.0494, 1.1819, 1.1311, 0.9645,\n                      1.0295, 1.0397, 1.1607, 1.0741, 1.1175, 1.0799, 1.1901, 1.1674, 1.1403,\n                      0.9916, 1.0659, 1.1174, 1.0758, 1.1066, 0.9846, 1.0613, 1.0391, 1.0473,\n                      1.1506, 0.9209, 1.0480, 1.2162, 0.9822, 1.0137, 1.1975, 0.9851, 1.0393,\n                      0.9249, 1.0223, 0.9550, 1.1284, 0.9797, 1.0042, 0.9866, 1.1081, 1.0367,\n                      1.0174, 1.0050, 0.9652, 1.1000, 1.1230, 0.9017, 1.1973, 1.0999, 1.0743,\n                      1.0961, 1.1684, 1.0787, 1.1255, 1.1137, 1.1150, 1.1472, 1.0066, 1.0684,\n                      1.1107, 1.0277, 1.0361, 1.0850, 1.0960, 1.1084, 1.1204, 1.0662, 1.0218,\n                      0.8856, 1.1541, 1.1633, 1.0839, 1.1482, 1.0471, 1.0529, 1.0742, 1.1492,\n                      1.1246, 0.9727, 1.3156, 1.1188, 1.1489, 1.1140, 1.0863, 1.0269, 1.0221,\n                      1.1224, 1.0111, 1.0335, 1.0327, 1.1092, 1.0548, 0.9917, 1.0576, 1.0331,\n                      1.1028, 1.1956, 1.1180, 1.0668, 0.9452, 1.0667, 1.0992, 1.0807, 1.0863,\n                      1.0320, 1.0579, 1.1442, 1.0323, 1.0793, 1.1080, 1.0321, 0.9984, 1.1709,\n                      1.0498, 1.0881, 1.1229, 0.9288, 1.1290, 0.9902, 1.1032, 1.0491, 1.0374,\n                      1.0320, 1.0118, 1.0616, 0.9661, 1.0504, 1.0710, 1.0978, 1.0327, 1.1175,\n                      1.0647, 1.0276, 0.9612, 1.1343, 1.0196, 1.1382, 1.0664, 1.0110, 0.9589,\n                      1.0218, 1.0026, 1.0305, 1.1360, 1.0634, 1.0648, 1.1082, 1.0561, 1.0637,\n                      0.9944, 0.9686, 1.0836, 0.9937, 0.9746, 1.0594, 0.9346, 1.0878, 0.9002,\n                      0.8835, 1.0382, 1.0779, 1.0945, 0.9637, 1.0251, 1.1016, 1.0432, 1.1249,\n                      1.0458, 1.0666, 1.1182, 1.0622, 0.9849, 1.0483, 1.0958, 1.0581, 1.0702,\n                      0.9780, 1.0239, 1.0868, 0.9656, 1.0942, 1.0406, 1.0783, 1.0879, 1.1383,\n                      1.2517, 1.1085, 1.1458, 0.9821, 1.0642, 1.1766, 1.0942, 1.0929, 1.0788,\n                      0.9669, 1.1096, 1.0827, 0.8980, 1.1024, 0.9421, 0.9839, 1.2139, 1.1854,\n                      0.9598, 1.0892, 1.0334, 1.0583, 1.1005, 1.1001, 0.9635, 1.0210, 1.0712,\n                      1.1111, 1.0900, 1.1969, 1.1720, 1.0540, 1.1457, 1.0533, 1.0525, 1.1451,\n                      0.9331, 1.0652, 1.0232, 0.9496, 1.0594, 1.2601, 1.0987, 1.0310, 1.1925,\n                      1.0157, 1.0672, 1.1201, 1.0423, 1.0917, 1.1516, 1.0371, 1.0061, 1.0820,\n                      1.0665, 1.1501, 1.0075, 0.9625, 1.1537, 0.9598, 1.1103, 1.1026, 1.0182,\n                      0.9215, 1.0911, 1.1517, 1.1185, 0.9135, 1.1580, 1.1121, 1.0727, 1.1020,\n                      1.0237, 1.1690, 1.0656, 1.1031, 1.0221, 1.2276, 1.2002, 0.9644, 1.1396,\n                      1.0472, 1.0365, 1.0763, 1.0295, 1.1392, 1.0072, 1.0851, 1.1078, 0.9623,\n                      1.1140, 1.2022, 1.0181, 1.0311, 1.0767, 0.9814, 0.9846, 1.0622, 1.1327,\n                      1.0256, 1.1281, 1.1154, 1.1711, 0.8773, 1.0988, 1.0981, 0.9911, 1.1248,\n                      1.0482, 1.2024, 1.0630, 0.8792, 1.0391, 1.0916, 0.9965, 1.1212, 1.0886,\n                      0.9849, 1.0196, 1.0970, 1.1195, 0.9469, 1.1139, 1.0694, 1.1929, 1.0386,\n                      1.0708, 1.0351, 1.0360, 1.1210, 1.1188, 1.2270, 1.1413, 1.1037, 1.1544,\n                      1.0305, 1.1067, 1.0572, 1.0494, 1.0999, 1.0296, 1.1179, 1.0538, 0.9914,\n                      0.9289, 1.0362, 1.0596, 1.0520, 1.0645, 1.0665, 1.0974, 1.0972, 1.0559,\n                      1.1659, 1.2051, 1.1080, 1.0035, 1.0106, 0.9802, 1.0180, 0.9098, 1.0030,\n                      0.9700, 1.0393, 1.1610, 1.1248, 1.0732, 0.9925, 1.0745, 1.0834, 1.0511,\n                      1.0651, 0.9616, 1.0895, 1.1732, 1.0817, 1.0848, 1.1107, 1.0075, 1.1833,\n                      1.1786, 0.9976, 1.1155, 0.9550, 1.0844, 1.0347, 1.2103, 1.0269, 1.0249,\n                      1.0421, 1.0883, 0.9358, 1.0506, 1.0364, 1.0074, 1.0325, 1.0926, 1.0757,\n                      1.0865, 1.0710, 1.1632, 1.0169, 1.1171, 1.1037, 1.0319, 1.0698, 1.0064,\n                      1.0539, 1.0819, 0.9390, 1.0372, 1.0386, 1.1253, 1.2404, 1.1544, 1.0898,\n                      0.9999, 1.0653, 0.9632, 1.0976, 1.0136, 1.1206, 1.0760, 1.1684, 1.0506,\n                      1.1958, 1.0928, 1.0564, 1.1817, 1.0168, 1.0155, 1.1021, 1.1238, 1.1155,\n                      0.9897, 0.9965, 1.0446, 0.9940, 1.0945, 1.1615, 1.0911, 1.1194, 0.9742,\n                      1.1051, 0.9920, 1.0620, 1.0697, 1.0982, 1.1268, 0.9946, 0.9922, 0.9957,\n                      1.1174, 1.0707, 1.1602, 1.0506, 1.0344, 1.0614, 1.0016, 1.1722, 0.8973,\n                      1.1263, 1.0813, 1.0265, 0.9385, 1.0148, 0.9951, 0.9907, 1.1628])),\n             ('layer4.1.bn2.bias',\n              tensor([-0.3193, -0.2645, -0.3015, -0.3004, -0.2852, -0.3426, -0.3198, -0.3322,\n                      -0.3598, -0.3491, -0.3119, -0.3602, -0.3336, -0.3130, -0.3660, -0.3137,\n                      -0.3582, -0.3678, -0.3277, -0.3108, -0.3066, -0.3543, -0.3764, -0.3339,\n                      -0.3386, -0.3251, -0.2676, -0.3534, -0.3506, -0.2693, -0.3820, -0.3378,\n                      -0.3520, -0.3344, -0.3651, -0.3604, -0.3079, -0.3311, -0.3001, -0.3818,\n                      -0.3461, -0.3315, -0.2446, -0.3029, -0.3422, -0.2952, -0.2868, -0.3404,\n                      -0.3360, -0.3496, -0.2845, -0.3281, -0.2870, -0.2837, -0.3083, -0.3515,\n                      -0.3169, -0.3403, -0.3145, -0.3148, -0.3310, -0.3072, -0.3781, -0.3828,\n                      -0.3598, -0.3825, -0.3786, -0.3250, -0.3431, -0.2979, -0.3030, -0.3681,\n                      -0.4227, -0.2927, -0.2888, -0.3836, -0.3354, -0.3141, -0.3056, -0.3244,\n                      -0.3398, -0.3152, -0.3105, -0.3617, -0.3404, -0.2998, -0.3827, -0.3237,\n                      -0.3224, -0.3679, -0.3361, -0.4091, -0.3968, -0.3065, -0.3703, -0.2668,\n                      -0.3182, -0.4385, -0.2787, -0.4038, -0.3247, -0.3310, -0.2956, -0.2644,\n                      -0.3834, -0.2690, -0.3449, -0.3419, -0.4025, -0.2904, -0.4123, -0.3686,\n                      -0.3050, -0.3048, -0.3112, -0.2964, -0.3012, -0.3473, -0.3723, -0.3389,\n                      -0.2960, -0.3297, -0.3576, -0.3509, -0.3642, -0.3248, -0.3060, -0.3681,\n                      -0.2702, -0.3209, -0.3279, -0.2543, -0.3175, -0.2672, -0.2902, -0.3688,\n                      -0.2876, -0.3724, -0.3639, -0.3097, -0.3406, -0.2983, -0.3258, -0.3542,\n                      -0.4018, -0.3271, -0.3482, -0.2334, -0.3260, -0.3622, -0.3099, -0.3326,\n                      -0.3677, -0.3471, -0.3162, -0.3264, -0.3850, -0.3367, -0.2871, -0.3130,\n                      -0.3123, -0.3476, -0.2937, -0.3028, -0.3645, -0.3050, -0.2616, -0.4075,\n                      -0.3828, -0.3532, -0.2901, -0.2620, -0.3131, -0.3325, -0.3763, -0.2823,\n                      -0.3605, -0.3637, -0.2674, -0.3454, -0.3815, -0.3156, -0.3403, -0.2848,\n                      -0.3647, -0.3785, -0.4431, -0.2411, -0.3770, -0.4060, -0.3333, -0.2806,\n                      -0.3245, -0.4091, -0.2850, -0.2718, -0.3845, -0.2631, -0.3833, -0.4578,\n                      -0.3427, -0.3294, -0.3452, -0.2977, -0.3495, -0.3215, -0.3832, -0.3648,\n                      -0.2726, -0.3766, -0.3418, -0.3206, -0.3443, -0.3032, -0.3209, -0.3124,\n                      -0.3388, -0.3603, -0.3135, -0.3367, -0.2923, -0.3188, -0.2732, -0.3548,\n                      -0.3838, -0.3071, -0.3339, -0.3855, -0.2649, -0.3552, -0.3041, -0.3875,\n                      -0.3565, -0.2906, -0.3115, -0.2983, -0.2911, -0.3650, -0.2793, -0.3506,\n                      -0.3628, -0.2802, -0.3197, -0.3769, -0.3434, -0.3190, -0.3913, -0.3270,\n                      -0.3285, -0.2912, -0.3355, -0.4135, -0.3273, -0.4057, -0.3188, -0.3495,\n                      -0.3783, -0.2878, -0.3725, -0.3500, -0.3286, -0.3421, -0.2566, -0.3073,\n                      -0.3935, -0.4003, -0.3331, -0.2966, -0.2686, -0.3702, -0.3318, -0.3737,\n                      -0.3167, -0.3105, -0.3754, -0.3025, -0.3042, -0.3151, -0.3256, -0.3380,\n                      -0.3707, -0.3441, -0.3079, -0.3114, -0.2937, -0.3887, -0.3671, -0.2985,\n                      -0.3454, -0.3239, -0.3342, -0.3538, -0.3041, -0.3826, -0.3075, -0.3420,\n                      -0.3413, -0.3498, -0.2668, -0.2841, -0.3396, -0.3138, -0.3548, -0.2618,\n                      -0.2509, -0.3176, -0.3538, -0.2838, -0.2730, -0.3376, -0.3508, -0.4048,\n                      -0.4187, -0.3333, -0.2859, -0.3782, -0.3431, -0.3241, -0.3161, -0.2487,\n                      -0.3536, -0.4053, -0.3115, -0.3348, -0.3407, -0.3142, -0.4013, -0.2577,\n                      -0.3458, -0.3169, -0.4133, -0.3212, -0.3098, -0.3515, -0.3141, -0.3278,\n                      -0.3651, -0.2690, -0.4004, -0.3698, -0.3883, -0.3011, -0.3480, -0.3188,\n                      -0.2776, -0.3912, -0.3150, -0.4075, -0.2692, -0.4019, -0.3080, -0.2780,\n                      -0.3473, -0.3521, -0.3271, -0.3692, -0.2996, -0.3111, -0.2824, -0.3278,\n                      -0.3055, -0.3000, -0.3537, -0.3349, -0.3282, -0.3091, -0.3110, -0.3598,\n                      -0.3480, -0.2893, -0.2707, -0.2470, -0.3004, -0.3563, -0.4459, -0.3302,\n                      -0.3750, -0.3293, -0.2810, -0.3629, -0.2883, -0.2479, -0.3428, -0.3491,\n                      -0.3732, -0.2947, -0.3882, -0.3073, -0.3571, -0.3003, -0.4222, -0.3335,\n                      -0.3496, -0.3728, -0.3159, -0.3000, -0.3563, -0.3618, -0.3629, -0.3383,\n                      -0.3280, -0.3942, -0.3280, -0.3181, -0.3819, -0.3257, -0.3377, -0.3300,\n                      -0.3665, -0.3401, -0.3471, -0.3399, -0.3534, -0.3086, -0.2714, -0.3512,\n                      -0.3096, -0.2908, -0.3158, -0.3487, -0.3053, -0.4035, -0.3236, -0.2909,\n                      -0.3056, -0.3433, -0.3265, -0.3754, -0.3336, -0.3766, -0.3825, -0.3816,\n                      -0.3199, -0.3672, -0.3101, -0.3795, -0.2782, -0.2819, -0.3194, -0.4480,\n                      -0.3158, -0.3278, -0.2114, -0.3180, -0.2996, -0.2951, -0.3002, -0.4101,\n                      -0.3411, -0.3759, -0.2817, -0.3883, -0.2936, -0.3128, -0.3417, -0.3027,\n                      -0.3493, -0.3133, -0.3777, -0.3585, -0.3671, -0.3220, -0.2942, -0.3861,\n                      -0.3420, -0.2474, -0.3106, -0.3296, -0.3372, -0.3363, -0.2953, -0.2894,\n                      -0.3234, -0.3092, -0.3286, -0.3822, -0.3641, -0.3225, -0.3031, -0.3235,\n                      -0.3346, -0.2950, -0.3586, -0.3496, -0.3381, -0.2933, -0.3187, -0.3375,\n                      -0.3353, -0.2891, -0.2242, -0.2897, -0.3724, -0.3877, -0.3126, -0.3103,\n                      -0.2832, -0.3454, -0.4038, -0.3307, -0.3268, -0.2704, -0.3564, -0.3905,\n                      -0.3156, -0.4187, -0.3353, -0.2669, -0.2983, -0.2754, -0.2974, -0.2821])),\n             ('layer4.1.bn2.running_mean',\n              tensor([-4.9753e+00, -4.8764e+00, -2.5693e+00, -4.9742e+00, -4.6362e+00,\n                      -5.5521e+00, -4.7745e+00, -6.7915e+00, -4.2659e+00, -4.7954e-01,\n                      -4.3747e+00, -2.7910e+00, -5.1000e+00, -4.9676e+00, -6.3387e+00,\n                      -4.6602e+00, -4.5002e+00, -5.0819e+00, -4.9518e+00, -5.2388e+00,\n                      -4.7957e+00, -3.4480e+00, -5.1191e+00, -4.6293e+00, -6.0889e+00,\n                      -8.1121e+00, -5.6787e+00, -7.7973e-01, -4.2687e+00, -4.1662e+00,\n                      -1.2417e+00, -5.6204e+00, -6.2434e+00, -2.7484e+00, -2.7601e+00,\n                      -4.1591e+00, -6.0623e+00, -4.0020e+00, -5.3400e+00, -5.2446e+00,\n                      -7.8131e+00, -5.4587e+00, -2.3001e+00, -7.1590e+00, -8.3215e+00,\n                      -5.6621e+00, -3.7202e+00, -6.3245e+00, -6.8592e+00, -5.2612e+00,\n                      -3.2988e+00, -6.5949e+00, -5.9954e+00, -3.2070e+00, -6.6336e+00,\n                      -5.2823e+00, -5.7751e+00, -2.6928e+00, -4.6145e+00, -6.1013e+00,\n                      -6.1081e+00, -7.6141e+00, -6.2667e+00, -8.4388e+00, -6.9744e+00,\n                      -2.8998e+00, -6.6778e+00, -6.9318e+00, -9.3678e+00, -6.0810e+00,\n                      -7.6500e+00, -5.5260e+00, -2.5359e-01, -4.1836e+00, -5.4472e+00,\n                      -6.1824e+00, -5.1129e+00, -1.1622e+00, -8.8952e+00, -3.1641e+00,\n                      -3.9236e+00, -5.2299e+00, -5.4381e+00, -6.6192e+00, -3.0995e+00,\n                      -4.0325e+00, -5.4882e+00, -4.9352e+00, -4.0503e+00, -6.8122e+00,\n                      -6.1368e+00, -3.4106e+00, -4.2650e+00, -4.8628e+00, -3.9111e+00,\n                      -6.0139e+00, -4.2001e+00, -3.8017e+00, -1.7849e+00, -4.4087e+00,\n                      -4.8647e+00, -4.3353e+00, -3.6280e+00, -4.3461e+00, -5.0310e+00,\n                      -7.8161e+00, -6.6086e+00, -3.2490e+00, -4.2737e+00, -6.7177e+00,\n                      -3.0957e-01, -3.5859e+00, -5.5698e+00, -3.7218e+00, -6.6281e+00,\n                      -3.9172e+00, -6.3391e+00, -7.9012e+00, -3.2338e+00, -9.2449e+00,\n                      -6.6496e+00, -5.8300e+00, -4.3099e+00, -7.8916e+00, -3.1075e+00,\n                      -3.6655e+00, -4.4740e+00, -2.8784e+00, -2.6516e+00, -6.3433e+00,\n                      -8.3124e+00, -9.8496e-01, -7.1908e+00, -5.1703e+00, -5.3918e+00,\n                      -2.8774e+00, -7.2630e+00, -4.2002e+00, -4.3525e+00, -3.8402e+00,\n                      -6.8036e+00, -6.6428e+00, -5.3836e+00, -4.9098e+00, -6.5351e+00,\n                      -7.5954e+00, -6.0416e+00, -7.1964e+00, -4.6829e+00, -5.1423e+00,\n                      -2.8096e+00, -5.5551e+00, -9.0833e+00, -1.8499e+00, -3.7751e+00,\n                      -4.2297e+00, -3.7476e+00, -4.1521e+00, -4.3220e+00, -2.6914e+00,\n                      -6.8006e+00, -4.3509e+00, -6.5731e+00, -5.1554e+00, -5.7011e+00,\n                      -3.3209e+00, -6.3408e+00, -3.6183e+00, -3.4832e+00, -4.6087e+00,\n                      -6.4479e-01, -6.2560e+00, -4.9178e+00, -6.2997e+00, -4.8418e+00,\n                      -4.6888e+00, -5.2384e+00, -5.7599e+00, -3.4104e+00, -2.1946e+00,\n                      -8.3599e+00, -2.7995e+00, -3.8020e+00, -4.8009e+00, -1.2418e+00,\n                      -4.7493e+00, -6.3056e+00, -2.7549e+00, -4.3267e+00, -5.5315e+00,\n                      -5.0742e+00, -5.2295e+00, -6.1143e+00, -3.3568e+00, -7.6721e+00,\n                      -8.4326e-01, -1.0013e+00, -2.8873e+00, -2.6295e+00, -7.9708e+00,\n                      -4.1617e+00, -5.1362e+00, -4.9308e+00, -8.2961e+00, -2.0014e+00,\n                      -7.5135e+00, -4.4826e+00, -2.6730e+00, -3.3138e+00, -7.6119e+00,\n                      -4.5494e+00, -3.1518e+00, -6.8429e+00, -5.1079e+00, -5.3890e+00,\n                      -6.2759e+00, -2.0821e+00, -6.0221e+00, -4.6286e+00, -6.5116e+00,\n                      -5.7154e+00, -5.4912e+00, -5.5975e+00, -2.1988e+00, -7.6782e+00,\n                      -5.0255e+00, -8.0635e+00, -8.0405e+00, -1.1714e+00, -1.9629e+00,\n                      -6.7150e+00, -1.6787e+00, -5.0675e+00, -4.0629e+00, -5.9264e+00,\n                      -2.3099e+00, -1.0194e-01, -4.2621e-01, -2.1614e+00, -4.8427e+00,\n                      -5.2834e+00, -4.6466e+00, -6.5672e+00, -1.0189e+01, -3.3695e+00,\n                      -4.1970e+00, -4.5876e+00, -3.1016e+00, -6.3150e+00, -5.8409e+00,\n                      -6.6735e+00, -2.8238e+00, -4.5170e+00, -7.0268e+00, -2.9564e+00,\n                      -1.8401e+00, -5.9861e+00, -4.5953e+00, -5.4176e+00, -3.5790e+00,\n                      -4.8920e+00, -3.1677e+00, -8.3481e+00, -3.2906e+00, -6.1360e+00,\n                      -5.4795e+00, -4.7116e+00, -3.3891e+00, -6.1612e+00, -3.3063e+00,\n                      -6.8252e+00, -7.2807e+00, -6.3647e+00, -4.5106e+00, -6.8056e+00,\n                      -3.9655e+00, -3.6682e+00, -1.6440e+00, -5.6463e+00, -5.2496e+00,\n                      -7.3464e+00, -8.2191e+00, -1.5907e+00, -6.3346e+00, -7.2253e+00,\n                      -4.2615e+00, -8.8379e+00, -4.0198e+00, -3.9708e+00, -3.0460e+00,\n                      -7.4644e+00, -5.2332e+00, -4.5663e+00, -5.4890e+00, -2.4743e+00,\n                      -3.2212e+00, -3.7935e+00, -2.6563e+00, -4.8594e+00, -3.4958e+00,\n                      -4.5582e+00, -9.8303e-01, -1.5740e+00, -2.7003e+00, -3.7082e+00,\n                      -3.3943e+00, -3.8021e+00, -7.5594e+00, -6.6158e+00, -7.3531e+00,\n                      -5.1609e+00, -6.9696e+00, -5.0818e+00, -4.6577e+00, -3.2177e+00,\n                      -6.4030e+00, -6.1414e+00, -5.6214e+00, -5.1823e+00, -4.1123e+00,\n                      -4.7117e+00, -5.0970e+00, -5.1378e+00, -4.4785e+00, -4.4460e+00,\n                      -4.9710e+00, -4.9562e+00, -3.3814e+00, -1.9472e+00, -3.9177e+00,\n                      -5.1068e+00, -4.0923e+00, -3.4104e+00, -3.6481e+00, -5.3418e+00,\n                      -2.6626e+00, -7.4549e+00, -5.2854e+00, -6.8749e+00, -5.6954e+00,\n                      -4.9299e+00, -2.4409e+00, -3.3273e+00, -3.5378e+00, -2.6626e+00,\n                      -2.9594e+00, -2.8313e+00, -1.4784e+00, -3.1892e+00, -1.8196e+00,\n                      -6.6726e+00, -2.8423e+00, -4.9312e+00, -5.2384e+00, -5.3069e+00,\n                      -5.9799e+00, -6.7492e+00, -4.4266e+00, -1.4011e+00, -2.0692e+00,\n                      -4.7965e+00, -6.0121e+00, -5.3256e+00, -6.1872e+00, -4.1080e+00,\n                      -3.5066e+00, -5.4727e+00, -2.2943e+00, -3.1495e+00, -3.2563e+00,\n                      -1.9149e+00, -4.0726e+00, -3.7246e+00, -5.0553e+00, -5.3795e+00,\n                      -5.7020e+00, -5.6653e+00, -8.5901e+00, -3.0465e+00, -6.9207e+00,\n                      -7.9984e+00, -4.6856e+00, -2.4181e+00, -6.3781e+00, -4.5566e+00,\n                      -7.7666e+00, -5.0357e+00, -3.7581e+00, -7.0127e+00, -2.7983e+00,\n                      -3.8980e+00, -3.4485e+00, -7.0997e+00, -6.0104e+00, -4.1518e+00,\n                      -5.4493e+00, -4.8756e+00,  5.5495e-01, -3.9939e+00, -3.0220e+00,\n                      -4.4707e+00, -3.0223e+00, -4.4317e+00, -5.5115e+00, -4.0022e+00,\n                      -1.8242e+00, -3.1268e+00, -4.5460e+00, -6.8334e+00, -7.5769e+00,\n                      -5.5230e+00, -4.8351e+00, -4.0844e+00, -5.1358e+00, -5.6902e+00,\n                      -7.3683e+00, -3.5197e+00, -3.9629e+00, -2.6868e+00, -3.6077e+00,\n                      -5.8721e+00, -4.8658e+00, -2.1094e+00, -3.9946e+00, -6.6960e+00,\n                      -5.2021e+00, -5.4816e+00, -6.0042e+00, -6.3856e+00, -4.4278e+00,\n                      -5.4218e+00, -3.0158e+00, -5.0089e+00, -8.2471e+00, -2.6453e+00,\n                      -4.7963e+00, -4.3189e+00, -2.5071e+00, -5.9431e+00, -2.3905e+00,\n                      -6.3595e+00, -4.1198e+00, -4.9486e+00, -5.3480e+00, -3.0392e-01,\n                      -7.4127e+00, -6.8043e+00, -3.8934e+00, -5.6440e+00, -7.6319e+00,\n                      -3.8117e+00, -6.6703e+00, -5.1228e+00, -6.8960e+00, -3.4034e+00,\n                      -4.8791e+00, -6.3523e+00, -5.9009e+00, -2.5506e+00, -4.8916e+00,\n                      -6.6415e+00, -1.2208e+00, -5.7230e+00, -3.2939e+00, -6.1649e+00,\n                      -4.9234e+00, -5.4100e+00, -4.2305e+00, -2.8931e+00, -3.9878e+00,\n                      -3.0028e+00, -6.3730e+00, -8.6136e-01, -3.9583e+00, -2.3950e+00,\n                      -3.0907e+00, -2.2836e+00, -4.6933e+00, -3.7427e+00, -7.0438e+00,\n                      -7.2696e+00, -5.6212e+00, -4.4288e+00, -5.6963e+00, -8.6889e+00,\n                      -4.6532e+00, -7.9521e+00, -6.6956e+00, -4.4578e+00, -5.1761e+00,\n                      -1.4324e+00, -7.3499e+00, -3.5438e+00, -5.8323e+00, -3.4482e+00,\n                      -5.5040e+00, -4.1084e+00, -5.2815e+00, -3.7326e+00, -6.3059e+00,\n                      -5.7430e+00, -5.9026e+00, -8.0121e+00, -2.0407e+00, -4.8024e+00,\n                      -4.3908e+00, -4.2385e+00, -2.1242e+00,  5.8278e-03, -4.1745e+00,\n                      -5.8387e+00, -4.8833e+00])),\n             ('layer4.1.bn2.running_var',\n              tensor([10.5786, 10.3320,  8.1193, 10.6606,  9.1549, 10.1037, 10.2968, 10.4568,\n                       8.1766, 10.6984,  8.5102,  9.7195,  9.5226,  8.3093,  8.0786,  8.8935,\n                       9.2113,  8.7328,  8.3005, 10.2079,  9.9180,  7.2846,  9.5135,  9.4560,\n                       9.3067,  9.6163,  9.1160,  8.5765,  8.5731,  8.7894,  9.8362,  8.6922,\n                       9.8731,  9.4230,  7.8292,  8.9682, 10.4785, 10.3115,  9.1422,  8.4236,\n                      11.2935,  9.1000, 11.5609,  8.8755,  9.8191,  9.3304,  8.9806, 10.4026,\n                      11.2141,  9.6070, 10.2418,  9.4941,  8.9955,  9.6096,  8.8339,  8.7137,\n                       8.5689,  8.3830,  9.3436,  8.3238,  9.5428,  9.7043,  7.4313,  9.4357,\n                       9.0931,  8.6051, 10.2408,  9.5476, 10.7610, 10.3739, 10.6631,  8.6965,\n                      10.3223, 11.6344, 10.4044,  9.4800,  9.5918,  7.4794, 10.2663,  9.6095,\n                      10.6934,  8.7441,  8.5080,  8.3663,  9.6359,  7.8110,  8.6039,  9.7166,\n                       8.8144,  8.0457,  7.2067,  9.2357, 10.9290,  8.7563, 10.2115, 10.0500,\n                       9.3617,  7.9227,  8.9372, 10.3784,  8.8838,  8.1441,  9.6756,  8.4915,\n                       8.0131,  9.2912, 10.7220,  8.1625, 10.1281,  9.5339, 12.6949,  9.2445,\n                       8.3462,  8.6877,  9.7106,  8.2981,  9.2779,  9.2250,  8.5095,  9.3240,\n                       8.7435,  9.7345,  7.9169,  9.8421,  8.3345,  9.2625, 10.3296, 10.3633,\n                      11.3974,  8.8136, 10.6713,  8.7172,  9.7294,  8.8498, 11.0812, 12.4321,\n                      10.2990,  9.0338, 11.3289,  9.1410, 10.3535, 10.1551,  8.8024, 10.3306,\n                       9.6941,  9.5224,  8.4943,  8.7901, 10.0776,  8.9394,  8.2002,  8.7624,\n                      10.9602,  9.6840,  8.5125,  8.2243, 10.1792,  8.1929,  9.6509, 10.0524,\n                       8.1930,  9.0486, 10.0898, 10.7229,  9.7064,  8.6431,  8.8770,  8.3492,\n                       9.3322, 10.0760, 10.8347,  9.9399, 10.3771,  8.8528,  8.8662,  9.1302,\n                       7.7203, 10.1825, 11.9787,  9.4290,  9.0618,  8.3811,  8.6786,  9.3093,\n                       8.6128,  9.3200,  9.7745,  8.5199,  9.0445, 10.3498,  8.1152, 10.2381,\n                      10.0760,  8.0260, 10.0168,  8.4999,  9.0081, 11.3094,  9.2674,  8.9622,\n                       8.9912,  8.9688,  8.7066, 11.6792,  8.5228,  9.4921,  8.4545,  9.1013,\n                       8.6572, 10.9411, 10.5285,  8.1357, 10.3280, 10.1382,  9.5398,  8.7948,\n                       9.3055,  8.4570, 10.1390,  9.9280,  7.6744,  8.7824, 10.7540,  9.4203,\n                       9.8365,  8.7819,  9.1783,  9.5273, 10.5099,  8.7861, 10.0691,  9.5967,\n                       8.0484,  8.8382, 11.0281,  9.4160,  9.9388,  8.8891, 10.2809, 10.1238,\n                       8.9662, 10.0200,  8.3514, 13.7151,  8.8668, 10.9719,  8.5214,  9.0579,\n                       9.1251,  9.7726,  9.8166, 10.5108, 13.0595,  8.1418,  9.3871,  9.2427,\n                       9.8527,  7.7014, 11.7157, 10.1565, 10.2403,  9.4563,  8.6051, 10.0118,\n                       9.8436,  9.1500, 10.0510,  9.8371,  9.2969,  9.5328, 12.5649, 11.9601,\n                       9.0443, 10.2993, 11.3628, 10.1467,  9.1958,  9.6728, 10.4982,  8.1510,\n                       9.4085,  9.9020,  8.9757, 10.0767, 10.5132,  9.4449, 10.7462,  8.3190,\n                       7.9431,  9.0851,  9.3690, 10.2999,  9.3544,  9.2161,  9.8303,  8.8979,\n                       8.7917,  9.0420,  9.3758,  9.4470,  9.6664,  8.3805,  9.5715,  8.4666,\n                      10.7768,  9.3824,  8.3487,  8.8465,  8.6481,  9.1199, 11.1335, 10.2723,\n                      10.4410,  8.9680,  8.9559,  8.2762,  9.4105,  8.6302,  8.9652,  8.2018,\n                       9.6508, 11.3978,  8.2623, 10.0685, 12.3167,  9.5908,  8.3216,  9.0061,\n                      11.0749,  9.4758,  8.7245,  7.7242,  9.1855,  9.4476,  8.2428, 10.7874,\n                      11.2514,  8.7291,  8.3212, 11.6751,  8.8007,  9.1394, 10.2056,  9.2916,\n                       8.2201,  9.6081,  7.8749, 10.3441,  9.3300,  7.8784,  9.6885,  8.0823,\n                       8.6160,  8.7205,  8.5730,  9.6724, 10.2211,  9.2300,  8.6080,  8.0566,\n                       9.7124,  9.0432,  8.7256,  8.3279,  8.4162,  8.1404,  9.0332, 10.3433,\n                       8.5728,  8.7745,  8.7534,  8.3352,  7.4560,  8.9851,  8.5232,  9.4923,\n                       9.7068,  9.5075,  8.0300,  9.1309, 11.9258, 10.8868,  9.4788, 11.4020,\n                       7.9088,  8.7173,  8.4384,  7.6511,  9.5109,  9.6519,  7.7802,  8.7478,\n                       9.1457,  8.6380,  9.8230,  8.2908, 10.3205, 10.5649,  9.4083, 12.0274,\n                      10.9183,  9.6670, 10.4317,  9.8672,  8.7329,  9.2025,  9.6637,  9.4677,\n                      10.3016,  9.5107,  8.1072,  9.2891,  9.4092,  9.7159,  9.5823, 10.0991,\n                       7.9041,  8.4401, 11.4802,  9.9912,  9.7482, 10.8648,  8.3411, 10.4097,\n                       9.2671,  8.6059, 12.0118, 10.1694,  9.1301,  8.8351,  8.7030,  9.5125,\n                       9.0683, 10.0663,  8.9299,  9.3106,  9.5324,  9.6807,  8.9803,  7.6620,\n                      10.2251,  8.2080,  7.8371,  8.7639,  7.9909,  9.3490,  9.9630,  8.8340,\n                      10.5583, 11.2693,  9.6499,  9.4051,  8.7444,  9.9397,  8.5436,  9.0624,\n                       9.6271,  9.7233,  8.6692,  9.0740, 11.6773, 10.5857,  8.5459,  9.1321,\n                       7.7055, 10.0655,  8.7228, 10.0671,  8.3120,  9.5533,  8.5139, 10.3254,\n                       9.6588,  8.7060,  8.9169,  8.7888,  9.6462, 10.6519,  9.9382, 10.8298,\n                      12.0924,  9.1123,  9.3893, 10.1528, 12.2299,  8.6133,  9.4586, 11.1169,\n                       9.3331, 11.7502,  8.8175,  9.2529,  9.4911,  9.3763,  8.6846, 11.8910,\n                      10.1261,  9.6782,  9.4414,  9.0096,  8.0692,  8.8389,  9.4488,  8.2939,\n                       8.4314,  8.7098,  9.5868, 10.7429, 10.4587,  7.8283, 10.1228, 11.1365])),\n             ('layer4.1.bn2.num_batches_tracked', tensor(14770)),\n             ('fc.weight',\n              tensor([[-0.0615, -0.0245,  0.0545,  ...,  0.1973,  0.1550, -0.1071],\n                      [-0.1192,  0.4129,  0.0924,  ...,  0.0140,  0.1235,  0.0410],\n                      [ 0.0176,  0.0514,  0.2388,  ..., -0.2288, -0.0499, -0.1529],\n                      ...,\n                      [-0.0677,  0.0094,  0.0240,  ...,  0.0918,  0.0475, -0.2477],\n                      [-0.0916, -0.0847,  0.1165,  ...,  0.0291, -0.0864,  0.2865],\n                      [-0.0589, -0.1477,  0.0103,  ..., -0.1154, -0.0031,  0.0182]])),\n             ('fc.bias',\n              tensor([-3.0607e-02,  1.5160e-02, -3.7891e-02,  5.5426e-02,  2.2524e-02,\n                       2.9247e-05, -1.0327e-02, -3.7550e-02, -4.1523e-02,  3.7655e-02,\n                      -2.1826e-02,  3.0143e-02,  1.9948e-02, -2.6718e-02, -5.0146e-03,\n                      -1.3948e-02, -5.0351e-02, -1.1498e-02, -1.6172e-02,  6.0408e-03,\n                      -3.0403e-02, -2.7409e-02, -4.0124e-02, -1.6633e-02,  1.6222e-02,\n                       3.4293e-02, -4.7670e-02,  1.3897e-02,  4.5623e-02,  7.2857e-03,\n                      -5.6908e-02, -2.1962e-02,  3.3048e-02, -5.9308e-02,  2.6516e-02,\n                      -2.6704e-02,  2.9102e-02, -4.4296e-03,  1.3434e-02,  1.5977e-02,\n                       1.1086e-02,  1.7396e-03,  9.0611e-03,  3.6332e-02,  7.9391e-03,\n                       2.0729e-02, -1.7113e-02,  1.9475e-03,  3.5453e-03,  9.8975e-03,\n                       9.3734e-03, -1.2872e-02, -1.3717e-02,  5.6633e-03, -1.0321e-02,\n                      -1.1342e-02,  9.6762e-03,  4.1573e-03, -7.4177e-03, -5.9139e-02,\n                      -5.2239e-02, -1.3015e-03, -5.6644e-02,  4.9597e-02,  1.7729e-02,\n                       3.6475e-02, -6.9686e-02,  8.0654e-03, -6.1731e-03,  2.6786e-03,\n                      -7.6265e-03,  2.9842e-02,  2.9057e-02, -3.4820e-04, -5.3871e-02,\n                       5.8352e-02,  2.2986e-03,  9.0691e-03,  3.2840e-02, -6.9247e-03,\n                      -9.7278e-03,  3.2985e-02,  2.3152e-02,  4.4692e-02,  1.1919e-02,\n                      -5.8637e-02,  3.5320e-02, -4.4832e-02, -1.8432e-02,  2.8514e-02,\n                       8.8598e-03,  3.4282e-02, -5.6492e-02, -2.5487e-03, -3.4813e-03,\n                       3.6707e-02,  5.9202e-02, -1.3211e-02,  6.0752e-02, -9.9677e-03]))])"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"# Save checkpoint\ntorch.save(model.state_dict(), '/kaggle/working/model1.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:23:45.741701Z","iopub.execute_input":"2025-06-04T17:23:45.742021Z","iopub.status.idle":"2025-06-04T17:23:45.852675Z","shell.execute_reply.started":"2025-06-04T17:23:45.742002Z","shell.execute_reply":"2025-06-04T17:23:45.851956Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def plot_training_history(history):\n    \"\"\"\n    Plot training and validation metrics\n    \"\"\"\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n    \n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Loss plot\n    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n    ax1.set_title('Training and Validation Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Accuracy plot\n    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n    ax2.set_title('Training and Validation Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy (%)')\n    ax2.legend()\n    ax2.grid(True)\n    \n    # Learning rate plot\n    ax3.plot(epochs, history['learning_rates'], 'g-')\n    ax3.set_title('Learning Rate Schedule')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Learning Rate')\n    ax3.set_yscale('log')\n    ax3.grid(True)\n    \n    # Overfitting check\n    train_val_diff = [t - v for t, v in zip(history['train_acc'], history['val_acc'])]\n    ax4.plot(epochs, train_val_diff, 'purple')\n    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n    ax4.set_title('Training vs Validation Gap')\n    ax4.set_xlabel('Epoch')\n    ax4.set_ylabel('Train Acc - Val Acc (%)')\n    ax4.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef detailed_validation_report(model, val_loader, device, class_names=None):\n    \"\"\"\n    Generate detailed validation report with confusion matrix\n    \"\"\"\n    val_loss, val_acc, val_preds, val_targets = validate_model(\n        model, val_loader, nn.CrossEntropyLoss(), device\n    )\n    \n    print(f\"\\n{'='*50}\")\n    print(\"DETAILED VALIDATION REPORT\")\n    print(f\"{'='*50}\")\n    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    \n    # Classification report\n    if class_names is None:\n        class_names = [f\"Class_{i}\" for i in range(len(set(val_targets)))]\n    \n    print(f\"\\n{'-'*50}\")\n    print(\"CLASSIFICATION REPORT\")\n    print(f\"{'-'*50}\")\n    print(classification_report(val_targets, val_preds, target_names=class_names))\n    \n    # Confusion matrix\n    cm = confusion_matrix(val_targets, val_preds)\n    print(f\"\\n{'-'*50}\")\n    print(\"CONFUSION MATRIX\")\n    print(f\"{'-'*50}\")\n    print(cm)\n    \n    return val_acc, val_loss, cm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:46:03.003319Z","iopub.status.idle":"2025-06-04T15:46:03.003609Z","shell.execute_reply.started":"2025-06-04T15:46:03.003468Z","shell.execute_reply":"2025-06-04T15:46:03.003481Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}